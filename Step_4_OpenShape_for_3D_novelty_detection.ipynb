{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["W_I1PHBrhadT","Jw-TaS50jZlp","AWm2jCYYxBhl","W0YW3xqHyJDT","tWsXrNENL40j","_90Y_MbTMe84","aiYWbvj2e3Vb","xml82DqdMFIf"],"mount_file_id":"1iIoUM_uq_cNO5BjdvNRnY4XyzHytTfj2","authorship_tag":"ABX9TyO4HeFk1ntCAevXJcg1i7c9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Notebook for step 4 of AML Project\n","This notebook is created for fine tuning the OpenShape model. We have used code from the following repositories:\n","\n","Open shape:\n","@misc{liu2023openshape,\n","      title={OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding},\n","      author={Minghua Liu and Ruoxi Shi and Kaiming Kuang and Yinhao Zhu and Xuanlin Li and Shizhong Han and Hong Cai and Fatih Porikli and Hao Su},\n","      year={2023},\n","      eprint={2305.10764},\n","      archivePrefix={arXiv},\n","      primaryClass={cs.CV}\n","}\n","\n","3DOS:\n","@inproceedings{\n","alliegro2022towards,\n","title={Towards Open Set 3D Learning: Benchmarking and Understanding Semantic Novelty Detection on Pointclouds},\n","author={Antonio Alliegro and Francesco Cappio Borlino and Tatiana Tommasi},\n","booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n","year={2022},\n","url={https://openreview.net/forum?id=X2dHozbd1at}\n","}"],"metadata":{"id":"9teFaLRdlp5Z"}},{"cell_type":"markdown","source":["# Clone git repos to environment"],"metadata":{"id":"W_I1PHBrhadT"}},{"cell_type":"code","source":["\n","!git clone https://github.com/Colin97/OpenShape_code.git\n","# Make sure you have git-lfs installed (https://git-lfs.com)\n","!git lfs install\n","!git clone https://huggingface.co/spaces/OpenShape/openshape-demo\n","\n","# if you want to clone without large files â€“ just their pointers\n","# prepend your git clone with the following env var:\n","!GIT_LFS_SKIP_SMUDGE=1\n","#!pip install -e .\n","!git clone https://huggingface.co/OpenShape/openshape-demo-support\n"],"metadata":{"id":"i2PTprijZ5vV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708088284337,"user_tz":-60,"elapsed":1130,"user":{"displayName":"Kristoffer Movik","userId":"14194010859010090109"}},"outputId":"2c0b8c2e-6f2b-4662-9421-04373a9a8b48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'OpenShape_code' already exists and is not an empty directory.\n","Git LFS initialized.\n","fatal: destination path 'openshape-demo' already exists and is not an empty directory.\n","fatal: destination path 'openshape-demo-support' already exists and is not an empty directory.\n"]}]},{"cell_type":"markdown","source":["# Install dependencies\n","Most dependencies are already satisfied by default in google colab"],"metadata":{"id":"Jw-TaS50jZlp"}},{"cell_type":"code","source":["pip install huggingface_hub wandb omegaconf torch_redstone einops tqdm open3d dgl timm\n","pip install --upgrade https://github.com/unlimblue/KNN_CUDA/releases/download/0.2/KNN_CUDA-0.2-py3-none-any.whl\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":111},"id":"4PSbBCbC26vP","executionInfo":{"status":"error","timestamp":1708088284337,"user_tz":-60,"elapsed":7,"user":{"displayName":"Kristoffer Movik","userId":"14194010859010090109"}},"outputId":"5cca6707-b320-451a-85a0-0c201493e7c7"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-54-36226d94238f>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-36226d94238f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install huggingface_hub wandb omegaconf torch_redstone einops tqdm open3d dgl timm\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","source":["# Log in to Huggingface"],"metadata":{"id":"AWm2jCYYxBhl"}},{"cell_type":"code","source":["from google.colab import output\n","output.enable_custom_widget_manager()"],"metadata":{"id":"3TLMaekmxBPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import login\n","login()"],"metadata":{"id":"nmYWjzhrZxF6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get the OpenShape model from Huggingface"],"metadata":{"id":"EmFurYFiyCSM"}},{"cell_type":"code","source":["%cd /content/openshape-demo-support\n","!pip install -e .\n","%cd /content"],"metadata":{"id":"_-UugGsGiumt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import openshape\n","model = openshape.load_pc_encoder('openshape-pointbert-vitg14-rgb')\n","\n","\n","model.eval()"],"metadata":{"id":"21n-SAviizUw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Function for fitting pointclouds to input dimentions for OpenShape"],"metadata":{"id":"W0YW3xqHyJDT"}},{"cell_type":"code","source":["import numpy as np\n","import open3d as o3d\n","import random\n","import torch\n","from OpenShape_code.src.utils.data import normalize_pc\n","\n","def load_ply(file_name, num_points=10000, y_up=True):\n","    pcd = o3d.io.read_point_cloud(file_name)  # Read the point cloud\n","    xyz = np.asarray(pcd.points)  # Get xyz coordinates\n","    rgb = np.asarray(pcd.colors)  # Get rgb colors\n","    n = xyz.shape[0]\n","\n","    # Sample num_points points if necessary\n","    if n > num_points:\n","        idx = random.sample(range(n), num_points)\n","        xyz = xyz[idx]\n","        rgb = rgb[idx]\n","    elif n < num_points:\n","        print(f\"Warning: requested {num_points} points, but file has only {n} points.\", file=sys.stderr)\n","\n","    # Adjust orientation by swapping y and z if requested\n","    if y_up:\n","        xyz[:, [1, 2]] = xyz[:, [2, 1]]\n","\n","    # Normalize the point cloud coordinates\n","    xyz_normalized = normalize_pc(xyz)\n","\n","    # Handle cases where rgb might be missing\n","    if rgb.size == 0:\n","        rgb = np.ones_like(xyz_normalized) * 0.4  # Default to a constant color if missing\n","\n","    # Concatenate xyz with rgb\n","    features = np.concatenate([xyz_normalized, rgb], axis=1)\n","\n","    # Convert to PyTorch tensors\n","    xyz_tensor = torch.from_numpy(xyz_normalized).float()\n","    features_tensor = torch.from_numpy(features).float()\n","\n","    # Add batch dimention to fit as single input to OpenShape model\n","    features_tensor = features_tensor.unsqueeze(0)  # Adds a batch dimension, making it [1, N, 6]\n","    features_tensor = features_tensor.transpose(1, 2)  # Transposes to get [1, 6, N], matching the expected [B, C, N] format\n","\n","    # Returning tensors instead of ME-specific batched coordinates\n","    return xyz_tensor, features_tensor\n","\n","def filter_for_OpenShape(pointcloud, num_points=10000, y_up=True):\n","    xyz = np.asarray(pointcloud.points)  # Get xyz coordinates\n","    rgb = np.asarray(pointcloud.colors)  # Get rgb colors\n","    n = xyz.shape[0]\n","\n","    # Sample num_points points if necessary\n","    if n > num_points:\n","        idx = random.sample(range(n), num_points)\n","        xyz = xyz[idx]\n","        rgb = rgb[idx]\n","    elif n < num_points:\n","        print(f\"Warning: requested {num_points} points, but file has only {n} points.\", file=sys.stderr)\n","\n","    # Adjust orientation by swapping y and z if requested\n","    if y_up:\n","        xyz[:, [1, 2]] = xyz[:, [2, 1]]\n","\n","    # Normalize the point cloud coordinates\n","    xyz_normalized = normalize_pc(xyz)\n","\n","    # Handle cases where rgb might be missing\n","    if rgb.size == 0:\n","        rgb = np.ones_like(xyz_normalized) * 0.4  # Default to a constant color if missing\n","\n","    # Concatenate xyz with rgb\n","    features = np.concatenate([xyz_normalized, rgb], axis=1)\n","\n","    # Convert to PyTorch tensors\n","    xyz_tensor = torch.from_numpy(xyz_normalized).float()\n","    features_tensor = torch.from_numpy(features).float()\n","\n","    # Add batch dimention to fit as single input to OpenShape model\n","    features_tensor = features_tensor.unsqueeze(0)  # Adds a batch dimension, making it [1, N, 6]\n","    features_tensor = features_tensor.transpose(1, 2)  # Transposes to get [1, 6, N], matching the expected [B, C, N] format\n","\n","    # Returning tensors instead of ME-specific batched coordinates\n","    return xyz_tensor, features_tensor\n"],"metadata":{"id":"_LyfD_SHnKEu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test that installation and environment works\n","\n","If a feature vector is printed it should all be A OK!"],"metadata":{"id":"tWsXrNENL40j"}},{"cell_type":"code","source":["xyz, feat = load_ply(\"/content/OpenShape_code/demo/owl.ply\")\n","\n","device = torch.device(\"cpu\")\n","model = model.to(device)\n","feat = feat.to(device)\n","\n","output = model.forward(feat)\n","\n","print(output)\n"],"metadata":{"id":"TqztyB8JnQVn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Clone 3DOS repo"],"metadata":{"id":"_90Y_MbTMe84"}},{"cell_type":"code","source":["# Clone 3D_OS repo from github\n","!git clone https://github.com/antoalli/3D_OS.git\n","!cd 3D_OS && chmod +x download_data.sh && ./download_data.sh\n","!pip install h5py protobuf lmdb msgpack-numpy ninja scikit-learn\n"],"metadata":{"id":"UaL5_GSiTzpg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Using SR1 as ID create dataloaders for train, ID, OOD1 and OOD2.\n","\n","The datasets we use are the same as the datasets used in 3DOS. We also use the code given in the 3DOS paper to create the dataloaders.\n","\n","\n","Citation:\n","\\@inproceedings{\n","alliegro2022towards,\n","title={Towards Open Set 3D Learning: Benchmarking and Understanding Semantic Novelty Detection on Pointclouds},\n","author={Antonio Alliegro and Francesco Cappio Borlino and Tatiana Tommasi},\n","booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n","year={2022},\n","url={https://openreview.net/forum?id=X2dHozbd1at}\n","}"],"metadata":{"id":"7ayDNCZQNgEe"}},{"cell_type":"code","source":["# Change folder to 3D_OS to get the same dataloaders\n","%cd /content/3D_OS\n","\n","import sys\n","import os\n","import warnings\n","import numpy as np\n","\n","sys.path.append(os.getcwd())\n","import os.path as osp\n","import time\n","from torch.cuda.amp import GradScaler, autocast\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from utils.utils import *\n","from utils.dist import *\n","# noinspection PyUnresolvedReferences\n","from utils.data_utils import H5_Dataset\n","#from datasets.modelnet import *\n","from datasets.scanobject import *\n","from models.classifiers import Classifier\n","from utils.ood_utils import get_confidence, eval_ood_sncore, iterate_data_odin, \\\n","    iterate_data_energy, iterate_data_gradnorm, iterate_data_react, estimate_react_thres, print_ood_output, \\\n","    get_penultimate_feats, get_network_output\n","import wandb\n","from base_args import add_base_args\n","from sklearn.metrics import accuracy_score, balanced_accuracy_score\n","from models.common import convert_model_state, logits_entropy_loss\n","from models.ARPL_utils import Generator, Discriminator\n","from classifiers.common import train_epoch_cla, train_epoch_rsmix_exposure, train_epoch_cs\n","from classifiers.trainer_ddp_cla_md import get_args, load_yaml, get_md_eval_loaders\n","\n","# Code from 3DOS repo classifiers/trainer_ddp_cla_md.py line 192-478\n","\n","#  python -m torch.distributed.launch --nproc_per_node=1 Failure_Analysis/custom\\\n","# tests/evaluating_PointNet_cosine_MLS.py --config cfgs/pn2-msg.yaml\n","# --exp_name PN2_cosine_SR1 --src SR1 --loss cosine -mode eval\n","# --ckpt_path outputs/PN2_cosine_SR1/models/model_best.pth\n","\n","\n","# Code from 3DOS repo classifiers/trainer_ddp_cla_md.py line 468-478\n","def eval_ood_md2sonn(opt, config):\n","    print(f\"Arguments: {opt}\")\n","    set_random_seed(opt.seed)\n","\n","    dataloader_config = {\n","        'batch_size': opt.batch_size, 'drop_last': False, 'shuffle': False,\n","        'num_workers': opt.num_workers, 'sampler': None, 'worker_init_fn': init_np_seed}\n","\n","    # whole evaluation is done on ScanObject RW data\n","    sonn_args = {\n","        'data_root': opt.data_root,\n","        'sonn_split': opt.sonn_split,\n","        'h5_file': opt.sonn_h5_name,\n","        'split': 'all',  # we use both training (unused) and test samples during evaluation\n","        'num_points': opt.num_points_test,  # default: use all 2048 sonn points to avoid sampling randomicity\n","        'transforms': None  # no augmentation applied at inference time\n","    }\n","\n","    train_loader, _ = get_md_eval_loaders(opt)\n","    if opt.src == 'SR1':\n","        print(\"Src is SR1\\n\")\n","        id_loader = DataLoader(ScanObject(class_choice=\"sonn_2_mdSet1\", **sonn_args), **dataloader_config)\n","        ood1_loader = DataLoader(ScanObject(class_choice=\"sonn_2_mdSet2\", **sonn_args), **dataloader_config)\n","    elif opt.src == 'SR2':\n","        print(\"Src is SR2\\n\")\n","        id_loader = DataLoader(ScanObject(class_choice=\"sonn_2_mdSet2\", **sonn_args), **dataloader_config)\n","        ood1_loader = DataLoader(ScanObject(class_choice=\"sonn_2_mdSet1\", **sonn_args), **dataloader_config)\n","    else:\n","        raise ValueError(f\"OOD evaluation - wrong src: {opt.src}\")\n","\n","    ood2_loader = DataLoader(ScanObject(class_choice=\"sonn_ood_common\", **sonn_args), **dataloader_config)\n","\n","    return train_loader, id_loader, ood1_loader, ood2_loader\n","\n","class Options:\n","    def __init__(self, dictionary):\n","        for key, value in dictionary.items():\n","            setattr(self, key, value)\n","\n","opt_dict = {\n","    \"apply_fix_cellphone\": True,\n","    \"augm_set\": 'rw',\n","    \"batch_size\": 1,\n","    \"checkpoints_dir\": 'outputs',\n","    \"ckpt_path\": 'outputs/PN2_cosine_SR1/models/model_best.pth',\n","    \"config\": 'cfgs/pn2-msg.yaml',\n","    \"corruption\": None,\n","    \"cs\": False,\n","    \"cs_beta\": 0.1,\n","    \"cs_gan_lr\": 0.0002,\n","    \"data_root\": \"/content/3D_OS/3D_OS_release_data\",\n","    \"epochs\": 250,\n","    \"eval_step\": 1,\n","    \"exp_name\": 'PN2_cosine_SR1',\n","    \"grad_norm_clip\": -1,\n","    \"local_rank\": 0,\n","    \"loss\": 'cosine',\n","    \"num_points\": 10000,\n","    \"num_points_test\": 10000,\n","    \"num_workers\": 6,\n","    \"resume\": None,\n","    \"save_feats\": None,\n","    \"save_step\": 10,\n","    \"script_mode\": 'eval',\n","    \"seed\": 1,\n","    \"sonn_h5_name\": 'objectdataset.h5',\n","    \"sonn_split\": 'main_split',\n","    \"src\": 'SR1',\n","    \"tar1\": 'none',\n","    \"tar2\": 'none',\n","    \"use_amp\": False,\n","    \"use_sync_bn\": False,\n","    \"wandb_group\": 'md-2-sonn-augmCorr',\n","    \"wandb_name\": None,\n","    \"wandb_proj\": 'benchmark-3d-ood-cla'\n","}\n","\n","opt = Options(opt_dict)\n","\n","config = {'optimizer': {'type': 'adam',\n","                        'skip_wd': [],\n","                        'weight_decay': 0.0001,\n","                        'kwargs': {'lr': 0.001}\n","                        },\n","          'scheduler': {'type': 'CosLR',\n","                        'kwargs': {'t_initial': 250,\n","                                   'cycle_limit': 1,\n","                                   'lr_min': 1e-05\n","                                   }\n","                        },\n","          'model': {'ENCO_NAME': 'pn2-msg',\n","                    'dropout': 0.5,\n","                    'cla_input_dim': 1024,\n","                    'act': 'relu'\n","                    }\n","          }\n","\n","print(opt)\n","print(config)\n","\n","train_loader, src_loader, tar1_loader, tar2_loader = eval_ood_md2sonn(opt, config)\n","\n","%cd /content\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"id":"kNUw_MKjNmBj","executionInfo":{"status":"error","timestamp":1708267382602,"user_tz":-60,"elapsed":5666,"user":{"displayName":"Kristoffer Movik","userId":"14194010859010090109"}},"outputId":"ae321c67-6e13-41fa-862c-31192dd69c4b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/3D_OS'\n","/content\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'utils'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-82cec73d662d>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# noinspection PyUnresolvedReferences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["# Test if dataloaders work"],"metadata":{"id":"Zbr8Hm7Ee_H-"}},{"cell_type":"code","source":["# Test if the data loaders work\n","import plotly.graph_objects as go\n","\n","for i, (data, label) in enumerate(src_loader):\n","    points = data\n","    points = points.reshape(-1, 3)\n","\n","    pcd = o3d.geometry.PointCloud()\n","    pcd.points = o3d.utility.Vector3dVector(points)\n","\n","    fig = go.Figure(\n","      data=[\n","          go.Scatter3d(\n","            x=points[:,0],\n","            y=points[:,1],\n","            z=points[:,2],\n","            mode='markers',\n","          )\n","      ],\n","      layout=dict(\n","            scene=dict(\n","            xaxis=dict(visible=False),\n","            yaxis=dict(visible=False),\n","            zaxis=dict(visible=False)\n","        )\n","      )\n","    )\n","    fig.show()\n","    break\n"],"metadata":{"id":"s4hkA0TVX8Tt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run data through the OpenShape model"],"metadata":{"id":"aiYWbvj2e3Vb"}},{"cell_type":"code","source":["# Run data through OpenShape\n","from tqdm import tqdm\n","device = torch.device(\"cpu\")\n","model = model.to(device)\n","\n","%cd /content\n","save_dir = \"OpenShape_outputs/SR1\"\n","if not os.path.exists(save_dir):\n","    print(f\"Creating directory: {save_dir}\")\n","    os.makedirs(save_dir)\n","\n","print(\"Running training data through the OpenShape model\")\n","train_feats = []\n","for i, (data, label) in tqdm(enumerate(train_loader)):\n","    points = data\n","    points = points.reshape(-1, 3)\n","    pcd = o3d.geometry.PointCloud()\n","    pcd.points = o3d.utility.Vector3dVector(points)\n","\n","    xyz, feat = filter_for_OpenShape(pcd)\n","    feat = feat.to(device)\n","\n","    output_feat = model.forward(feat)\n","\n","    train_feats.append(output_feat.detach().numpy())\n","\n","outfile = save_dir + \"/train_feats.npy\"\n","np.save(outfile, np.array(train_feats))\n","print(\"Saved data to \" + outfile)\n","\n","print(\"Running ID data through the OpenShape model\")\n","src_feats = np.array([])\n","for i, (data, label) in tqdm(enumerate(src_loader)):\n","    points = data\n","    points = points.reshape(-1, 3)\n","    pcd = o3d.geometry.PointCloud()\n","    pcd.points = o3d.utility.Vector3dVector(points)\n","\n","    xyz, feat = filter_for_OpenShape(pcd)\n","    feat = feat.to(device)\n","\n","    output_feat = model.forward(feat)\n","\n","    src_feats.append(output_feat.detach().numpy())\n","\n","outfile = save_dir + \"/src_feats\"\n","np.save(outfile, np.array(src_feats))\n","print(\"Saved data to \" + outfile)\n","\n","print(\"Rinning OOD1 data throug the OpenShape model\")\n","tar1_feats = np.array([])\n","for i, (data, label) in tqdm(enumerate(tar1_loader)):\n","    points = data\n","    points = points.reshape(-1, 3)\n","    pcd = o3d.geometry.PointCloud()\n","    pcd.points = o3d.utility.Vector3dVector(points)\n","\n","    xyz, feat = filter_for_OpenShape(pcd)\n","    feat = feat.to(device)\n","\n","    output_feat = model.forward(feat)\n","\n","    tar1_feats.append(output_feat.detach().numpy())\n","\n","outfile = save_dir + \"/tar1_feats\"\n","np.save(outfile, np.array(tar1_feats))\n","print(\"Saved data to \" + outfile)\n","\n","print(\"Rinning OOD2 data throug the OpenShape model\")\n","tar2_feats = np.array([])\n","for i, (data, label) in tqdm(enumerate(tar2_loader)):\n","    points = data\n","    points = points.reshape(-1, 3)\n","    pcd = o3d.geometry.PointCloud()\n","    pcd.points = o3d.utility.Vector3dVector(points)\n","\n","    xyz, feat = filter_for_OpenShape(pcd)\n","    feat = feat.to(device)\n","\n","    output_feat = model.forward(feat)\n","\n","    tar2_feats.append(output_feat.detach().numpy())\n","\n","outfile = save_dir + \"/tar2_feats\"\n","np.save(outfile, np.array(tar2_feats))\n","print(\"Saved data to \" + outfile)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sZ1oZolje1Z8","executionInfo":{"status":"ok","timestamp":1708082199491,"user_tz":-60,"elapsed":2049380,"user":{"displayName":"Kristoffer Movik","userId":"14194010859010090109"}},"outputId":"7e37ebfa-f9ff-475e-9e03-ebf266655429"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2378it [34:08,  1.16it/s]\n"]}]},{"cell_type":"code","source":["old_train_feats = train_feats\n","old_src_feats = src_feats\n","old_tar1_feats = tar1_feats\n","old_tar2_feats = tar2_feats"],"metadata":{"id":"OIf20zNmFfOo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load np arrays and convert to torch tensors\n","\n","train_feats = np.load(\"OpenShape_outputs/train_feats.npy\")\n","src_feats = np.load(\"OpenShape_outputs/src_feats.npy\")\n","tar1_feats = np.load(\"OpenShape_outputs/tar1_feats.npy\")\n","tar2_feats = np.load(\"OpenShape_outputs/tar2_feats.npy\")\n","\n","train_feats = torch.from_numpy(train_feats)\n","src_feats = torch.from_numpy(src_feats)\n","tar1_feats = torch.from_numpy(tar1_feats)\n","tar2_feats = torch.from_numpy(tar2_feats)"],"metadata":{"id":"NCPg9vn0_yaR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example NumPy array and target number of tensors\n","x_elements = old_tar2_feats  # Create an array of 24 elements\n","y_tensors = 847  # Want to divide into 4 tensors\n","\n","# Ensure x_elements can be evenly divided into y_tensors\n","if len(x_elements) % y_tensors != 0:\n","    raise ValueError(\"The number of elements in the array must be divisible by y_tensors\")\n","\n","# Split the array into y parts\n","split_arrays = np.array_split(x_elements, y_tensors)\n","\n","# Convert each part into a tensor and stack them into a single tensor\n","tensors_list = [torch.tensor(part) for part in split_arrays]\n","final_tar2_feats = torch.stack(tensors_list)\n","\n","print(final_tar2_feats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cTb24cdVFuLn","executionInfo":{"status":"ok","timestamp":1708090161760,"user_tz":-60,"elapsed":251,"user":{"displayName":"Kristoffer Movik","userId":"14194010859010090109"}},"outputId":"71c00724-687b-40fb-a621-e1bc90ba1aea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-44.6219, -11.1272, -29.5261,  ..., -12.1809, -21.1658,  -6.4345],\n","        [-21.0046, -17.1524, -33.3568,  ..., -12.6945,  -5.2376,  -4.8490],\n","        [-37.6113,   4.8579, -13.5774,  ...,  -1.6024, -29.2849, -25.7459],\n","        ...,\n","        [-17.1690,  -4.1219, -31.9059,  ...,   2.7161,  -2.6888,  14.1641],\n","        [-32.7395,  -8.1993, -34.6582,  ..., -27.9810,  18.0565,   1.0996],\n","        [-20.5269, -13.4782, -68.9780,  ..., -35.9190, -10.1037,  35.2361]],\n","       dtype=torch.float64)\n"]}]},{"cell_type":"code","source":["train_feats = final_train_feats\n","src_feats = final_src_feats\n","tar1_feats = final_tar1_feats\n","tar2_feats = final_tar2_feats"],"metadata":{"id":"yOIb191NG52U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from knn_cuda import KNN\n","knn = KNN(k=1, transpose_mode=True)\n","\n","device = torch.device(\"cuda\")\n","train_feats = train_feats.to(device)\n","src_feats = src_feats.to(device)\n","tar1_feats = tar1_feats.to(device)\n","tar2_feats = tar2_feats.to(device)\n","\n","################################################\n","print(\"Euclidean distances in a non-normalized space:\")\n","# eucl distance in a non-normalized space\n","src_dist, src_ids = knn(train_feats.unsqueeze(0), src_feats.unsqueeze(0))\n","src_dist = src_dist.squeeze().cpu()\n","src_ids = src_ids.squeeze().cpu()  # index of nearest training sample\n","src_scores = 1 / src_dist\n","#src_pred = np.asarray([train_labels[i] for i in src_ids])  # pred is label of nearest training sample\n","\n","# OOD tar1\n","tar1_dist, _ = knn(train_feats.unsqueeze(0), tar1_feats.unsqueeze(0))\n","tar1_dist = tar1_dist.squeeze().cpu()\n","tar1_scores = 1 / tar1_dist\n","\n","# OOD tar2\n","tar2_dist, _ = knn(train_feats.unsqueeze(0), tar2_feats.unsqueeze(0))\n","tar2_dist = tar2_dist.squeeze().cpu()\n","tar2_scores = 1 / tar2_dist\n","\n","eval_ood_sncore(\n","    scores_list=[src_scores, tar1_scores, tar2_scores],\n","    preds_list=[None, None, None],  # [src_pred, None, None],\n","    labels_list=[None, None, None],  # [src_labels, None, None],\n","    src_label=1  # confidence should be higher for ID samples\n",")\n"],"metadata":{"id":"YX1QSDHwaUUC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708090447478,"user_tz":-60,"elapsed":369,"user":{"displayName":"Kristoffer Movik","userId":"14194010859010090109"}},"outputId":"a2402768-f268-49aa-e893-50b0cc555401"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Euclidean distances in a non-normalized space:\n","AUROC - Src label: 1, Tar label: 0\n","SRC->TAR1:      AUROC: 0.7093, FPR95: 0.8883, AUPR_IN: 0.8188, AUPR_OUT: 0.5500\n","SRC->TAR2:      AUROC: 0.7706, FPR95: 0.8383, AUPR_IN: 0.8525, AUPR_OUT: 0.6355\n","SRC->TAR1+TAR2: AUROC: 0.7410, FPR95: 0.8624, AUPR_IN: 0.7369, AUPR_OUT: 0.7438\n","to spreadsheet: 0.7092644649827088,0.8883248730964467,0.8188178037655336,0.549951277846648,0.7706087103769103,0.8382526564344747,0.8524633410968205,0.6354814382874194,0.7410434104560352,0.8623853211009175,0.7368651347487273,0.7437531223314934\n"]},{"output_type":"execute_result","data":{"text/plain":["(-1,\n"," -1,\n"," {'fpr_at_95_tpr': 0.8883248730964467,\n","  'detection_error': 0.5638243436430416,\n","  'auroc': 0.7092644649827088,\n","  'aupr_in': 0.8188178037655336,\n","  'aupr_out': 0.549951277846648},\n"," {'fpr_at_95_tpr': 0.8382526564344747,\n","  'detection_error': 0.5197436287962169,\n","  'auroc': 0.7706087103769103,\n","  'aupr_in': 0.8524633410968205,\n","  'aupr_out': 0.6354814382874194},\n"," {'fpr_at_95_tpr': 0.8623853211009175,\n","  'detection_error': 0.4015435670100146,\n","  'auroc': 0.7410434104560352,\n","  'aupr_in': 0.7368651347487273,\n","  'aupr_out': 0.7437531223314934})"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":[],"metadata":{"id":"DZRYXCU2L-Mv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Using SR2 as ID create dataloaders for train, ID, OOD1 and OOD2.\n","\n","The datasets we use are the same as the datasets used in 3DOS. We also use the code given in the 3DOS paper to create the dataloaders.\n","\n","\n","Citation:\n","\\@inproceedings{\n","alliegro2022towards,\n","title={Towards Open Set 3D Learning: Benchmarking and Understanding Semantic Novelty Detection on Pointclouds},\n","author={Antonio Alliegro and Francesco Cappio Borlino and Tatiana Tommasi},\n","booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n","year={2022},\n","url={https://openreview.net/forum?id=X2dHozbd1at}\n","}"],"metadata":{"id":"xml82DqdMFIf"}},{"cell_type":"code","source":["# Clone 3D_OS repo from github\n","!git clone https://github.com/antoalli/3D_OS.git\n","!cd 3D_OS && chmod +x download_data.sh && ./download_data.sh\n","!pip install h5py protobuf lmdb msgpack-numpy ninja scikit-learn"],"metadata":{"id":"mefZnt87MFIf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Change folder to 3D_OS to get the same dataloaders\n","%cd /content/3D_OS\n","\n","import sys\n","import os\n","import warnings\n","import numpy as np\n","\n","sys.path.append(os.getcwd())\n","import os.path as osp\n","import time\n","from torch.cuda.amp import GradScaler, autocast\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from utils.utils import *\n","from utils.dist import *\n","# noinspection PyUnresolvedReferences\n","from utils.data_utils import H5_Dataset\n","#from datasets.modelnet import *\n","from datasets.scanobject import *\n","from models.classifiers import Classifier\n","from utils.ood_utils import get_confidence, eval_ood_sncore, iterate_data_odin, \\\n","    iterate_data_energy, iterate_data_gradnorm, iterate_data_react, estimate_react_thres, print_ood_output, \\\n","    get_penultimate_feats, get_network_output\n","import wandb\n","from base_args import add_base_args\n","from sklearn.metrics import accuracy_score, balanced_accuracy_score\n","from models.common import convert_model_state, logits_entropy_loss\n","from models.ARPL_utils import Generator, Discriminator\n","from classifiers.common import train_epoch_cla, train_epoch_rsmix_exposure, train_epoch_cs\n","from classifiers.trainer_ddp_cla_md import get_args, load_yaml, get_md_eval_loaders\n","\n","# Code from 3DOS repo classifiers/trainer_ddp_cla_md.py line 192-478\n","\n","#  python -m torch.distributed.launch --nproc_per_node=1 Failure_Analysis/custom\\\n","# tests/evaluating_PointNet_cosine_MLS.py --config cfgs/pn2-msg.yaml\n","# --exp_name PN2_cosine_SR1 --src SR1 --loss cosine -mode eval\n","# --ckpt_path outputs/PN2_cosine_SR1/models/model_best.pth\n","\n","\n","# Code from 3DOS repo classifiers/trainer_ddp_cla_md.py line 468-478\n","def eval_ood_md2sonn(opt, config):\n","    print(f\"Arguments: {opt}\")\n","    set_random_seed(opt.seed)\n","\n","    dataloader_config = {\n","        'batch_size': opt.batch_size, 'drop_last': False, 'shuffle': False,\n","        'num_workers': opt.num_workers, 'sampler': None, 'worker_init_fn': init_np_seed}\n","\n","    # whole evaluation is done on ScanObject RW data\n","    sonn_args = {\n","        'data_root': opt.data_root,\n","        'sonn_split': opt.sonn_split,\n","        'h5_file': opt.sonn_h5_name,\n","        'split': 'all',  # we use both training (unused) and test samples during evaluation\n","        'num_points': opt.num_points_test,  # default: use all 2048 sonn points to avoid sampling randomicity\n","        'transforms': None  # no augmentation applied at inference time\n","    }\n","\n","    train_loader, _ = get_md_eval_loaders(opt)\n","    if opt.src == 'SR1':\n","        print(\"Src is SR1\\n\")\n","        id_loader = DataLoader(ScanObject(class_choice=\"sonn_2_mdSet1\", **sonn_args), **dataloader_config)\n","        ood1_loader = DataLoader(ScanObject(class_choice=\"sonn_2_mdSet2\", **sonn_args), **dataloader_config)\n","    elif opt.src == 'SR2':\n","        print(\"Src is SR2\\n\")\n","        id_loader = DataLoader(ScanObject(class_choice=\"sonn_2_mdSet2\", **sonn_args), **dataloader_config)\n","        ood1_loader = DataLoader(ScanObject(class_choice=\"sonn_2_mdSet1\", **sonn_args), **dataloader_config)\n","    else:\n","        raise ValueError(f\"OOD evaluation - wrong src: {opt.src}\")\n","\n","    ood2_loader = DataLoader(ScanObject(class_choice=\"sonn_ood_common\", **sonn_args), **dataloader_config)\n","\n","    return train_loader, id_loader, ood1_loader, ood2_loader\n","\n","class Options:\n","    def __init__(self, dictionary):\n","        for key, value in dictionary.items():\n","            setattr(self, key, value)\n","\n","opt_dict = {\n","    \"apply_fix_cellphone\": True,\n","    \"augm_set\": 'rw',\n","    \"batch_size\": 1,\n","    \"checkpoints_dir\": 'outputs',\n","    \"ckpt_path\": 'outputs/PN2_cosine_SR1/models/model_best.pth',\n","    \"config\": 'cfgs/pn2-msg.yaml',\n","    \"corruption\": None,\n","    \"cs\": False,\n","    \"cs_beta\": 0.1,\n","    \"cs_gan_lr\": 0.0002,\n","    \"data_root\": \"/content/3D_OS/3D_OS_release_data\",\n","    \"epochs\": 250,\n","    \"eval_step\": 1,\n","    \"exp_name\": 'PN2_cosine_SR1',\n","    \"grad_norm_clip\": -1,\n","    \"local_rank\": 0,\n","    \"loss\": 'cosine',\n","    \"num_points\": 10000,\n","    \"num_points_test\": 10000,\n","    \"num_workers\": 6,\n","    \"resume\": None,\n","    \"save_feats\": None,\n","    \"save_step\": 10,\n","    \"script_mode\": 'eval',\n","    \"seed\": 1,\n","    \"sonn_h5_name\": 'objectdataset.h5',\n","    \"sonn_split\": 'main_split',\n","    \"src\": 'SR2',\n","    \"tar1\": 'none',\n","    \"tar2\": 'none',\n","    \"use_amp\": False,\n","    \"use_sync_bn\": False,\n","    \"wandb_group\": 'md-2-sonn-augmCorr',\n","    \"wandb_name\": None,\n","    \"wandb_proj\": 'benchmark-3d-ood-cla'\n","}\n","\n","opt = Options(opt_dict)\n","\n","config = {'optimizer': {'type': 'adam',\n","                        'skip_wd': [],\n","                        'weight_decay': 0.0001,\n","                        'kwargs': {'lr': 0.001}\n","                        },\n","          'scheduler': {'type': 'CosLR',\n","                        'kwargs': {'t_initial': 250,\n","                                   'cycle_limit': 1,\n","                                   'lr_min': 1e-05\n","                                   }\n","                        },\n","          'model': {'ENCO_NAME': 'pn2-msg',\n","                    'dropout': 0.5,\n","                    'cla_input_dim': 1024,\n","                    'act': 'relu'\n","                    }\n","          }\n","\n","print(opt)\n","print(config)\n","\n","train_loader, src_loader, tar1_loader, tar2_loader = eval_ood_md2sonn(opt, config)\n","\n","%cd /content\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708091856700,"user_tz":-60,"elapsed":4248,"user":{"displayName":"Kristoffer Movik","userId":"14194010859010090109"}},"outputId":"0c115b1d-3d80-418b-c9bf-a3e3723a4751","id":"9cAnEBcAMFIm"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/3D_OS\n","<__main__.Options object at 0x7d88f38ddd50>\n","{'optimizer': {'type': 'adam', 'skip_wd': [], 'weight_decay': 0.0001, 'kwargs': {'lr': 0.001}}, 'scheduler': {'type': 'CosLR', 'kwargs': {'t_initial': 250, 'cycle_limit': 1, 'lr_min': 1e-05}}, 'model': {'ENCO_NAME': 'pn2-msg', 'dropout': 0.5, 'cla_input_dim': 1024, 'act': 'relu'}}\n","Arguments: <__main__.Options object at 0x7d88f38ddd50>\n","ModelNet40_OOD - Reading data from h5py file: /content/3D_OS/3D_OS_release_data/modelnet40_normal_resampled/ood_sets_cache/SR2_train.h5\n","ModelNet40_OOD - split: train, categories: {'bed': 0, 'toilet': 1, 'desk': 2, 'monitor': 3, 'table': 2}\n","SR2 train data len: 1916\n","ModelNet40_OOD - Reading data from h5py file: /content/3D_OS/3D_OS_release_data/modelnet40_normal_resampled/ood_sets_cache/SR2_test.h5\n","ModelNet40_OOD - split: test, categories: {'bed': 0, 'toilet': 1, 'desk': 2, 'monitor': 3, 'table': 2}\n","Src is SR2\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning:\n","\n","This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"]},{"output_type":"stream","name":"stdout","text":["ScanObject - num_points: 10000, sonn_split: main_split, h5_suffix: objectdataset.h5, split: all, class_choice: {10: 0, 14: 1, 5: 2, 6: 3, 9: 2}, num samples: 788\n","ScanObject - num_points: 10000, sonn_split: main_split, h5_suffix: objectdataset.h5, split: all, class_choice: {4: 0, 8: 1, 7: 2, 12: 3, 13: 4}, num samples: 1255\n","ScanObject - num_points: 10000, sonn_split: main_split, h5_suffix: objectdataset.h5, split: all, class_choice: {0: 404, 1: 404, 2: 404, 3: 404, 11: 404}, num samples: 847\n","/content\n"]}]},{"cell_type":"markdown","source":["# Run data through the OpenShape model"],"metadata":{"id":"Vgi9h1EMNLlG"}},{"cell_type":"code","source":["# Run data through OpenShape\n","from tqdm import tqdm\n","device = torch.device(\"cpu\")\n","model = model.to(device)\n","\n","%cd /content\n","save_dir = \"OpenShape_outputs/SR2\"\n","if not os.path.exists(save_dir):\n","    print(f\"Creating directory: {save_dir}\")\n","    os.makedirs(save_dir)\n","\n","print(\"Running training data through the OpenShape model\")\n","train_feats = []\n","for i, (data, label) in tqdm(enumerate(train_loader)):\n","    points = data\n","    points = points.reshape(-1, 3)\n","    pcd = o3d.geometry.PointCloud()\n","    pcd.points = o3d.utility.Vector3dVector(points)\n","\n","    xyz, feat = filter_for_OpenShape(pcd)\n","    feat = feat.to(device)\n","\n","    output_feat = model.forward(feat)\n","\n","    train_feats.append(output_feat.detach().numpy())\n","\n","outfile = save_dir + \"/train_feats.npy\"\n","np.save(outfile, np.array(train_feats))\n","print(\"Saved data to \" + outfile)"],"metadata":{"id":"FSnsoYXAVnBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","print(\"Running ID data through the OpenShape model\")\n","src_feats = []\n","for i, (data, label) in tqdm(enumerate(src_loader)):\n","    points = data\n","    points = points.reshape(-1, 3)\n","    pcd = o3d.geometry.PointCloud()\n","    pcd.points = o3d.utility.Vector3dVector(points)\n","\n","    xyz, feat = filter_for_OpenShape(pcd)\n","    feat = feat.to(device)\n","\n","    output_feat = model.forward(feat)\n","\n","    src_feats.append(output_feat.detach().numpy())\n","\n","outfile = save_dir + \"/src_feats\"\n","np.save(outfile, np.array(src_feats))\n","print(\"Saved data to \" + outfile)\n","\n","print(\"Rinning OOD1 data throug the OpenShape model\")\n","tar1_feats = []\n","for i, (data, label) in tqdm(enumerate(tar1_loader)):\n","    points = data\n","    points = points.reshape(-1, 3)\n","    pcd = o3d.geometry.PointCloud()\n","    pcd.points = o3d.utility.Vector3dVector(points)\n","\n","    xyz, feat = filter_for_OpenShape(pcd)\n","    feat = feat.to(device)\n","\n","    output_feat = model.forward(feat)\n","\n","    tar1_feats.append(output_feat.detach().numpy())\n","\n","outfile = save_dir + \"/tar1_feats\"\n","np.save(outfile, np.array(tar1_feats))\n","print(\"Saved data to \" + outfile)\n","\n","print(\"Rinning OOD2 data throug the OpenShape model\")\n","tar2_feats = []\n","for i, (data, label) in tqdm(enumerate(tar2_loader)):\n","    points = data\n","    points = points.reshape(-1, 3)\n","    pcd = o3d.geometry.PointCloud()\n","    pcd.points = o3d.utility.Vector3dVector(points)\n","\n","    xyz, feat = filter_for_OpenShape(pcd)\n","    feat = feat.to(device)\n","\n","    output_feat = model.forward(feat)\n","\n","    tar2_feats.append(output_feat.detach().numpy())\n","\n","outfile = save_dir + \"/tar2_feats\"\n","np.save(outfile, np.array(tar2_feats))\n","print(\"Saved data to \" + outfile)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708096574870,"user_tz":-60,"elapsed":802385,"user":{"displayName":"Kristoffer Movik","userId":"14194010859010090109"}},"outputId":"9be162b6-fd09-4991-f23a-81fd0fe32a0b","id":"YhueWBRCNLlN"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Running ID data through the OpenShape model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning:\n","\n","This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n","788it [11:18,  1.16it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Saved data to OpenShape_outputs/SR2/src_feats\n","Rinning OOD1 data throug the OpenShape model\n"]},{"output_type":"stream","name":"stderr","text":["\n","1255it [18:35,  1.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Saved data to OpenShape_outputs/SR2/tar1_feats\n","Rinning OOD2 data throug the OpenShape model\n"]},{"output_type":"stream","name":"stderr","text":["\n","847it [12:20,  1.14it/s]"]},{"output_type":"stream","name":"stdout","text":["Saved data to OpenShape_outputs/SR2/tar2_feats\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# Load np arrays and convert to torch tensors\n","\n","train_feats = np.load(\"OpenShape_outputs/SR2/train_feats.npy\")\n","src_feats = np.load(\"OpenShape_outputs/SR2/src_feats.npy\")\n","tar1_feats = np.load(\"OpenShape_outputs/SR2/tar1_feats.npy\")\n","tar2_feats = np.load(\"OpenShape_outputs/SR2/tar2_feats.npy\")\n","\n","train_feats = np.squeeze(train_feats, axis=1)\n","src_feats = np.squeeze(src_feats, axis=1)\n","tar1_feats = np.squeeze(tar1_feats, axis=1)\n","tar2_feats = np.squeeze(tar2_feats, axis=1)\n","\n","train_feats = torch.from_numpy(train_feats)\n","src_feats = torch.from_numpy(src_feats)\n","tar1_feats = torch.from_numpy(tar1_feats)\n","tar2_feats = torch.from_numpy(tar2_feats)"],"metadata":{"id":"gRPrhaGGNLlO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from knn_cuda import KNN\n","knn = KNN(k=1, transpose_mode=True)\n","\n","device = torch.device(\"cuda\")\n","train_feats = train_feats.to(device)\n","src_feats = src_feats.to(device)\n","tar1_feats = tar1_feats.to(device)\n","tar2_feats = tar2_feats.to(device)\n","\n","################################################\n","print(\"Euclidean distances in a non-normalized space:\")\n","# eucl distance in a non-normalized space\n","src_dist, src_ids = knn(train_feats.unsqueeze(0), src_feats.unsqueeze(0))\n","src_dist = src_dist.squeeze().cpu()\n","src_ids = src_ids.squeeze().cpu()  # index of nearest training sample\n","src_scores = 1 / src_dist\n","#src_pred = np.asarray([train_labels[i] for i in src_ids])  # pred is label of nearest training sample\n","\n","# OOD tar1\n","tar1_dist, _ = knn(train_feats.unsqueeze(0), tar1_feats.unsqueeze(0))\n","tar1_dist = tar1_dist.squeeze().cpu()\n","tar1_scores = 1 / tar1_dist\n","\n","# OOD tar2\n","tar2_dist, _ = knn(train_feats.unsqueeze(0), tar2_feats.unsqueeze(0))\n","tar2_dist = tar2_dist.squeeze().cpu()\n","tar2_scores = 1 / tar2_dist\n","\n","eval_ood_sncore(\n","    scores_list=[src_scores, tar1_scores, tar2_scores],\n","    preds_list=[None, None, None],  # [src_pred, None, None],\n","    labels_list=[None, None, None],  # [src_labels, None, None],\n","    src_label=1  # confidence should be higher for ID samples\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708097899622,"user_tz":-60,"elapsed":533,"user":{"displayName":"Kristoffer Movik","userId":"14194010859010090109"}},"outputId":"4b0be8a6-163c-454a-aeb5-e59ad8524552","id":"-phAca6dNLlO"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Euclidean distances in a non-normalized space:\n","AUROC - Src label: 1, Tar label: 0\n","SRC->TAR1:      AUROC: 0.6373, FPR95: 0.8032, AUPR_IN: 0.4992, AUPR_OUT: 0.7571\n","SRC->TAR2:      AUROC: 0.5183, FPR95: 0.9457, AUPR_IN: 0.5220, AUPR_OUT: 0.5336\n","SRC->TAR1+TAR2: AUROC: 0.5893, FPR95: 0.8606, AUPR_IN: 0.3466, AUPR_OUT: 0.8031\n","to spreadsheet: 0.6372873986288349,0.803187250996016,0.49922345612929653,0.7570801190469467,0.5182774078713166,0.9456906729634003,0.5219628685531112,0.5336026283680585,0.5893323738088453,0.8606089438629876,0.3465921559420079,0.8031033611301948\n"]},{"output_type":"execute_result","data":{"text/plain":["(-1,\n"," -1,\n"," {'fpr_at_95_tpr': 0.803187250996016,\n","  'detection_error': 0.33731978323428935,\n","  'auroc': 0.6372873986288349,\n","  'aupr_in': 0.49922345612929653,\n","  'aupr_out': 0.7570801190469467},\n"," {'fpr_at_95_tpr': 0.9456906729634003,\n","  'detection_error': 0.47496332993193746,\n","  'auroc': 0.5182774078713166,\n","  'aupr_in': 0.5219628685531112,\n","  'aupr_out': 0.5336026283680585},\n"," {'fpr_at_95_tpr': 0.8606089438629876,\n","  'detection_error': 0.2577222190311235,\n","  'auroc': 0.5893323738088453,\n","  'aupr_in': 0.3465921559420079,\n","  'aupr_out': 0.8031033611301948})"]},"metadata":{},"execution_count":87}]},{"cell_type":"code","source":["# Run this command to save data to google drive\n","#!cp -r /content/OpenShape_outputs \"/content/drive/MyDrive/Skole/Advanced machine learning project\""],"metadata":{"id":"yAAs_TCUNLlO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"nBNqsX-cZhXv"}}]}