{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5828a78273940fba3e4862d709f1d11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_045654e8d3ca46b983a289707c9a941d",
              "IPY_MODEL_42562c17ea29431c8f591cc536921758",
              "IPY_MODEL_5f0d8729f08f4670b176c1c4689e588e",
              "IPY_MODEL_6e2f5b98a3b2464c8641ce44a294dfdf",
              "IPY_MODEL_fc5e9a5c74184a34a0a119cbbca2fc01"
            ],
            "layout": "IPY_MODEL_a92e5f0341b7427583a99c4fdcecc9f3",
            "tabbable": null,
            "tooltip": null
          }
        },
        "045654e8d3ca46b983a289707c9a941d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_1d6c5c93f0e94597beb85e7e8dc07710",
            "placeholder": "​",
            "style": "IPY_MODEL_095b3c66bdc040d4a4a3d61ff38da815",
            "tabbable": null,
            "tooltip": null,
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "42562c17ea29431c8f591cc536921758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_allow_html": false,
            "disabled": false,
            "layout": "IPY_MODEL_adca324689894f74bbb0f06b1f2ddc18",
            "placeholder": "​",
            "style": "IPY_MODEL_8fe6b9b103724505b609e22c7b42ea3b",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "5f0d8729f08f4670b176c1c4689e588e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_allow_html": false,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_9c064f25fe5e465d8061f478477bebe1",
            "style": "IPY_MODEL_ec527e5808b547e6b08aea885be06f95",
            "tabbable": null,
            "tooltip": null,
            "value": true
          }
        },
        "6e2f5b98a3b2464c8641ce44a294dfdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_0deb251e82c143b58263e332580212d9",
            "style": "IPY_MODEL_f2a2f1baa4af4978aad9ae7537298021",
            "tabbable": null,
            "tooltip": null
          }
        },
        "fc5e9a5c74184a34a0a119cbbca2fc01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_dcb48975d67a40c3b424aa2e84c351e3",
            "placeholder": "​",
            "style": "IPY_MODEL_96dadd367afe49b29f6af0ce4a221f3a",
            "tabbable": null,
            "tooltip": null,
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "a92e5f0341b7427583a99c4fdcecc9f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "1d6c5c93f0e94597beb85e7e8dc07710": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "095b3c66bdc040d4a4a3d61ff38da815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "adca324689894f74bbb0f06b1f2ddc18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fe6b9b103724505b609e22c7b42ea3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "TextStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "9c064f25fe5e465d8061f478477bebe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec527e5808b547e6b08aea885be06f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "CheckboxStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": ""
          }
        },
        "0deb251e82c143b58263e332580212d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2a2f1baa4af4978aad9ae7537298021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_family": null,
            "font_size": null,
            "font_style": null,
            "font_variant": null,
            "font_weight": null,
            "text_color": null,
            "text_decoration": null
          }
        },
        "dcb48975d67a40c3b424aa2e84c351e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96dadd367afe49b29f6af0ce4a221f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "99340a3daa0e491bbb153f5a4466012a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f27bd7214b0f4e8f8c622daa81a4809b",
              "IPY_MODEL_00ee3617b4bc49fa940775fe990380f2",
              "IPY_MODEL_d83ee825067e4f3b9985e895c6f5e2b1"
            ],
            "layout": "IPY_MODEL_2572fce4846b4003a4998c5e11ea59e2",
            "tabbable": null,
            "tooltip": null
          }
        },
        "f27bd7214b0f4e8f8c622daa81a4809b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_29e1287d3d5e4f79bf893667c30837fb",
            "placeholder": "​",
            "style": "IPY_MODEL_dc49543f2dad45e988130587e4d859fe",
            "tabbable": null,
            "tooltip": null,
            "value": "model.pt: 100%"
          }
        },
        "00ee3617b4bc49fa940775fe990380f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_cfc363b629c24381aaebd5d041807de1",
            "max": 388091433,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_708530d93bd6424ab3d75c7ec7c15503",
            "tabbable": null,
            "tooltip": null,
            "value": 388091433
          }
        },
        "d83ee825067e4f3b9985e895c6f5e2b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_38017bb4901b41d5b09a5fa1ac242a00",
            "placeholder": "​",
            "style": "IPY_MODEL_6bd7c580818c4d8ea6738ce87cda02c6",
            "tabbable": null,
            "tooltip": null,
            "value": " 388M/388M [00:03&lt;00:00, 134MB/s]"
          }
        },
        "2572fce4846b4003a4998c5e11ea59e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29e1287d3d5e4f79bf893667c30837fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc49543f2dad45e988130587e4d859fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "cfc363b629c24381aaebd5d041807de1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "708530d93bd6424ab3d75c7ec7c15503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38017bb4901b41d5b09a5fa1ac242a00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bd7c580818c4d8ea6738ce87cda02c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "d3cbebfc3fc54c11a5bd3d35f53dba8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a163e980b6a84edf9bc5fb9dfbb12a69",
              "IPY_MODEL_1e0c59f9af3d486cbdef015b2ae11c83",
              "IPY_MODEL_be70707ba6884e5887d64b7a3aea0bc0"
            ],
            "layout": "IPY_MODEL_80d9f55f23f3407ba61bcacb9156f67a",
            "tabbable": null,
            "tooltip": null
          }
        },
        "a163e980b6a84edf9bc5fb9dfbb12a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_49925e54fb08444b8f39fe2c8f4f5677",
            "placeholder": "​",
            "style": "IPY_MODEL_211c09a3f15949dab074792ce58e6c1f",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "1e0c59f9af3d486cbdef015b2ae11c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_7fe3b63a07384c54b3e91eab75da05ee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf273efeb4384d84b8a740c56b19351c",
            "tabbable": null,
            "tooltip": null,
            "value": 0
          }
        },
        "be70707ba6884e5887d64b7a3aea0bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_41250e1bbcbe4d658da7eefcdf110384",
            "placeholder": "​",
            "style": "IPY_MODEL_e226d37980fd4f79ac427033e78126eb",
            "tabbable": null,
            "tooltip": null,
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "80d9f55f23f3407ba61bcacb9156f67a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49925e54fb08444b8f39fe2c8f4f5677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "211c09a3f15949dab074792ce58e6c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "7fe3b63a07384c54b3e91eab75da05ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bf273efeb4384d84b8a740c56b19351c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41250e1bbcbe4d658da7eefcdf110384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e226d37980fd4f79ac427033e78126eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook for the extension of the project\n",
        "\n",
        "This note book is the extension part of the project of the cource Advanced Machine Learning teached at Politecnico di Torino.\n",
        "\n",
        "In this notebook we create a simulated a real-life application of quality control in a production line. The products that are in distribution are products that are produced correctly. While products that are out of distribution is assumed to be defect.\n",
        "\n",
        "We used code and data from the 3DOS paper and the OpenShape paper\n",
        "\n",
        "Open shape:\n",
        "@misc{liu2023openshape,\n",
        "      title={OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding},\n",
        "      author={Minghua Liu and Ruoxi Shi and Kaiming Kuang and Yinhao Zhu and Xuanlin Li and Shizhong Han and Hong Cai and Fatih Porikli and Hao Su},\n",
        "      year={2023},\n",
        "      eprint={2305.10764},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.CV}\n",
        "}\n",
        "\n",
        "3DOS:\n",
        "@inproceedings{\n",
        "alliegro2022towards,\n",
        "title={Towards Open Set 3D Learning: Benchmarking and Understanding Semantic Novelty Detection on Pointclouds},\n",
        "author={Antonio Alliegro and Francesco Cappio Borlino and Tatiana Tommasi},\n",
        "booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n",
        "year={2022},\n",
        "url={https://openreview.net/forum?id=X2dHozbd1at}\n",
        "}"
      ],
      "metadata": {
        "id": "9teFaLRdlp5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone git repos to environment"
      ],
      "metadata": {
        "id": "W_I1PHBrhadT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git clone https://github.com/Colin97/OpenShape_code.git\n",
        "# Make sure you have git-lfs installed (https://git-lfs.com)\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/spaces/OpenShape/openshape-demo\n",
        "\n",
        "# if you want to clone without large files – just their pointers\n",
        "# prepend your git clone with the following env var:\n",
        "!GIT_LFS_SKIP_SMUDGE=1\n",
        "#!pip install -e .\n",
        "!git clone https://huggingface.co/OpenShape/openshape-demo-support\n"
      ],
      "metadata": {
        "id": "i2PTprijZ5vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcdb429a-6b5d-458c-d79b-aba3189b18e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OpenShape_code'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 119 (delta 31), reused 12 (delta 12), pack-reused 80\u001b[K\n",
            "Receiving objects: 100% (119/119), 3.45 MiB | 8.73 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "Git LFS initialized.\n",
            "Cloning into 'openshape-demo'...\n",
            "remote: Enumerating objects: 188, done.\u001b[K\n",
            "remote: Total 188 (delta 0), reused 0 (delta 0), pack-reused 188\u001b[K\n",
            "Receiving objects: 100% (188/188), 1.43 MiB | 7.26 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "Cloning into 'openshape-demo-support'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 102 (delta 9), reused 0 (delta 0), pack-reused 83\u001b[K\n",
            "Receiving objects: 100% (102/102), 28.53 KiB | 14.26 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies\n",
        "Most dependencies are already satisfied by default in google colab"
      ],
      "metadata": {
        "id": "Jw-TaS50jZlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub wandb omegaconf torch_redstone einops tqdm open3d dgl timm\n",
        "!pip install --upgrade https://github.com/unlimblue/KNN_CUDA/releases/download/0.2/KNN_CUDA-0.2-py3-none-any.whl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4PSbBCbC26vP",
        "outputId": "cd91ab09-a3c9-4bcb-dc7b-29208fb2ace4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_redstone\n",
            "  Downloading torch.redstone-0.0.5-py3-none-any.whl (18 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Collecting open3d\n",
            "  Downloading open3d-0.18.0-cp310-cp310-manylinux_2_27_x86_64.whl (399.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dgl\n",
            "  Downloading dgl-1.1.3-cp310-cp310-manylinux1_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm\n",
            "  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.40.4-py2.py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.9/257.9 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_redstone) (1.25.2)\n",
            "Collecting dash>=2.6.0 (from open3d)\n",
            "  Downloading dash-2.15.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.0.1)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.9.2)\n",
            "Collecting configargparse (from open3d)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d)\n",
            "  Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from open3d)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (9.4.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.2.2)\n",
            "Collecting pyquaternion (from open3d)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2.1)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.2.5)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.15.0)\n",
            "Collecting dash-html-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-table==5.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Collecting retrying (from dash>=2.6.0->open3d)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (7.0.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading comm-0.2.1-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.10 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyterlab-widgets~=3.0.10 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.10)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (3.2.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=2.2.3->open3d) (2.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.1.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.17.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (8.2.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.17.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat>=5.7.0->open3d) (4.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=1c28b6dead1b3ecca489f91684dfe8df22d59e465e6d9fb4dd065124228e8583\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, antlr4-python3-runtime, addict, widgetsnbextension, torch_redstone, smmap, setproctitle, sentry-sdk, retrying, pyquaternion, omegaconf, jedi, einops, docker-pycreds, configargparse, comm, gitdb, dgl, ipywidgets, GitPython, dash, wandb, timm, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.6\n",
            "    Uninstalling widgetsnbextension-3.6.6:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.6\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed GitPython-3.1.42 addict-2.4.0 antlr4-python3-runtime-4.9.3 comm-0.2.1 configargparse-1.7 dash-2.15.0 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 dgl-1.1.3 docker-pycreds-0.4.0 einops-0.7.0 gitdb-4.0.11 ipywidgets-8.1.2 jedi-0.19.1 omegaconf-2.3.0 open3d-0.18.0 pyquaternion-0.9.9 retrying-1.3.4 sentry-sdk-1.40.4 setproctitle-1.3.3 smmap-5.0.1 timm-0.9.12 torch_redstone-0.0.5 wandb-0.16.3 widgetsnbextension-4.0.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting KNN-CUDA==0.2\n",
            "  Downloading https://github.com/unlimblue/KNN_CUDA/releases/download/0.2/KNN_CUDA-0.2-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from KNN-CUDA==0.2) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from KNN-CUDA==0.2) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from KNN-CUDA==0.2) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (2.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->KNN-CUDA==0.2) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->KNN-CUDA==0.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->KNN-CUDA==0.2) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->KNN-CUDA==0.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->KNN-CUDA==0.2) (1.3.0)\n",
            "Installing collected packages: KNN-CUDA\n",
            "Successfully installed KNN-CUDA-0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log in to Huggingface"
      ],
      "metadata": {
        "id": "AWm2jCYYxBhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "3TLMaekmxBPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "nmYWjzhrZxF6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "b5828a78273940fba3e4862d709f1d11",
            "045654e8d3ca46b983a289707c9a941d",
            "42562c17ea29431c8f591cc536921758",
            "5f0d8729f08f4670b176c1c4689e588e",
            "6e2f5b98a3b2464c8641ce44a294dfdf",
            "fc5e9a5c74184a34a0a119cbbca2fc01",
            "a92e5f0341b7427583a99c4fdcecc9f3",
            "1d6c5c93f0e94597beb85e7e8dc07710",
            "095b3c66bdc040d4a4a3d61ff38da815",
            "adca324689894f74bbb0f06b1f2ddc18",
            "8fe6b9b103724505b609e22c7b42ea3b",
            "9c064f25fe5e465d8061f478477bebe1",
            "ec527e5808b547e6b08aea885be06f95",
            "0deb251e82c143b58263e332580212d9",
            "f2a2f1baa4af4978aad9ae7537298021",
            "dcb48975d67a40c3b424aa2e84c351e3",
            "96dadd367afe49b29f6af0ce4a221f3a"
          ]
        },
        "outputId": "0ede8ad4-83d2-453e-89c1-59e7cff003c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5828a78273940fba3e4862d709f1d11"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the OpenShape model from Huggingface"
      ],
      "metadata": {
        "id": "EmFurYFiyCSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/openshape-demo-support\n",
        "!pip install -e .\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "_-UugGsGiumt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27f836f-9990-4bec-b1e3-7afbb29394ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/openshape-demo-support\n",
            "Obtaining file:///content/openshape-demo-support\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: openshape\n",
            "  Running setup.py develop for openshape\n",
            "Successfully installed openshape-0.1\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openshape\n",
        "model = openshape.load_pc_encoder('openshape-pointbert-vitg14-rgb')\n",
        "\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "21n-SAviizUw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "99340a3daa0e491bbb153f5a4466012a",
            "f27bd7214b0f4e8f8c622daa81a4809b",
            "00ee3617b4bc49fa940775fe990380f2",
            "d83ee825067e4f3b9985e895c6f5e2b1",
            "2572fce4846b4003a4998c5e11ea59e2",
            "29e1287d3d5e4f79bf893667c30837fb",
            "dc49543f2dad45e988130587e4d859fe",
            "cfc363b629c24381aaebd5d041807de1",
            "708530d93bd6424ab3d75c7ec7c15503",
            "38017bb4901b41d5b09a5fa1ac242a00",
            "6bd7c580818c4d8ea6738ce87cda02c6"
          ]
        },
        "outputId": "4446be60-6356-477e-e7ac-770d7a40ea5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.pt:   0%|          | 0.00/388M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99340a3daa0e491bbb153f5a4466012a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Projected(\n",
              "  (ppat): PointPatchTransformer(\n",
              "    (sa): PointNetSetAbstraction(\n",
              "      (mlp_convs): ModuleList(\n",
              "        (0): Conv2d(9, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (mlp_bns): ModuleList(\n",
              "        (0-1): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (lift): Sequential(\n",
              "      (0): Conv1d(259, 512, kernel_size=(1,), stride=(1,))\n",
              "      (1): Lambda()\n",
              "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x ModuleList(\n",
              "          (0): PreNorm(\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (fn): Attention(\n",
              "              (attend): Softmax(dim=-1)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "              (to_out): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=1536, bias=True)\n",
              "                (1): GELU(approximate='none')\n",
              "                (2): Dropout(p=0.0, inplace=False)\n",
              "                (3): Linear(in_features=1536, out_features=512, bias=True)\n",
              "                (4): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (proj): Linear(in_features=512, out_features=1280, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for fitting pointclouds to input dimentions for OpenShape"
      ],
      "metadata": {
        "id": "W0YW3xqHyJDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import open3d as o3d\n",
        "import random\n",
        "import torch\n",
        "from OpenShape_code.src.utils.data import normalize_pc\n",
        "\n",
        "def load_ply(file_name, num_points=10000, y_up=True):\n",
        "    pcd = o3d.io.read_point_cloud(file_name)  # Read the point cloud\n",
        "    xyz = np.asarray(pcd.points)  # Get xyz coordinates\n",
        "    rgb = np.asarray(pcd.colors)  # Get rgb colors\n",
        "    n = xyz.shape[0]\n",
        "\n",
        "    # Sample num_points points if necessary\n",
        "    if n > num_points:\n",
        "        idx = random.sample(range(n), num_points)\n",
        "        xyz = xyz[idx]\n",
        "        rgb = rgb[idx]\n",
        "    elif n < num_points:\n",
        "        print(f\"Warning: requested {num_points} points, but file has only {n} points.\", file=sys.stderr)\n",
        "\n",
        "    # Adjust orientation by swapping y and z if requested\n",
        "    if y_up:\n",
        "        xyz[:, [1, 2]] = xyz[:, [2, 1]]\n",
        "\n",
        "    # Normalize the point cloud coordinates\n",
        "    xyz_normalized = normalize_pc(xyz)\n",
        "\n",
        "    # Handle cases where rgb might be missing\n",
        "    if rgb.size == 0:\n",
        "        rgb = np.ones_like(xyz_normalized) * 0.4  # Default to a constant color if missing\n",
        "\n",
        "    # Concatenate xyz with rgb\n",
        "    features = np.concatenate([xyz_normalized, rgb], axis=1)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    xyz_tensor = torch.from_numpy(xyz_normalized).float()\n",
        "    features_tensor = torch.from_numpy(features).float()\n",
        "\n",
        "    # Add batch dimention to fit as single input to OpenShape model\n",
        "    features_tensor = features_tensor.unsqueeze(0)  # Adds a batch dimension, making it [1, N, 6]\n",
        "    features_tensor = features_tensor.transpose(1, 2)  # Transposes to get [1, 6, N], matching the expected [B, C, N] format\n",
        "\n",
        "    # Returning tensors instead of ME-specific batched coordinates\n",
        "    return xyz_tensor, features_tensor\n",
        "\n",
        "def filter_for_OpenShape(pointcloud, num_points=10000, y_up=True):\n",
        "    xyz = np.asarray(pointcloud.points)  # Get xyz coordinates\n",
        "    rgb = np.asarray(pointcloud.colors)  # Get rgb colors\n",
        "    n = xyz.shape[0]\n",
        "\n",
        "    # Sample num_points points if necessary\n",
        "    if n > num_points:\n",
        "        idx = random.sample(range(n), num_points)\n",
        "        xyz = xyz[idx]\n",
        "        #rgb = rgb[idx]\n",
        "    elif n < num_points:\n",
        "        print(f\"Warning: requested {num_points} points, but file has only {n} points.\", file=sys.stderr)\n",
        "\n",
        "    # Adjust orientation by swapping y and z if requested\n",
        "    if y_up:\n",
        "        xyz[:, [1, 2]] = xyz[:, [2, 1]]\n",
        "\n",
        "    # Normalize the point cloud coordinates\n",
        "    xyz_normalized = normalize_pc(xyz)\n",
        "\n",
        "    # Handle cases where rgb might be missing\n",
        "    if rgb.size == 0:\n",
        "        rgb = np.ones_like(xyz_normalized) * 0.4  # Default to a constant color if missing\n",
        "\n",
        "    # Concatenate xyz with rgb\n",
        "    features = np.concatenate([xyz_normalized, rgb], axis=1)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    xyz_tensor = torch.from_numpy(xyz_normalized).float()\n",
        "    features_tensor = torch.from_numpy(features).float()\n",
        "\n",
        "    # Add batch dimention to fit as single input to OpenShape model\n",
        "    features_tensor = features_tensor.unsqueeze(0)  # Adds a batch dimension, making it [1, N, 6]\n",
        "    features_tensor = features_tensor.transpose(1, 2)  # Transposes to get [1, 6, N], matching the expected [B, C, N] format\n",
        "\n",
        "    # Returning tensors instead of ME-specific batched coordinates\n",
        "    return xyz_tensor, features_tensor\n"
      ],
      "metadata": {
        "id": "_LyfD_SHnKEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test that installation and environment works\n",
        "\n",
        "If a feature vector is printed it should all be A OK!"
      ],
      "metadata": {
        "id": "tWsXrNENL40j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xyz, feat = load_ply(\"/content/OpenShape_code/demo/owl.ply\")\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "feat = feat.to(device)\n",
        "\n",
        "output = model.forward(feat)\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "TqztyB8JnQVn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92,
          "referenced_widgets": [
            "d3cbebfc3fc54c11a5bd3d35f53dba8d",
            "a163e980b6a84edf9bc5fb9dfbb12a69",
            "1e0c59f9af3d486cbdef015b2ae11c83",
            "be70707ba6884e5887d64b7a3aea0bc0",
            "80d9f55f23f3407ba61bcacb9156f67a",
            "49925e54fb08444b8f39fe2c8f4f5677",
            "211c09a3f15949dab074792ce58e6c1f",
            "7fe3b63a07384c54b3e91eab75da05ee",
            "bf273efeb4384d84b8a740c56b19351c",
            "41250e1bbcbe4d658da7eefcdf110384",
            "e226d37980fd4f79ac427033e78126eb"
          ]
        },
        "outputId": "e8716600-b0e3-4606-ba49-963073462171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3cbebfc3fc54c11a5bd3d35f53dba8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-59.1732, -47.6009, -24.0886,  ..., -29.0734,  51.5984, -14.3870]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone 3DOS repo"
      ],
      "metadata": {
        "id": "_90Y_MbTMe84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone 3D_OS repo from github\n",
        "%cd /content\n",
        "!git clone https://github.com/antoalli/3D_OS.git\n",
        "!cd 3D_OS && chmod +x download_data.sh && ./download_data.sh\n",
        "!pip install h5py protobuf lmdb msgpack-numpy ninja scikit-learn"
      ],
      "metadata": {
        "id": "UaL5_GSiTzpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701105f7-82db-4a6d-c7ae-a70e5cb38f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into '3D_OS'...\n",
            "remote: Enumerating objects: 152, done.\u001b[K\n",
            "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
            "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
            "remote: Total 152 (delta 51), reused 142 (delta 44), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (152/152), 129.35 KiB | 995.00 KiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Downloading data in /content/3D_OS/3D_OS_release_data\n",
            "============Downloading ShapeNetCore resampled in \n",
            "--2024-02-18 15:56:31--  https://www.dropbox.com/s/oa3qbujpugw4d43/sncore_fps_4096.tar?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6021:18::a27d:4112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/oa3qbujpugw4d43/sncore_fps_4096.tar [following]\n",
            "--2024-02-18 15:56:32--  https://www.dropbox.com/s/dl/oa3qbujpugw4d43/sncore_fps_4096.tar\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce8fa813fb20d0f3f9f70e3bf03.dl-eu.dropboxusercontent.com/cd/0/get/CNhaeDD-YkqZQsFmYpnpuPNyYgkUJ1sQwqBLfg8DBWMTOPncxUGZ636xPbRU_SQCMPAutTI-bj-Kf123R75apsUFTrVqqc8GJ07CL2sUEbZEikMpnc_CrtgLhNF1UJgp_plumctrIJ35Q8AoY0g_H06L/file?dl=1# [following]\n",
            "--2024-02-18 15:56:33--  https://uce8fa813fb20d0f3f9f70e3bf03.dl-eu.dropboxusercontent.com/cd/0/get/CNhaeDD-YkqZQsFmYpnpuPNyYgkUJ1sQwqBLfg8DBWMTOPncxUGZ636xPbRU_SQCMPAutTI-bj-Kf123R75apsUFTrVqqc8GJ07CL2sUEbZEikMpnc_CrtgLhNF1UJgp_plumctrIJ35Q8AoY0g_H06L/file?dl=1\n",
            "Resolving uce8fa813fb20d0f3f9f70e3bf03.dl-eu.dropboxusercontent.com (uce8fa813fb20d0f3f9f70e3bf03.dl-eu.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uce8fa813fb20d0f3f9f70e3bf03.dl-eu.dropboxusercontent.com (uce8fa813fb20d0f3f9f70e3bf03.dl-eu.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5037998080 (4.7G) [application/binary]\n",
            "Saving to: ‘/content/3D_OS/3D_OS_release_data/tmp_sncore_fps_4096.tar’\n",
            "\n",
            "/content/3D_OS/3D_O 100%[===================>]   4.69G  17.5MB/s    in 4m 12s  \n",
            "\n",
            "2024-02-18 16:00:46 (19.0 MB/s) - ‘/content/3D_OS/3D_OS_release_data/tmp_sncore_fps_4096.tar’ saved [5037998080/5037998080]\n",
            "\n",
            "============\n",
            "============Downloading ModelNet40 + OOD Splits in \n",
            "--2024-02-18 16:01:22--  https://www.dropbox.com/s/c2x3h59nxprjs21/modelnet40_normal_resampled.tar?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6021:18::a27d:4112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/c2x3h59nxprjs21/modelnet40_normal_resampled.tar [following]\n",
            "--2024-02-18 16:01:22--  https://www.dropbox.com/s/dl/c2x3h59nxprjs21/modelnet40_normal_resampled.tar\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc5b61147a20f08abfd649be845f.dl-eu.dropboxusercontent.com/cd/0/get/CNiS_P9fP1OV_57u-6OY-Qez-x_Fof4_ki5TA611LJJJWanum84Pk1fUI-IHgXK03aiPOk6YLu2ZVouchIrNAOtgL6u4fbUhtQSzPeAI19O5U-qAW0U6LysexbCv3rkHLgLHF9lF_jK5EYmrYaqdRC44/file?dl=1# [following]\n",
            "--2024-02-18 16:01:22--  https://uc5b61147a20f08abfd649be845f.dl-eu.dropboxusercontent.com/cd/0/get/CNiS_P9fP1OV_57u-6OY-Qez-x_Fof4_ki5TA611LJJJWanum84Pk1fUI-IHgXK03aiPOk6YLu2ZVouchIrNAOtgL6u4fbUhtQSzPeAI19O5U-qAW0U6LysexbCv3rkHLgLHF9lF_jK5EYmrYaqdRC44/file?dl=1\n",
            "Resolving uc5b61147a20f08abfd649be845f.dl-eu.dropboxusercontent.com (uc5b61147a20f08abfd649be845f.dl-eu.dropboxusercontent.com)... 162.125.2.15, 2620:100:6021:15::a27d:410f\n",
            "Connecting to uc5b61147a20f08abfd649be845f.dl-eu.dropboxusercontent.com (uc5b61147a20f08abfd649be845f.dl-eu.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7599001600 (7.1G) [application/binary]\n",
            "Saving to: ‘/content/3D_OS/3D_OS_release_data/tmp_modelnet40_normal_resampled.tar’\n",
            "\n",
            "/content/3D_OS/3D_O 100%[===================>]   7.08G  20.8MB/s    in 6m 7s   \n",
            "\n",
            "2024-02-18 16:07:31 (19.7 MB/s) - ‘/content/3D_OS/3D_OS_release_data/tmp_modelnet40_normal_resampled.tar’ saved [7599001600/7599001600]\n",
            "\n",
            "============\n",
            "============Downloading ScanObjectNN in \n",
            "--2024-02-18 16:08:18--  https://www.dropbox.com/s/gu0p3rych1k26b7/ScanObjectNN.tar?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6021:18::a27d:4112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/gu0p3rych1k26b7/ScanObjectNN.tar [following]\n",
            "--2024-02-18 16:08:19--  https://www.dropbox.com/s/dl/gu0p3rych1k26b7/ScanObjectNN.tar\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7e7c14ce4e1e96dfdbb876eba2.dl-eu.dropboxusercontent.com/cd/0/get/CNiSm02idvKF8nYdNkaJajUOYLc3dCT99tnpoQ1Ln-QnfU-ACUj77b3w498Aiy6lhk4TfKQm2FC8yyrIvUD4kKlWXhwyrV_bWp58ZdRNhOVxQ01mvdi7-a_DQunqaiMXqXeMKHwTFX21CShwbt7DTLGk/file?dl=1# [following]\n",
            "--2024-02-18 16:08:19--  https://uc7e7c14ce4e1e96dfdbb876eba2.dl-eu.dropboxusercontent.com/cd/0/get/CNiSm02idvKF8nYdNkaJajUOYLc3dCT99tnpoQ1Ln-QnfU-ACUj77b3w498Aiy6lhk4TfKQm2FC8yyrIvUD4kKlWXhwyrV_bWp58ZdRNhOVxQ01mvdi7-a_DQunqaiMXqXeMKHwTFX21CShwbt7DTLGk/file?dl=1\n",
            "Resolving uc7e7c14ce4e1e96dfdbb876eba2.dl-eu.dropboxusercontent.com (uc7e7c14ce4e1e96dfdbb876eba2.dl-eu.dropboxusercontent.com)... 162.125.2.15, 2620:100:6021:15::a27d:410f\n",
            "Connecting to uc7e7c14ce4e1e96dfdbb876eba2.dl-eu.dropboxusercontent.com (uc7e7c14ce4e1e96dfdbb876eba2.dl-eu.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2623662080 (2.4G) [application/binary]\n",
            "Saving to: ‘/content/3D_OS/3D_OS_release_data/tmp_ScanObjectNN.tar’\n",
            "\n",
            "/content/3D_OS/3D_O 100%[===================>]   2.44G  19.7MB/s    in 2m 12s  \n",
            "\n",
            "2024-02-18 16:10:32 (19.0 MB/s) - ‘/content/3D_OS/3D_OS_release_data/tmp_ScanObjectNN.tar’ saved [2623662080/2623662080]\n",
            "\n",
            "============\n",
            "============Downloading ModelNet40 with corruptions in \n",
            "--2024-02-18 16:10:43--  https://www.dropbox.com/s/28u4swbyyn3wflz/ModelNet40_corrupted.tar?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/28u4swbyyn3wflz/ModelNet40_corrupted.tar [following]\n",
            "--2024-02-18 16:10:44--  https://www.dropbox.com/s/dl/28u4swbyyn3wflz/ModelNet40_corrupted.tar\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc9751019934b2a45ef8bd372b66.dl-eu.dropboxusercontent.com/cd/0/get/CNhcAwm9pOzPPyA5I7tv65cgn4v9Uy8AUWVSU44tw4p8GV3NQqqwYvWY95Oh3H5nMxyut4Uszc51Badmei0Z9tbtPtsNoXNEiVNwfG9ORPzITYmTgwBd99eGv_JqnBKAXWkKnZ0FcDhdFN5jKUOzX9Qc/file?dl=1# [following]\n",
            "--2024-02-18 16:10:44--  https://uc9751019934b2a45ef8bd372b66.dl-eu.dropboxusercontent.com/cd/0/get/CNhcAwm9pOzPPyA5I7tv65cgn4v9Uy8AUWVSU44tw4p8GV3NQqqwYvWY95Oh3H5nMxyut4Uszc51Badmei0Z9tbtPtsNoXNEiVNwfG9ORPzITYmTgwBd99eGv_JqnBKAXWkKnZ0FcDhdFN5jKUOzX9Qc/file?dl=1\n",
            "Resolving uc9751019934b2a45ef8bd372b66.dl-eu.dropboxusercontent.com (uc9751019934b2a45ef8bd372b66.dl-eu.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uc9751019934b2a45ef8bd372b66.dl-eu.dropboxusercontent.com (uc9751019934b2a45ef8bd372b66.dl-eu.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1107384320 (1.0G) [application/binary]\n",
            "Saving to: ‘/content/3D_OS/3D_OS_release_data/tmp_ModelNet40_corrupted.tar’\n",
            "\n",
            "/content/3D_OS/3D_O 100%[===================>]   1.03G  19.6MB/s    in 57s     \n",
            "\n",
            "2024-02-18 16:11:42 (18.5 MB/s) - ‘/content/3D_OS/3D_OS_release_data/tmp_ModelNet40_corrupted.tar’ saved [1107384320/1107384320]\n",
            "\n",
            "============\n",
            "Finished\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
            "Collecting lmdb\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msgpack-numpy\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy) (1.0.7)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Installing collected packages: ninja, lmdb, msgpack-numpy\n",
            "Successfully installed lmdb-1.4.1 msgpack-numpy-0.4.8 ninja-1.11.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for displaying point cloud"
      ],
      "metadata": {
        "id": "ktomz4FSAei3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_pointcloud(pointcloud):\n",
        "    points = pointcloud.points\n",
        "\n",
        "    fig = go.Figure(\n",
        "      data=[\n",
        "          go.Scatter3d(\n",
        "            x=np.asarray(points)[:,0],\n",
        "            y=np.asarray(points)[:,1],\n",
        "            z=np.asarray(points)[:,2],\n",
        "            mode='markers',\n",
        "          )\n",
        "      ],\n",
        "      layout=dict(\n",
        "            scene=dict(\n",
        "            xaxis=dict(visible=False),\n",
        "            yaxis=dict(visible=False),\n",
        "            zaxis=dict(visible=False)\n",
        "        )\n",
        "      )\n",
        "    )\n",
        "    fig.show()\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "SSEL5SEVAdZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create data for chair\n",
        "\n",
        "We use a 3D chair downloaded from:\n",
        "\n",
        "https://free3d.com/3d-model/office-chair-871087.html\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "seoQcJoGmBkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bpy"
      ],
      "metadata": {
        "id": "o52eGIp-mJ2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26573a7-330b-47f4-f8c2-6c5d6a070afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bpy\n",
            "  Downloading bpy-4.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (390.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.1/390.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from bpy) (3.0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpy) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpy) (2.31.0)\n",
            "Collecting zstandard (from bpy)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bpy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpy) (2024.2.2)\n",
            "Installing collected packages: zstandard, bpy\n",
            "Successfully installed bpy-4.0.0 zstandard-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def obj_to_pointcloud(obj_file_path, num_samples=10000):\n",
        "    vertices = []\n",
        "    with open(obj_file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            if line.startswith('v '):\n",
        "                parts = line.strip().split()\n",
        "                vertices.append([float(parts[1]), float(parts[2]), float(parts[3])])\n",
        "\n",
        "    vertices_array = np.array(vertices)\n",
        "\n",
        "    if vertices_array.shape[0] < num_samples:\n",
        "        raise ValueError(f\"Not enough vertices to sample: {vertices_array.shape[0]} available, {num_samples} requested.\")\n",
        "\n",
        "    sampled_indices = np.random.choice(vertices_array.shape[0], size=num_samples, replace=False)\n",
        "    sampled_vertices = vertices_array[sampled_indices]\n",
        "\n",
        "    return sampled_vertices\n",
        "\n",
        "obj_file_path = '/content/Office chair.obj'\n",
        "\n",
        "try:\n",
        "    point_cloud = obj_to_pointcloud(obj_file_path)\n",
        "    output_file_path = 'output_point_cloud.npy'\n",
        "    np.save(output_file_path, point_cloud)\n",
        "    print(f\"Sampled point cloud saved to {output_file_path}\")\n",
        "except ValueError as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlLMJBaQuLeM",
        "outputId": "7058ae3a-9c87-48dd-bc8d-5f86dfcd52e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled point cloud saved to output_point_cloud.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataset of correctly produced chairs\n",
        "To get realistic measurements we add gaussian noise to the point cloud to simulate measurement noise."
      ],
      "metadata": {
        "id": "ApGQMjIqxGPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import open3d as o3d\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class PointCloudDatasetWithLabels(Dataset):\n",
        "    def __init__(self, point_clouds, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            point_clouds (list): List of point clouds, each point cloud is a numpy array or PyTorch tensor.\n",
        "            labels (list): List of labels corresponding to each point cloud.\n",
        "        \"\"\"\n",
        "        assert len(point_clouds) == len(labels), \"Point clouds and labels must have the same length\"\n",
        "        self.point_clouds = [torch.tensor(pc, dtype=torch.float) if not torch.is_tensor(pc) else pc for pc in point_clouds]\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.point_clouds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.point_clouds[idx], self.labels[idx]\n",
        "\n",
        "def add_gaussian_noise_to_pointcloud(pcd, mean=0, std_dev=0.01):\n",
        "    points = np.asarray(pcd.points)\n",
        "\n",
        "    noise = np.random.normal(mean, std_dev, points.shape)\n",
        "\n",
        "    noisy_points = points + noise\n",
        "\n",
        "    noisy_pcd = o3d.geometry.PointCloud()\n",
        "    noisy_pcd.points = o3d.utility.Vector3dVector(noisy_points)\n",
        "\n",
        "    return noisy_pcd\n",
        "\n",
        "def generate_noisy_chairs(pointcloud, number_of_noisy_chairs=100):\n",
        "    noisy_chairs = []\n",
        "    for i in range(0, number_of_noisy_chairs):\n",
        "        noisy_pcd = add_gaussian_noise_to_pointcloud(pointcloud)\n",
        "        noisy_chairs.append(noisy_pcd)\n",
        "    return noisy_chairs\n",
        "\n",
        "\n",
        "points = np.load(\"/content/output_point_cloud.npy\")\n",
        "points = torch.from_numpy(points)\n",
        "print(points.shape)\n",
        "points = points.reshape(-1, 3)\n",
        "print(points.shape)\n",
        "\n",
        "pcd = o3d.geometry.PointCloud()\n",
        "pcd.points = o3d.utility.Vector3dVector(points)\n",
        "\n",
        "noisy_chairs = generate_noisy_chairs(pcd, number_of_noisy_chairs=600)\n",
        "noisy_chairs_labels = []\n",
        "for i in range(0, len(noisy_chairs)):\n",
        "    _, noisy_chairs[i] = filter_for_OpenShape(noisy_chairs[i], num_points=10000)\n",
        "    noisy_chairs_labels.append(1)\n",
        "\n",
        "# Assuming `point_cloud_list` is your list of point cloud inputs\n",
        "noisy_correct_chairs_dataset = PointCloudDatasetWithLabels(noisy_chairs, noisy_chairs_labels)\n",
        "\n",
        "# Create a DataLoader\n",
        "noisy_correct_chairs_dataloader = DataLoader(noisy_correct_chairs_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "total_size = len(noisy_correct_chairs_dataset)\n",
        "train_size = int(total_size * 0.5)  # 80% of the total size\n",
        "test_size = total_size - train_size  # The rest for testing/validation\n",
        "\n",
        "# Perform the split\n",
        "train_dataset, id_dataset = random_split(noisy_correct_chairs_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "id_loader = DataLoader(id_dataset, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HqxGLHfoFGh",
        "outputId": "62d1bd07-d2ef-427a-d241-c353531c0182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 3])\n",
            "torch.Size([10000, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataset of defect chairs"
      ],
      "metadata": {
        "id": "T9hTd58X8iqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import open3d as o3d\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "# Localized Distortion\n",
        "def distort_chair(chair_points):\n",
        "    max_values = np.amax(chair_points, axis=0)\n",
        "    x_max, y_max, z_max = max_values[0], max_values[1], max_values[2]\n",
        "\n",
        "    min_values = np.amin(chair_points, axis=0)\n",
        "    x_min, y_min, z_min = min_values[0], min_values[1], min_values[2]\n",
        "\n",
        "    min_bound = np.array([x_min, y_min, z_min])\n",
        "    max_bound = np.array([x_max, y_max, z_max])\n",
        "    region_indices = np.all((chair_points >= min_bound) & (chair_points <= max_bound), axis=1)\n",
        "    chair_points[region_indices] += np.random.normal(0, 0.5, chair_points[region_indices].shape)\n",
        "    return chair_points\n",
        "\n",
        "# Missing parts\n",
        "def remove_points_from_chair(chair_points):\n",
        "    num_missing_points = 100\n",
        "    missing_indices = np.random.choice(len(chair_points), num_missing_points, replace=False)\n",
        "    chair_points = np.delete(chair_points, missing_indices, axis=0)\n",
        "    return chair_points\n",
        "\n",
        "def remove_points_within_radius(point_cloud, radius):\n",
        "    random_index = np.random.randint(len(point_cloud))\n",
        "    random_point = point_cloud[random_index]\n",
        "\n",
        "    squared_distances = np.sum((point_cloud - random_point) ** 2, axis=1)\n",
        "    squared_radius = radius ** 2\n",
        "\n",
        "    mask = squared_distances > squared_radius\n",
        "\n",
        "    return point_cloud[mask]\n",
        "\n",
        "def add_random_points(chair_points, final_size=10000):\n",
        "    size = len(chair_points)\n",
        "    points_to_add = final_size - size\n",
        "\n",
        "    noise_indices = np.random.choice(len(chair_points), points_to_add, replace=False)\n",
        "    new_points = chair_points[noise_indices] + np.random.normal(0, 0.1, chair_points[noise_indices].shape)\n",
        "\n",
        "    return np.concatenate((chair_points, new_points), axis=0)\n",
        "\n",
        "# Scale some parts\n",
        "def scale_chair(chair_points, scale):\n",
        "    scale_factor = scale\n",
        "    chair_points *= scale_factor\n",
        "    return chair_points\n",
        "\n",
        "def add_noise_to_chair(chair_points, std = 10):\n",
        "    noise_indices = np.random.choice(len(chair_points), 10, replace=False)\n",
        "    chair_points[noise_indices] += np.random.normal(0, std, chair_points[noise_indices].shape)\n",
        "    return chair_points\n",
        "\n",
        "# Make chair asymmertric\n",
        "def make_chair_asymmetric(chair_points):\n",
        "    asymmetry_indices = chair_points[:, 0] > np.mean(chair_points[:, 0])  # For example, right half\n",
        "    chair_points[asymmetry_indices] *= np.array([1.0, 0.9, 0.9])  # Scale down y and z\n",
        "    return chair_points\n",
        "\n",
        "\n",
        "def apply_asymmetrical_scaling(point_cloud, scale_factors=np.array([1.0, 0.9, 0.9])):\n",
        "    axis = np.random.choice([0, 1, 2])\n",
        "\n",
        "    median_value = np.median(point_cloud[:, axis])\n",
        "\n",
        "    if np.random.rand() > 0.5:\n",
        "        selected_half_mask = point_cloud[:, axis] > median_value\n",
        "    else:\n",
        "        selected_half_mask = point_cloud[:, axis] <= median_value\n",
        "\n",
        "    point_cloud[selected_half_mask] *= np.array(scale_factors)\n",
        "\n",
        "    return point_cloud\n",
        "\n",
        "\n",
        "def random_rotation_matrix():\n",
        "    \"\"\"Generate a random rotation matrix.\"\"\"\n",
        "    theta = np.random.uniform(0, 2 * np.pi)\n",
        "\n",
        "    axis = np.random.randn(3)\n",
        "    axis = axis / np.linalg.norm(axis)\n",
        "\n",
        "    K = np.array([[0, -axis[2], axis[1]],\n",
        "                  [axis[2], 0, -axis[0]],\n",
        "                  [-axis[1], axis[0], 0]])\n",
        "    rotation_matrix = np.eye(3) + np.sin(theta) * K + (1 - np.cos(theta)) * np.dot(K, K)\n",
        "\n",
        "    return rotation_matrix\n",
        "\n",
        "def create_random_defect(pcd, type_of_defect):\n",
        "    chair_points = np.asarray(pcd.points)\n",
        "\n",
        "    if type_of_defect == \"distort\":\n",
        "        chair_points = distort_chair(chair_points)\n",
        "    elif type_of_defect == \"remove parts\":\n",
        "        chair_points = remove_points_within_radius(chair_points, 300)\n",
        "        chair_points = add_random_points(chair_points)\n",
        "    elif type_of_defect == \"scale\":\n",
        "        chair_points = scale_chair(chair_points, scale=random.uniform(0.8, 1.3))\n",
        "    elif type_of_defect == \"add noise\":\n",
        "        chair_points = add_noise_to_chair(chair_points)\n",
        "    elif type_of_defect == \"asymmetrical\":\n",
        "        rotation_matrix = random_rotation_matrix()\n",
        "        rotation_matrix_inv = rotation_matrix.T\n",
        "        chair_points = np.dot(chair_points, rotation_matrix.T)\n",
        "        chair_points = apply_asymmetrical_scaling(chair_points, scale_factors=np.array([1.0, 0.8, 0.8]))\n",
        "        chair_points = np.dot(chair_points, rotation_matrix_inv.T)\n",
        "\n",
        "    defect_pcd = o3d.geometry.PointCloud()\n",
        "    defect_pcd.points = o3d.utility.Vector3dVector(chair_points)\n",
        "    return defect_pcd\n",
        "\n",
        "def generate_defect_chairs(pointcloud, type_of_defects, number_of_noisy_chairs=100):\n",
        "    defect_chairs = []\n",
        "    num_each_type = number_of_noisy_chairs //len(type_of_defects)\n",
        "    for defect in type_of_defects:\n",
        "        for i in range(0, num_each_type):\n",
        "            # Add defect\n",
        "            defect_pcd = create_random_defect(pcd, defect)\n",
        "            # Add measurement noise\n",
        "            defect_pcd = add_gaussian_noise_to_pointcloud(defect_pcd, mean=0, std_dev=0.01)\n",
        "            defect_chairs.append(defect_pcd)\n",
        "    return defect_chairs\n",
        "\n",
        "type_of_defects = [\"distort\", \"remove parts\", \"scale\", \"add noise\", \"asymmetrical\"]\n",
        "defect_chairs = generate_defect_chairs(pcd, type_of_defects, number_of_noisy_chairs=500)\n",
        "defect_chairs_labels = np.zeros(len(defect_chairs))\n",
        "for i in range(0, len(defect_chairs)):\n",
        "    _, defect_chairs[i] = filter_for_OpenShape(defect_chairs[i], num_points=10000)\n",
        "\n",
        "# Assuming `point_cloud_list` is your list of point cloud inputs\n",
        "noisy_defect_chairs_dataset = PointCloudDatasetWithLabels(defect_chairs, defect_chairs_labels)\n",
        "\n",
        "# Create a DataLoader\n",
        "ood_loader = DataLoader(noisy_defect_chairs_dataset, batch_size=1, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "0-QjNG6U8n-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the DataLoader\n",
        "for data in train_loader:\n",
        "    print(\"train_loader\")\n",
        "    if isinstance(data, tuple) or isinstance(data, list):\n",
        "        inputs, targets = data\n",
        "        print(f\"Input batch dimensions: {inputs.shape}\")\n",
        "        print(f\"Target batch dimensions: {targets.shape}\")\n",
        "    else:\n",
        "        # If there are no targets/labels\n",
        "        print(f\"Input batch dimensions: {data.shape}\")\n",
        "    break  # Break after the first batch to just check the dimensions\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "for data in id_loader:\n",
        "    print(\"id_loader\")\n",
        "    if isinstance(data, tuple) or isinstance(data, list):\n",
        "        inputs, targets = data\n",
        "        print(f\"Input batch dimensions: {inputs.shape}\")\n",
        "        print(f\"Target batch dimensions: {targets.shape}\")\n",
        "    else:\n",
        "        # If there are no targets/labels\n",
        "        print(f\"Input batch dimensions: {data.shape}\")\n",
        "    break  # Break after the first batch to just check the dimensions\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "for data in ood_loader:\n",
        "    print(\"ood_loader\")\n",
        "    if isinstance(data, tuple) or isinstance(data, list):\n",
        "        inputs, targets = data\n",
        "        print(f\"Input batch dimensions: {inputs.shape}\")\n",
        "        print(f\"Target batch dimensions: {targets.shape}\")\n",
        "    else:\n",
        "        # If there are no targets/labels\n",
        "        print(f\"Input batch dimensions: {data.shape}\")\n",
        "    break  # Break after the first batch to just check the dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzmaKFSTh13Y",
        "outputId": "52e255aa-3998-4c5d-9733-b1b9172ae39d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loader\n",
            "Input batch dimensions: torch.Size([1, 1, 6, 10000])\n",
            "Target batch dimensions: torch.Size([1])\n",
            "id_loader\n",
            "Input batch dimensions: torch.Size([1, 1, 6, 10000])\n",
            "Target batch dimensions: torch.Size([1])\n",
            "ood_loader\n",
            "Input batch dimensions: torch.Size([1, 1, 6, 10000])\n",
            "Target batch dimensions: torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nBNqsX-cZhXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the PointNet++ for the same task"
      ],
      "metadata": {
        "id": "mpBwf3quAO8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title JS Shell\n",
        "%%html\n",
        "<div id=term_demo></div>\n",
        "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
        "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
        "<script>\n",
        "  $('#term_demo').terminal(function(command) {\n",
        "      if (command !== '') {\n",
        "          try {\n",
        "              var result = window.eval(command);\n",
        "              if (result !== undefined) {\n",
        "                  this.echo(new String(result));\n",
        "              }\n",
        "          } catch(e) {\n",
        "              this.error(new String(e));\n",
        "          }\n",
        "      } else {\n",
        "          this.echo('');\n",
        "      }\n",
        "  }, {\n",
        "      greetings: 'Welcome to JavaScript Shell',\n",
        "      name: 'js_demo',\n",
        "      height: 200,\n",
        "      prompt: 'js> '\n",
        "  });\n",
        "\n",
        "from IPython.display import JSON\n",
        "from google.colab import output\n",
        "from subprocess import getoutput\n",
        "import os\n",
        "\n",
        "def shell(command):\n",
        "  if command.startswith('cd'):\n",
        "    path = command.strip().split(maxsplit=1)[1]\n",
        "    os.chdir(path)\n",
        "    return JSON([''])\n",
        "  return JSON([getoutput(command)])\n",
        "output.register_callback('shell', shell)\n",
        "\n",
        "#@title Colab Shell\n",
        "%%html\n",
        "<div id=term_demo></div>\n",
        "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
        "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
        "<script>\n",
        "  $('#term_demo').terminal(async function(command) {\n",
        "      if (command !== '') {\n",
        "          try {\n",
        "              let res = await google.colab.kernel.invokeFunction('shell', [command])\n",
        "              let out = res.data['application/json'][0]\n",
        "              this.echo(new String(out))\n",
        "          } catch(e) {\n",
        "              this.error(new String(e));\n",
        "          }\n",
        "      } else {\n",
        "          this.echo('');\n",
        "      }\n",
        "  }, {\n",
        "      greetings: 'Welcome to Colab Shell',\n",
        "      name: 'colab_demo',\n",
        "      height: 250,\n",
        "      prompt: 'colab > '\n",
        "  });"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "TdQ85WJQAOT8",
        "outputId": "757917eb-9ce7-4a22-b76f-2051acbe02be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div id=term_demo></div>\n",
              "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
              "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
              "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
              "<script>\n",
              "  $('#term_demo').terminal(function(command) {\n",
              "      if (command !== '') {\n",
              "          try {\n",
              "              var result = window.eval(command);\n",
              "              if (result !== undefined) {\n",
              "                  this.echo(new String(result));\n",
              "              }\n",
              "          } catch(e) {\n",
              "              this.error(new String(e));\n",
              "          }\n",
              "      } else {\n",
              "          this.echo('');\n",
              "      }\n",
              "  }, {\n",
              "      greetings: 'Welcome to JavaScript Shell',\n",
              "      name: 'js_demo',\n",
              "      height: 200,\n",
              "      prompt: 'js> '\n",
              "  });\n",
              "\n",
              "from IPython.display import JSON\n",
              "from google.colab import output\n",
              "from subprocess import getoutput\n",
              "import os\n",
              "\n",
              "def shell(command):\n",
              "  if command.startswith('cd'):\n",
              "    path = command.strip().split(maxsplit=1)[1]\n",
              "    os.chdir(path)\n",
              "    return JSON([''])\n",
              "  return JSON([getoutput(command)])\n",
              "output.register_callback('shell', shell)\n",
              "\n",
              "#@title Colab Shell\n",
              "%%html\n",
              "<div id=term_demo></div>\n",
              "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
              "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
              "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
              "<script>\n",
              "  $('#term_demo').terminal(async function(command) {\n",
              "      if (command !== '') {\n",
              "          try {\n",
              "              let res = await google.colab.kernel.invokeFunction('shell', [command])\n",
              "              let out = res.data['application/json'][0]\n",
              "              this.echo(new String(out))\n",
              "          } catch(e) {\n",
              "              this.error(new String(e));\n",
              "          }\n",
              "      } else {\n",
              "          this.echo('');\n",
              "      }\n",
              "  }, {\n",
              "      greetings: 'Welcome to Colab Shell',\n",
              "      name: 'colab_demo',\n",
              "      height: 250,\n",
              "      prompt: 'colab > '\n",
              "  });\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a PointNet model\n",
        "Code from repo: https://github.com/fxia22/pointnet.pytorch.git\n",
        "\n",
        "From paper:\n",
        "Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point\n",
        "sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer\n",
        "vision and pattern recognition, pages 652–660, 2017."
      ],
      "metadata": {
        "id": "hOC-SblCHA9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fxia22/pointnet.pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh3I2DjvUMLW",
        "outputId": "a031305e-6648-4edc-a9f6-faa7ec9fe160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pointnet.pytorch'...\n",
            "remote: Enumerating objects: 213, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 213 (delta 0), reused 2 (delta 0), pack-reused 211\u001b[K\n",
            "Receiving objects: 100% (213/213), 229.91 KiB | 976.00 KiB/s, done.\n",
            "Resolving deltas: 100% (125/125), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataset"
      ],
      "metadata": {
        "id": "Q7736Kf1pZod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import open3d as o3d\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class PointCloudDatasetWithLabels(Dataset):\n",
        "    def __init__(self, point_clouds, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            point_clouds (list): List of point clouds, each point cloud is a numpy array or PyTorch tensor.\n",
        "            labels (list): List of labels corresponding to each point cloud.\n",
        "        \"\"\"\n",
        "        assert len(point_clouds) == len(labels), \"Point clouds and labels must have the same length\"\n",
        "        self.point_clouds = [torch.tensor(pc, dtype=torch.float) if not torch.is_tensor(pc) else pc for pc in point_clouds]\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.point_clouds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.point_clouds[idx], self.labels[idx]\n",
        "\n",
        "def add_gaussian_noise_to_pointcloud(pcd, mean=0, std_dev=0.01):\n",
        "    points = np.asarray(pcd.points)\n",
        "\n",
        "    noise = np.random.normal(mean, std_dev, points.shape)\n",
        "\n",
        "    noisy_points = points + noise\n",
        "\n",
        "    noisy_pcd = o3d.geometry.PointCloud()\n",
        "    noisy_pcd.points = o3d.utility.Vector3dVector(noisy_points)\n",
        "\n",
        "    return noisy_pcd\n",
        "\n",
        "def generate_noisy_chairs(pointcloud, number_of_noisy_chairs=100):\n",
        "    noisy_chairs = []\n",
        "    for i in range(0, number_of_noisy_chairs):\n",
        "        noisy_pcd = add_gaussian_noise_to_pointcloud(pointcloud)\n",
        "        noisy_chairs.append(noisy_pcd)\n",
        "    return noisy_chairs\n",
        "\n",
        "\n",
        "points = np.load(\"/content/output_point_cloud.npy\")\n",
        "points = torch.from_numpy(points)\n",
        "print(points.shape)\n",
        "points = points.reshape(-1, 3)\n",
        "print(points.shape)\n",
        "\n",
        "pcd = o3d.geometry.PointCloud()\n",
        "pcd.points = o3d.utility.Vector3dVector(points)\n",
        "\n",
        "noisy_chairs = generate_noisy_chairs(pcd, number_of_noisy_chairs=600)\n",
        "noisy_chairs_labels = []\n",
        "for i in range(0, len(noisy_chairs)):\n",
        "    _, noisy_chairs[i] = filter_for_OpenShape(noisy_chairs[i], num_points=10000)\n",
        "    noisy_chairs_labels.append(6)\n",
        "\n",
        "# Assuming `point_cloud_list` is your list of point cloud inputs\n",
        "noisy_correct_chairs_dataset = PointCloudDatasetWithLabels(noisy_chairs, noisy_chairs_labels)\n",
        "\n",
        "# Create a DataLoader\n",
        "noisy_correct_chairs_dataloader = DataLoader(noisy_correct_chairs_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "total_size = len(noisy_correct_chairs_dataset)\n",
        "train_size = int(total_size * 0.5)  # 80% of the total size\n",
        "test_size = total_size - train_size  # The rest for testing/validation\n",
        "\n",
        "# Perform the split\n",
        "train_dataset, id_dataset = random_split(noisy_correct_chairs_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "id_loader = DataLoader(id_dataset, batch_size=1, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "CF_2Nmm0arXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2f8a7f-c0e7-416c-ef2f-76e3f28073b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 3])\n",
            "torch.Size([10000, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py protobuf lmdb msgpack-numpy ninja scikit-learn"
      ],
      "metadata": {
        "id": "riuBtkJVqXby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce4ab891-7e81-4ec4-dd42-133952a4b4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: msgpack-numpy in /usr/local/lib/python3.10/dist-packages (0.4.8)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy) (1.0.7)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub wandb omegaconf torch_redstone einops tqdm open3d dgl timm\n",
        "!pip install --upgrade https://github.com/unlimblue/KNN_CUDA/releases/download/0.2/KNN_CUDA-0.2-py3-none-any.whl"
      ],
      "metadata": {
        "id": "iHL-F8Z_rY0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81c6e51e-a520-4d06-bb6d-a87e39503a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.3)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: torch_redstone in /usr/local/lib/python3.10/dist-packages (0.0.5)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: open3d in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.10/dist-packages (1.1.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.42)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.40.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_redstone) (1.25.2)\n",
            "Requirement already satisfied: dash>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.15.0)\n",
            "Requirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.0.1)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.9.2)\n",
            "Requirement already satisfied: configargparse in /usr/local/lib/python3.10/dist-packages (from open3d) (1.7)\n",
            "Requirement already satisfied: ipywidgets>=8.0.4 in /usr/local/lib/python3.10/dist-packages (from open3d) (8.1.2)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from open3d) (2.4.0)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (9.4.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.2.2)\n",
            "Requirement already satisfied: pyquaternion in /usr/local/lib/python3.10/dist-packages (from open3d) (0.9.9)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2.1)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.2.5)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.15.0)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.0.0)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.3.4)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (7.0.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (0.2.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.10 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (4.0.10)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.10)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (3.2.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=2.2.3->open3d) (2.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.1.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.17.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (8.2.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.17.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat>=5.7.0->open3d) (4.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Collecting KNN-CUDA==0.2\n",
            "  Downloading https://github.com/unlimblue/KNN_CUDA/releases/download/0.2/KNN_CUDA-0.2-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from KNN-CUDA==0.2) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from KNN-CUDA==0.2) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from KNN-CUDA==0.2) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->KNN-CUDA==0.2) (2.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->KNN-CUDA==0.2) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->KNN-CUDA==0.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->KNN-CUDA==0.2) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->KNN-CUDA==0.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->KNN-CUDA==0.2) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change folder to 3D_OS to get the same dataloaders\n",
        "%cd \"/content/3D_OS\"\n",
        "!ls utils\n",
        "\n",
        "import os\n",
        "\n",
        "# Change this path to the root directory of the cloned repository\n",
        "repo_root = '/content/3D_OS'\n",
        "\n",
        "os.chdir(repo_root)\n",
        "print(\"Current Working Directory:\", os.getcwd())\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "sys.path.append(os.getcwd())\n",
        "import os.path as osp\n",
        "import time\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from utils.utils import *\n",
        "from utils.dist import *\n",
        "# noinspection PyUnresolvedReferences\n",
        "from utils.data_utils import H5_Dataset\n",
        "#from datasets.modelnet import *\n",
        "from datasets.scanobject import *\n",
        "from models.classifiers import Classifier\n",
        "from utils.ood_utils import get_confidence, eval_ood_sncore, iterate_data_odin, \\\n",
        "    iterate_data_energy, iterate_data_gradnorm, iterate_data_react, estimate_react_thres, print_ood_output, \\\n",
        "    get_penultimate_feats, get_network_output\n",
        "import wandb\n",
        "from base_args import add_base_args\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from models.common import convert_model_state, logits_entropy_loss\n",
        "from models.ARPL_utils import Generator, Discriminator\n",
        "from classifiers.common import train_epoch_cla, train_epoch_rsmix_exposure, train_epoch_cs\n",
        "from classifiers.trainer_ddp_cla_md import get_args, load_yaml, get_md_eval_loaders\n",
        "\n",
        "# Code from 3DOS repo classifiers/trainer_ddp_cla_md.py line 192-478\n",
        "\n",
        "#  python -m torch.distributed.launch --nproc_per_node=1 Failure_Analysis/custom\\\n",
        "# tests/evaluating_PointNet_cosine_MLS.py --config cfgs/pn2-msg.yaml\n",
        "# --exp_name PN2_cosine_SR1 --src SR1 --loss cosine -mode eval\n",
        "# --ckpt_path outputs/PN2_cosine_SR1/models/model_best.pth\n",
        "\n",
        "\n",
        "# Code from 3DOS repo classifiers/trainer_ddp_cla_md.py line 468-478\n",
        "def eval_ood_md2sonn(opt, config):\n",
        "    print(f\"Arguments: {opt}\")\n",
        "    set_random_seed(opt.seed)\n",
        "\n",
        "    dataloader_config = {\n",
        "        'batch_size': opt.batch_size, 'drop_last': False, 'shuffle': False,\n",
        "        'num_workers': opt.num_workers, 'sampler': None, 'worker_init_fn': init_np_seed}\n",
        "\n",
        "    # whole evaluation is done on ScanObject RW data\n",
        "    sonn_args = {\n",
        "        'data_root': opt.data_root,\n",
        "        'sonn_split': opt.sonn_split,\n",
        "        'h5_file': opt.sonn_h5_name,\n",
        "        'split': 'all',  # we use both training (unused) and test samples during evaluation\n",
        "        'num_points': opt.num_points_test,  # default: use all 2048 sonn points to avoid sampling randomicity\n",
        "        'transforms': None  # no augmentation applied at inference time\n",
        "    }\n",
        "\n",
        "    train_loader, _ = get_md_eval_loaders(opt)\n",
        "    if opt.src == 'SR1':\n",
        "        print(\"Src is SR1\\n\")\n",
        "        id_loader = DataLoader(ScanObject(class_choice=\"sonn_2_mdSet1\", **sonn_args), **dataloader_config)\n",
        "        ood1_loader = DataLoader(ScanObject(class_choice=\"sonn_2_mdSet2\", **sonn_args), **dataloader_config)\n",
        "    elif opt.src == 'SR2':\n",
        "        print(\"Src is SR2\\n\")\n",
        "        id_loader = DataLoader(ScanObject(class_choice=\"sonn_2_mdSet2\", **sonn_args), **dataloader_config)\n",
        "        ood1_loader = DataLoader(ScanObject(class_choice=\"sonn_2_mdSet1\", **sonn_args), **dataloader_config)\n",
        "    else:\n",
        "        raise ValueError(f\"OOD evaluation - wrong src: {opt.src}\")\n",
        "\n",
        "    ood2_loader = DataLoader(ScanObject(class_choice=\"sonn_ood_common\", **sonn_args), **dataloader_config)\n",
        "\n",
        "    return train_loader, id_loader, ood1_loader, ood2_loader\n",
        "\n",
        "class Options:\n",
        "    def __init__(self, dictionary):\n",
        "        for key, value in dictionary.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "opt_dict = {\n",
        "    \"apply_fix_cellphone\": True,\n",
        "    \"augm_set\": 'rw',\n",
        "    \"batch_size\": 1,\n",
        "    \"checkpoints_dir\": 'outputs',\n",
        "    \"ckpt_path\": 'outputs/PN2_cosine_SR1/models/model_best.pth',\n",
        "    \"config\": 'cfgs/pn2-msg.yaml',\n",
        "    \"corruption\": None,\n",
        "    \"cs\": False,\n",
        "    \"cs_beta\": 0.1,\n",
        "    \"cs_gan_lr\": 0.0002,\n",
        "    \"data_root\": \"/content/3D_OS/3D_OS_release_data\",\n",
        "    \"epochs\": 250,\n",
        "    \"eval_step\": 1,\n",
        "    \"exp_name\": 'PN2_cosine_SR1',\n",
        "    \"grad_norm_clip\": -1,\n",
        "    \"local_rank\": 0,\n",
        "    \"loss\": 'cosine',\n",
        "    \"num_points\": 10000,\n",
        "    \"num_points_test\": 10000,\n",
        "    \"num_workers\": 6,\n",
        "    \"resume\": None,\n",
        "    \"save_feats\": None,\n",
        "    \"save_step\": 10,\n",
        "    \"script_mode\": 'eval',\n",
        "    \"seed\": 1,\n",
        "    \"sonn_h5_name\": 'objectdataset.h5',\n",
        "    \"sonn_split\": 'main_split',\n",
        "    \"src\": 'SR1',\n",
        "    \"tar1\": 'none',\n",
        "    \"tar2\": 'none',\n",
        "    \"use_amp\": False,\n",
        "    \"use_sync_bn\": False,\n",
        "    \"wandb_group\": 'md-2-sonn-augmCorr',\n",
        "    \"wandb_name\": None,\n",
        "    \"wandb_proj\": 'benchmark-3d-ood-cla'\n",
        "}\n",
        "\n",
        "opt = Options(opt_dict)\n",
        "\n",
        "config = {'optimizer': {'type': 'adam',\n",
        "                        'skip_wd': [],\n",
        "                        'weight_decay': 0.0001,\n",
        "                        'kwargs': {'lr': 0.001}\n",
        "                        },\n",
        "          'scheduler': {'type': 'CosLR',\n",
        "                        'kwargs': {'t_initial': 250,\n",
        "                                   'cycle_limit': 1,\n",
        "                                   'lr_min': 1e-05\n",
        "                                   }\n",
        "                        },\n",
        "          'model': {'ENCO_NAME': 'pn2-msg',\n",
        "                    'dropout': 0.5,\n",
        "                    'cla_input_dim': 1024,\n",
        "                    'act': 'relu'\n",
        "                    }\n",
        "          }\n",
        "\n",
        "print(opt)\n",
        "print(config)\n",
        "\n",
        "train_loader_SR, src_loader_SR, tar1_loader_SR, tar2_loader_SR = eval_ood_md2sonn(opt, config)\n",
        "\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "WzsElplniq4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cd8800d-2b93-4aa9-a3d9-08c1c36324c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/3D_OS\n",
            "data_utils.py  dist.py\t__init__.py  ood_metrics.py  ood_utils.py  rsmix_provider.py  utils.py\n",
            "Current Working Directory: /content/3D_OS\n",
            "Cannot import torchlars\n",
            "Cannot load RSCNN: No module named 'pointnet2_ops'\n",
            "Cannot load PCT: No module named 'pointnet2_ops'\n",
            "Cannot load PointMLP: No module named 'pointnet2_ops'\n",
            "Cannot load PointNet2: No module named 'pointnet2_ops'\n",
            "<__main__.Options object at 0x7eb739a53f10>\n",
            "{'optimizer': {'type': 'adam', 'skip_wd': [], 'weight_decay': 0.0001, 'kwargs': {'lr': 0.001}}, 'scheduler': {'type': 'CosLR', 'kwargs': {'t_initial': 250, 'cycle_limit': 1, 'lr_min': 1e-05}}, 'model': {'ENCO_NAME': 'pn2-msg', 'dropout': 0.5, 'cla_input_dim': 1024, 'act': 'relu'}}\n",
            "Arguments: <__main__.Options object at 0x7eb739a53f10>\n",
            "ModelNet40_OOD - Reading data from h5py file: /content/3D_OS/3D_OS_release_data/modelnet40_normal_resampled/ood_sets_cache/SR1_train.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/3D_OS/utils/rsmix_provider.py:157: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if len(label_batch.shape) is 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelNet40_OOD - split: train, categories: {'chair': 0, 'bookshelf': 1, 'door': 2, 'sink': 3, 'sofa': 4}\n",
            "SR1 train data len: 2378\n",
            "ModelNet40_OOD - Reading data from h5py file: /content/3D_OS/3D_OS_release_data/modelnet40_normal_resampled/ood_sets_cache/SR1_test.h5\n",
            "ModelNet40_OOD - split: test, categories: {'chair': 0, 'bookshelf': 1, 'door': 2, 'sink': 3, 'sofa': 4}\n",
            "Src is SR1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ScanObject - num_points: 10000, sonn_split: main_split, h5_suffix: objectdataset.h5, split: all, class_choice: {4: 0, 8: 1, 7: 2, 12: 3, 13: 4}, num samples: 1255\n",
            "ScanObject - num_points: 10000, sonn_split: main_split, h5_suffix: objectdataset.h5, split: all, class_choice: {10: 0, 14: 1, 5: 2, 6: 3, 9: 2}, num samples: 788\n",
            "ScanObject - num_points: 10000, sonn_split: main_split, h5_suffix: objectdataset.h5, split: all, class_choice: {0: 404, 1: 404, 2: 404, 3: 404, 11: 404}, num samples: 847\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
        "\n",
        "\n",
        "data_list = []\n",
        "label_list = []\n",
        "\n",
        "for data, label in train_loader_SR:\n",
        "    tensor_permuted = data.permute(0, 2, 1)\n",
        "    tensor_permuted = tensor_permuted.squeeze(0)\n",
        "    data_list.append(tensor_permuted)\n",
        "    label_list.append(label.item())\n",
        "print(len(data_list))\n",
        "print(len(label_list))\n",
        "for data, label in train_loader:\n",
        "    points = data[:, :, :3, :]\n",
        "    points = points.squeeze(1)\n",
        "    points = points.squeeze(0)\n",
        "    data_list.append(points)\n",
        "    label_list.append(label.item())\n",
        "\n",
        "print(len(data_list))\n",
        "print(len(label_list))\n",
        "print(label_list)\n",
        "BATCH_SIZE = 10\n",
        "combined_dataset = PointCloudDatasetWithLabels(data_list, label_list)\n",
        "\n",
        "combined_dataloader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf_j8-qxiqaV",
        "outputId": "ef2ca5e7-1bd0-4bcd-9774-16c41fd07ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2378\n",
            "2378\n",
            "2678\n",
            "2678\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(combined_dataloader, 0):\n",
        "    points, target = data\n",
        "    print(points.shape, \"   \", target)"
      ],
      "metadata": {
        "id": "_M8FDuiO0muQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e90e2a-4f1c-4b1d-d110-964dc065351e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 10000])     tensor([1, 3, 1, 4, 3, 0, 6, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 4, 1, 3, 4, 0, 4, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 4, 0, 4, 1, 0, 4, 0, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 4, 1, 0, 0, 0, 4, 1, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 1, 1, 0, 4, 1, 0, 6, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 3, 2, 0, 0, 0, 0, 1, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 3, 0, 3, 1, 0, 4, 1, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 0, 0, 4, 1, 6, 1, 1, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 0, 6, 6, 0, 4, 3, 6, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 1, 1, 4, 0, 4, 1, 1, 6, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 6, 4, 1, 1, 1, 3, 0, 6, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 3, 1, 2, 2, 0, 3, 6, 0, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 1, 6, 0, 2, 1, 0, 6, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 4, 0, 4, 4, 2, 0, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 1, 0, 4, 0, 0, 4, 0, 1, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 0, 0, 1, 0, 0, 6, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 1, 4, 1, 6, 1, 0, 0, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 4, 2, 0, 4, 4, 0, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 1, 4, 4, 0, 0, 4, 0, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([2, 1, 0, 0, 6, 4, 0, 4, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([2, 4, 4, 1, 0, 0, 1, 0, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 1, 4, 0, 4, 4, 4, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 4, 4, 1, 0, 0, 4, 4, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 1, 6, 0, 3, 0, 4, 0, 1, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 1, 0, 0, 0, 4, 6, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 1, 0, 0, 1, 3, 6, 6, 0, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 4, 1, 0, 1, 4, 1, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 6, 4, 4, 2, 0, 4, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 0, 4, 0, 4, 4, 6, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 3, 4, 1, 6, 0, 6, 4, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 6, 1, 0, 4, 0, 1, 0, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 0, 6, 0, 0, 1, 4, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 6, 3, 1, 0, 6, 6, 0, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 4, 1, 1, 0, 2, 2, 4, 0, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 1, 0, 1, 0, 0, 0, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 2, 4, 6, 0, 1, 1, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 0, 3, 2, 3, 6, 0, 0, 6, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([3, 4, 0, 6, 3, 4, 0, 2, 3, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 4, 6, 1, 0, 3, 4, 1, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 1, 0, 0, 4, 1, 1, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 1, 4, 4, 4, 0, 4, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 6, 6, 1, 4, 4, 4, 6, 0, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 0, 0, 4, 6, 1, 4, 1, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 2, 4, 1, 0, 1, 2, 1, 0, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 6, 1, 1, 0, 1, 0, 0, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 4, 6, 4, 1, 1, 4, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 0, 6, 4, 0, 1, 6, 4, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 3, 0, 4, 4, 1, 0, 0, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 0, 4, 1, 4, 0, 4, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 6, 0, 1, 6, 0, 4, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([3, 2, 0, 0, 1, 0, 2, 1, 1, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 6, 1, 1, 0, 4, 0, 0, 4, 3])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 6, 6, 3, 6, 0, 1, 4, 6, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 4, 1, 4, 0, 0, 4, 4, 4, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 1, 2, 4, 4, 4, 6, 2, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 1, 0, 1, 0, 1, 0, 0, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 0, 0, 4, 0, 6, 0, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 6, 1, 0, 4, 1, 1, 3, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 3, 1, 6, 6, 1, 0, 2, 0, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 0, 1, 0, 1, 4, 4, 4, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 1, 0, 1, 2, 0, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 2, 4, 1, 1, 0, 1, 3, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 3, 3, 3, 4, 4, 1, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 4, 1, 1, 1, 0, 4, 6, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 0, 0, 6, 4, 0, 6, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 4, 0, 4, 4, 0, 1, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 6, 1, 3, 4, 0, 1, 4, 6, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 0, 0, 4, 0, 1, 0, 4, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 0, 6, 0, 4, 4, 6, 1, 3])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 1, 4, 1, 4, 4, 4, 0, 3])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 0, 1, 1, 6, 4, 0, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 1, 4, 2, 6, 4, 0, 4, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 4, 3, 4, 1, 4, 1, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 6, 4, 1, 3, 4, 4, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 0, 1, 1, 0, 1, 4, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 3, 0, 4, 0, 4, 0, 4, 1, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 4, 0, 4, 1, 4, 6, 6, 4, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 4, 4, 4, 4, 6, 1, 0, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 6, 6, 6, 0, 2, 4, 0, 3, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 6, 6, 4, 0, 4, 1, 6, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 1, 0, 4, 0, 4, 0, 6, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 4, 6, 4, 0, 0, 1, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 4, 4, 1, 1, 4, 2, 1, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([3, 4, 4, 0, 6, 6, 0, 4, 0, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 6, 1, 3, 0, 0, 0, 1, 1, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 0, 0, 0, 3, 4, 1, 6, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 0, 0, 0, 1, 0, 4, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 1, 0, 0, 0, 1, 4, 4, 4, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 0, 0, 6, 1, 4, 0, 3, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 4, 0, 6, 0, 6, 0, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 1, 1, 0, 2, 0, 1, 0, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 1, 0, 0, 2, 1, 0, 2, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 4, 3, 4, 0, 0, 1, 4, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 6, 0, 6, 2, 0, 1, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 1, 2, 0, 1, 2, 6, 6, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 0, 3, 4, 1, 6, 6, 4, 0, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 4, 4, 1, 0, 2, 0, 4, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 0, 0, 0, 6, 1, 0, 6, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 1, 0, 0, 0, 0, 0, 0, 4, 3])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 4, 4, 0, 1, 1, 0, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 6, 1, 4, 6, 1, 3, 1, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 1, 0, 4, 4, 1, 4, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([3, 1, 3, 0, 1, 4, 1, 1, 1, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 0, 4, 0, 4, 4, 2, 6, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 3, 4, 0, 4, 1, 4, 0, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 3, 4, 1, 0, 0, 4, 4, 4, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 4, 4, 4, 1, 1, 6, 0, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 0, 6, 4, 6, 3, 0, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 0, 0, 0, 1, 4, 0, 3, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 4, 4, 6, 1, 0, 3, 1, 4, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 4, 0, 1, 4, 0, 4, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 4, 6, 6, 3, 3, 4, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 2, 6, 4, 0, 0, 1, 0, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 6, 3, 0, 0, 3, 1, 0, 6, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 4, 4, 2, 4, 1, 4, 0, 3])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 4, 0, 4, 0, 6, 1, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 4, 1, 4, 4, 0, 0, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 0, 6, 4, 0, 0, 1, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 3, 0, 4, 4, 4, 0, 0, 6, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 0, 4, 6, 4, 4, 3, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 1, 1, 3, 6, 0, 0, 0, 1, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 6, 4, 4, 0, 1, 1, 1, 6, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 4, 6, 1, 1, 0, 2, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 0, 6, 4, 1, 1, 0, 0, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 0, 2, 3, 4, 4, 4, 4, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 0, 0, 0, 1, 4, 6, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 0, 0, 1, 0, 6, 0, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([2, 4, 4, 4, 1, 4, 1, 0, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([2, 4, 4, 2, 2, 0, 4, 1, 0, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 6, 6, 2, 0, 6, 0, 0, 0, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 3, 0, 4, 1, 0, 4, 4, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 0, 0, 0, 0, 4, 4, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([2, 1, 1, 4, 4, 1, 4, 0, 1, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 1, 1, 1, 4, 1, 0, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 0, 4, 0, 0, 4, 1, 0, 0, 3])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 4, 0, 6, 6, 0, 0, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 6, 3, 0, 1, 6, 6, 6, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 6, 3, 0, 6, 4, 0, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 1, 1, 0, 6, 0, 1, 0, 3, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 1, 6, 6, 4, 0, 4, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 4, 1, 6, 1, 4, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 1, 3, 1, 0, 1, 4, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 6, 6, 1, 6, 0, 3, 4, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 4, 4, 4, 3, 6, 1, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 6, 0, 3, 1, 4, 6, 1, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 6, 0, 0, 0, 4, 6, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 4, 0, 0, 6, 0, 1, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 4, 4, 0, 0, 0, 0, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 4, 1, 1, 0, 4, 0, 4, 0, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 1, 1, 4, 1, 0, 0, 6, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 1, 1, 0, 4, 4, 1, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 1, 4, 0, 4, 0, 4, 4, 1, 3])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 0, 4, 6, 4, 1, 4, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 2, 0, 4, 0, 0, 0, 2, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 1, 0, 2, 0, 0, 4, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 2, 0, 4, 0, 0, 0, 4, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 0, 0, 1, 0, 1, 0, 1, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 4, 6, 6, 6, 6, 2, 6, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 0, 4, 0, 0, 2, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 0, 4, 0, 4, 1, 1, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 1, 1, 0, 4, 0, 4, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 4, 1, 4, 1, 0, 4, 0, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 2, 4, 0, 6, 0, 3, 0, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 0, 6, 0, 3, 1, 4, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([2, 1, 0, 0, 1, 0, 4, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 1, 1, 1, 1, 0, 1, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 3, 4, 0, 0, 4, 0, 3, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 1, 6, 4, 1, 1, 4, 2, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([3, 4, 6, 0, 6, 0, 4, 0, 0, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 3, 0, 0, 1, 1, 1, 4, 6, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 4, 4, 4, 1, 6, 1, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 0, 4, 6, 4, 0, 0, 6, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 4, 2, 4, 4, 4, 0, 0, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 3, 0, 4, 0, 4, 4, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 2, 1, 2, 4, 6, 4, 3, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([2, 0, 4, 4, 0, 4, 4, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([3, 0, 2, 0, 4, 1, 1, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 0, 0, 0, 4, 2, 1, 4, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 6, 4, 4, 0, 0, 4, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 0, 2, 1, 2, 0, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 0, 4, 0, 0, 1, 1, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 0, 4, 6, 4, 1, 4, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 4, 0, 1, 0, 4, 4, 0, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 1, 0, 0, 2, 1, 3, 1, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 3, 1, 4, 1, 0, 2, 0, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 6, 0, 0, 3, 3, 0, 6, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 6, 4, 1, 2, 4, 1, 0, 3, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 1, 6, 6, 4, 0, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 0, 4, 4, 2, 6, 0, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 0, 1, 0, 0, 0, 1, 4, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 3, 0, 0, 1, 1, 4, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 1, 0, 3, 1, 1, 0, 4, 4, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 0, 1, 4, 0, 0, 4, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 0, 4, 0, 1, 1, 0, 1, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 1, 4, 1, 6, 4, 0, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 4, 4, 1, 0, 4, 1, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 6, 0, 6, 0, 0, 4, 1, 6, 3])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 6, 1, 0, 1, 6, 4, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 4, 4, 1, 1, 4, 1, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 4, 4, 0, 6, 4, 1, 4, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 1, 1, 0, 6, 4, 0, 4, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 0, 0, 0, 0, 1, 1, 6, 4, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 0, 0, 6, 0, 4, 1, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([3, 0, 6, 3, 0, 4, 4, 4, 4, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 3, 4, 2, 4, 2, 4, 0, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 1, 4, 0, 1, 0, 3, 4, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 3, 4, 6, 0, 4, 6, 4, 1, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 6, 1, 0, 0, 4, 1, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 0, 1, 0, 4, 1, 0, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 6, 0, 0, 0, 6, 1, 6, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 2, 1, 6, 3, 6, 4, 0, 6, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 0, 6, 1, 4, 4, 2, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 6, 4, 1, 4, 1, 0, 6, 6, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 1, 1, 1, 4, 0, 1, 1, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([3, 6, 3, 4, 1, 4, 0, 2, 0, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 0, 4, 0, 0, 4, 0, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 0, 0, 6, 0, 1, 1, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 3, 4, 3, 1, 6, 2, 6, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([3, 4, 3, 4, 0, 0, 0, 0, 6, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 0, 6, 0, 4, 0, 1, 6, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 0, 0, 0, 1, 0, 4, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 6, 4, 0, 4, 4, 6, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 3, 4, 6, 4, 0, 0, 0, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 3, 4, 4, 6, 6, 2, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 6, 0, 6, 1, 1, 4, 4, 2, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 6, 3, 1, 0, 6, 4, 2, 2, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 1, 3, 0, 6, 4, 0, 0, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 4, 1, 3, 4, 3, 1, 1, 6, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 1, 3, 1, 0, 0, 0, 2, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 0, 0, 0, 4, 3, 1, 4, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 6, 4, 0, 6, 0, 4, 3, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 4, 4, 6, 0, 6, 0, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 2, 0, 0, 6, 4, 1, 0, 4, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 6, 1, 0, 4, 0, 0, 1, 0, 3])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 4, 0, 1, 6, 4, 3, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 1, 0, 2, 4, 1, 4, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 4, 0, 0, 4, 0, 1, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 2, 1, 4, 0, 0, 0, 6, 0, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 0, 1, 4, 4, 4, 1, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 6, 3, 4, 4, 2, 4, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([3, 3, 0, 6, 4, 1, 0, 0, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 0, 0, 4, 1, 1, 4, 6, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 3, 0, 1, 4, 4, 0, 0, 6, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 4, 4, 1, 1, 3, 0, 4, 6, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 4, 0, 4, 3, 0, 0, 2, 1, 3])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 0, 1, 0, 1, 4, 4, 6, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 4, 1, 4, 1, 1, 4, 4, 1, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 1, 0, 1, 0, 4, 4, 4, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 6, 6, 6, 4, 2, 4, 1, 0, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([1, 3, 4, 0, 0, 2, 0, 0, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 3, 1, 1, 4, 4, 1, 0, 2, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 4, 6, 1, 4, 0, 0, 4, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 0, 6, 1, 3, 0, 0, 2, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([2, 4, 6, 0, 6, 0, 6, 6, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 4, 0, 1, 1, 1, 4, 4, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 4, 4, 1, 1, 1, 3, 4, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 6, 0, 0, 1, 0, 0, 0, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 6, 4, 1, 0, 0, 0, 3, 1, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 4, 1, 4, 0, 0, 6, 2, 1])\n",
            "torch.Size([10, 3, 10000])     tensor([6, 0, 4, 0, 4, 1, 0, 1, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([2, 4, 1, 4, 0, 4, 0, 4, 0, 0])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 1, 0, 0, 1, 1, 1, 0, 4, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 1, 0, 4, 4, 0, 0, 1, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 0, 0, 4, 4, 2, 1, 3, 4, 4])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 2, 1, 1, 0, 0, 0, 1, 0, 2])\n",
            "torch.Size([10, 3, 10000])     tensor([0, 0, 3, 0, 0, 1, 4, 4, 1, 6])\n",
            "torch.Size([10, 3, 10000])     tensor([4, 1, 4, 6, 1, 4, 1, 4, 3, 2])\n",
            "torch.Size([8, 3, 10000])     tensor([1, 0, 3, 1, 1, 0, 3, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class STN3d(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(STN3d, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 9)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.size()[0]\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n",
        "        if x.is_cuda:\n",
        "            iden = iden.cuda()\n",
        "        x = x + iden\n",
        "        x = x.view(-1, 3, 3)\n",
        "        return x\n",
        "\n",
        "\n",
        "class STNkd(nn.Module):\n",
        "    def __init__(self, k=64):\n",
        "        super(STNkd, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, k*k)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.size()[0]\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1,self.k*self.k).repeat(batchsize,1)\n",
        "        if x.is_cuda:\n",
        "            iden = iden.cuda()\n",
        "        x = x + iden\n",
        "        x = x.view(-1, self.k, self.k)\n",
        "        return x\n",
        "\n",
        "class PointNetfeat(nn.Module):\n",
        "    def __init__(self, global_feat = True, feature_transform = False):\n",
        "        super(PointNetfeat, self).__init__()\n",
        "        self.stn = STN3d()\n",
        "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.global_feat = global_feat\n",
        "        self.feature_transform = feature_transform\n",
        "        if self.feature_transform:\n",
        "            self.fstn = STNkd(k=64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n_pts = x.size()[2]\n",
        "        trans = self.stn(x)\n",
        "        x = x.transpose(2, 1)\n",
        "        x = torch.bmm(x, trans)\n",
        "        x = x.transpose(2, 1)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        if self.feature_transform:\n",
        "            trans_feat = self.fstn(x)\n",
        "            x = x.transpose(2,1)\n",
        "            x = torch.bmm(x, trans_feat)\n",
        "            x = x.transpose(2,1)\n",
        "        else:\n",
        "            trans_feat = None\n",
        "\n",
        "        pointfeat = x\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "        if self.global_feat:\n",
        "            return x, trans, trans_feat\n",
        "        else:\n",
        "            x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
        "            return torch.cat([x, pointfeat], 1), trans, trans_feat\n",
        "\n",
        "class PointNetCls(nn.Module):\n",
        "    def __init__(self, k=2, feature_transform=False):\n",
        "        super(PointNetCls, self).__init__()\n",
        "        self.feature_transform = feature_transform\n",
        "        self.feat = PointNetfeat(global_feat=True, feature_transform=feature_transform)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, k)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, trans, trans_feat = self.feat(x)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1), trans, trans_feat\n",
        "\n",
        "    def get_feature_space_of_last_layer(self, x):\n",
        "        x, trans, trans_feat = self.feat(x)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PointNetDenseCls(nn.Module):\n",
        "    def __init__(self, k = 2, feature_transform=False):\n",
        "        super(PointNetDenseCls, self).__init__()\n",
        "        self.k = k\n",
        "        self.feature_transform=feature_transform\n",
        "        self.feat = PointNetfeat(global_feat=False, feature_transform=feature_transform)\n",
        "        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n",
        "        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.size()[0]\n",
        "        n_pts = x.size()[2]\n",
        "        x, trans, trans_feat = self.feat(x)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.conv4(x)\n",
        "        x = x.transpose(2,1).contiguous()\n",
        "        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n",
        "        x = x.view(batchsize, n_pts, self.k)\n",
        "        return x, trans, trans_feat\n",
        "\n",
        "def feature_transform_regularizer(trans):\n",
        "    d = trans.size()[1]\n",
        "    batchsize = trans.size()[0]\n",
        "    I = torch.eye(d)[None, :, :]\n",
        "    if trans.is_cuda:\n",
        "        I = I.cuda()\n",
        "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2)))\n",
        "    return loss\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sim_data = Variable(torch.rand(32,3,2500))\n",
        "    trans = STN3d()\n",
        "    out = trans(sim_data)\n",
        "    print('stn', out.size())\n",
        "    print('loss', feature_transform_regularizer(out))\n",
        "\n",
        "    sim_data_64d = Variable(torch.rand(32, 64, 2500))\n",
        "    trans = STNkd(k=64)\n",
        "    out = trans(sim_data_64d)\n",
        "    print('stn64d', out.size())\n",
        "    print('loss', feature_transform_regularizer(out))\n",
        "\n",
        "    pointfeat = PointNetfeat(global_feat=True)\n",
        "    out, _, _ = pointfeat(sim_data)\n",
        "    print('global feat', out.size())\n",
        "\n",
        "    pointfeat = PointNetfeat(global_feat=False)\n",
        "    out, _, _ = pointfeat(sim_data)\n",
        "    print('point feat', out.size())\n",
        "\n",
        "    cls = PointNetCls(k = 5)\n",
        "    out, _, _ = cls(sim_data)\n",
        "    print('class', out.size())\n",
        "\n",
        "    seg = PointNetDenseCls(k = 3)\n",
        "    out, _, _ = seg(sim_data)\n",
        "    print('seg', out.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw8ec2CsIjv0",
        "outputId": "fe4ed13a-e0a2-45ce-f528-4ec843ced368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stn torch.Size([32, 3, 3])\n",
            "loss tensor(1.8352, grad_fn=<MeanBackward0>)\n",
            "stn64d torch.Size([32, 64, 64])\n",
            "loss tensor(124.9212, grad_fn=<MeanBackward0>)\n",
            "global feat torch.Size([32, 1024])\n",
            "point feat torch.Size([32, 1088, 2500])\n",
            "class torch.Size([32, 5])\n",
            "seg torch.Size([32, 2500, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pointnet.pytorch\n",
        "!pip install -e .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAajhmAiVfa9",
        "outputId": "805df25e-5eca-4ccb-f550-944eb79acec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pointnet.pytorch\n",
            "Obtaining file:///content/pointnet.pytorch\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pointnet==0.0.1) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pointnet==0.0.1) (4.66.2)\n",
            "Requirement already satisfied: plyfile in /usr/local/lib/python3.10/dist-packages (from pointnet==0.0.1) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from plyfile->pointnet==0.0.1) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pointnet==0.0.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pointnet==0.0.1) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pointnet==0.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pointnet==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pointnet==0.0.1) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->pointnet==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->pointnet==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pointnet==0.0.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pointnet==0.0.1) (1.3.0)\n",
            "Installing collected packages: pointnet\n",
            "  Attempting uninstall: pointnet\n",
            "    Found existing installation: pointnet 0.0.1\n",
            "    Uninstalling pointnet-0.0.1:\n",
            "      Successfully uninstalled pointnet-0.0.1\n",
            "  Running setup.py develop for pointnet\n",
            "Successfully installed pointnet-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from pointnet.dataset import ShapeNetDataset, ModelNetDataset\n",
        "from pointnet.model import feature_transform_regularizer\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "dataloader = combined_dataloader\n",
        "\n",
        "testdataloader = combined_dataloader\n",
        "\n",
        "batch_size = BATCH_SIZE\n",
        "\n",
        "classifier = PointNetCls(k=7, feature_transform=False)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "classifier.to(\"cuda\")\n",
        "\n",
        "num_batch = len(dataloader) / 1\n",
        "\n",
        "nepoch = 150\n",
        "for epoch in range(250):\n",
        "    scheduler.step()\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        points, target = data\n",
        "        #points = points[:, :, :3, :]\n",
        "        #points = points.squeeze(1)\n",
        "        #points = points.transpose(2, 1)\n",
        "        points, target = points.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        classifier = classifier.train()\n",
        "        pred, trans, trans_feat = classifier(points)\n",
        "        loss = F.nll_loss(pred, target)\n",
        "        #if opt.feature_transform:\n",
        "        #    loss += feature_transform_regularizer(trans_feat) * 0.001\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred_choice = pred.data.max(1)[1]\n",
        "        correct = pred_choice.eq(target.data).cpu().sum()\n",
        "        print('[%d: %d/%d] train loss: %f accuracy: %f' % (epoch, i, num_batch, loss.item(), correct.item() / float(batch_size)))\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            j, data = next(enumerate(testdataloader, 0))\n",
        "            points, target = data\n",
        "            #points = points[:, :, :3, :]\n",
        "            #points = points.squeeze(1)\n",
        "            #target = target[:, 0]\n",
        "            #points = points.transpose(2, 1)\n",
        "            points, target = points.cuda(), target.cuda()\n",
        "            classifier = classifier.eval()\n",
        "            pred, _, _ = classifier(points)\n",
        "            loss = F.nll_loss(pred, target)\n",
        "            pred_choice = pred.data.max(1)[1]\n",
        "            correct = pred_choice.eq(target.data).cpu().sum()\n",
        "            print('[%d: %d/%d] %s loss: %f accuracy: %f' % (epoch, i, num_batch, 'test', loss.item(), correct.item()/float(batch_size)))\n",
        "\n",
        "torch.save(classifier.state_dict(), str(\"model\" + str(epoch)))\n",
        "\n",
        "total_correct = 0\n",
        "total_testset = 0\n",
        "for i,data in tqdm(enumerate(testdataloader, 0)):\n",
        "    points, target = data\n",
        "    target = target[:, 0]\n",
        "    points = points.transpose(2, 1)\n",
        "    points, target = points.cuda(), target.cuda()\n",
        "    classifier = classifier.eval()\n",
        "    pred, _, _ = classifier(points)\n",
        "    pred_choice = pred.data.max(1)[1]\n",
        "    correct = pred_choice.eq(target.data).cpu().sum()\n",
        "    total_correct += correct.item()\n",
        "    total_testset += points.size()[0]\n",
        "\n",
        "print(\"final accuracy {}\".format(total_correct / float(total_testset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XdqQ1bmqTu1p",
        "outputId": "cca269d2-4f07-485e-df9b-7769fca02a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStrømmer utdata som er avkortet til de siste 5000 linjene.\u001b[0m\n",
            "[50: 240/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[50: 240/268] test loss: 0.002573 accuracy: 1.000000\n",
            "[50: 241/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[50: 242/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[50: 243/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[50: 244/268] train loss: 0.000433 accuracy: 1.000000\n",
            "[50: 245/268] train loss: 0.071532 accuracy: 0.900000\n",
            "[50: 246/268] train loss: 0.004997 accuracy: 1.000000\n",
            "[50: 247/268] train loss: 0.000210 accuracy: 1.000000\n",
            "[50: 248/268] train loss: 0.000167 accuracy: 1.000000\n",
            "[50: 249/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[50: 250/268] train loss: 0.000191 accuracy: 1.000000\n",
            "[50: 250/268] test loss: 0.000537 accuracy: 1.000000\n",
            "[50: 251/268] train loss: 0.058713 accuracy: 1.000000\n",
            "[50: 252/268] train loss: 0.000189 accuracy: 1.000000\n",
            "[50: 253/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[50: 254/268] train loss: 0.000537 accuracy: 1.000000\n",
            "[50: 255/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[50: 256/268] train loss: 0.000512 accuracy: 1.000000\n",
            "[50: 257/268] train loss: 0.000133 accuracy: 1.000000\n",
            "[50: 258/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[50: 259/268] train loss: 0.137531 accuracy: 0.900000\n",
            "[50: 260/268] train loss: 0.000420 accuracy: 1.000000\n",
            "[50: 260/268] test loss: 0.000037 accuracy: 1.000000\n",
            "[50: 261/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[50: 262/268] train loss: 0.010210 accuracy: 1.000000\n",
            "[50: 263/268] train loss: 0.000122 accuracy: 1.000000\n",
            "[50: 264/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[50: 265/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[50: 266/268] train loss: 0.005105 accuracy: 1.000000\n",
            "[50: 267/268] train loss: 0.025869 accuracy: 0.800000\n",
            "[51: 0/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[51: 0/268] test loss: 0.002471 accuracy: 1.000000\n",
            "[51: 1/268] train loss: 0.000238 accuracy: 1.000000\n",
            "[51: 2/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[51: 3/268] train loss: 0.001553 accuracy: 1.000000\n",
            "[51: 4/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[51: 5/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[51: 6/268] train loss: 0.002237 accuracy: 1.000000\n",
            "[51: 7/268] train loss: 0.000135 accuracy: 1.000000\n",
            "[51: 8/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[51: 9/268] train loss: 0.000200 accuracy: 1.000000\n",
            "[51: 10/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[51: 10/268] test loss: 0.000190 accuracy: 1.000000\n",
            "[51: 11/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[51: 12/268] train loss: 0.000162 accuracy: 1.000000\n",
            "[51: 13/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[51: 14/268] train loss: 0.000175 accuracy: 1.000000\n",
            "[51: 15/268] train loss: 0.000139 accuracy: 1.000000\n",
            "[51: 16/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[51: 17/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[51: 18/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[51: 19/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[51: 20/268] train loss: 0.002048 accuracy: 1.000000\n",
            "[51: 20/268] test loss: 0.000057 accuracy: 1.000000\n",
            "[51: 21/268] train loss: 0.000207 accuracy: 1.000000\n",
            "[51: 22/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[51: 23/268] train loss: 0.000577 accuracy: 1.000000\n",
            "[51: 24/268] train loss: 0.000217 accuracy: 1.000000\n",
            "[51: 25/268] train loss: 0.000165 accuracy: 1.000000\n",
            "[51: 26/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[51: 27/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[51: 28/268] train loss: 0.000114 accuracy: 1.000000\n",
            "[51: 29/268] train loss: 0.000384 accuracy: 1.000000\n",
            "[51: 30/268] train loss: 0.000133 accuracy: 1.000000\n",
            "[51: 30/268] test loss: 0.000535 accuracy: 1.000000\n",
            "[51: 31/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[51: 32/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[51: 33/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[51: 34/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[51: 35/268] train loss: 0.000252 accuracy: 1.000000\n",
            "[51: 36/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[51: 37/268] train loss: 0.000759 accuracy: 1.000000\n",
            "[51: 38/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[51: 39/268] train loss: 0.001205 accuracy: 1.000000\n",
            "[51: 40/268] train loss: 0.000177 accuracy: 1.000000\n",
            "[51: 40/268] test loss: 0.000131 accuracy: 1.000000\n",
            "[51: 41/268] train loss: 0.000257 accuracy: 1.000000\n",
            "[51: 42/268] train loss: 0.003905 accuracy: 1.000000\n",
            "[51: 43/268] train loss: 0.000082 accuracy: 1.000000\n",
            "[51: 44/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[51: 45/268] train loss: 0.000460 accuracy: 1.000000\n",
            "[51: 46/268] train loss: 0.000129 accuracy: 1.000000\n",
            "[51: 47/268] train loss: 0.000133 accuracy: 1.000000\n",
            "[51: 48/268] train loss: 0.000804 accuracy: 1.000000\n",
            "[51: 49/268] train loss: 0.003554 accuracy: 1.000000\n",
            "[51: 50/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[51: 50/268] test loss: 0.001351 accuracy: 1.000000\n",
            "[51: 51/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[51: 52/268] train loss: 0.001024 accuracy: 1.000000\n",
            "[51: 53/268] train loss: 0.000703 accuracy: 1.000000\n",
            "[51: 54/268] train loss: 0.000001 accuracy: 1.000000\n",
            "[51: 55/268] train loss: 0.000172 accuracy: 1.000000\n",
            "[51: 56/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[51: 57/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[51: 58/268] train loss: 0.000284 accuracy: 1.000000\n",
            "[51: 59/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[51: 60/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[51: 60/268] test loss: 0.000838 accuracy: 1.000000\n",
            "[51: 61/268] train loss: 0.000225 accuracy: 1.000000\n",
            "[51: 62/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[51: 63/268] train loss: 0.000549 accuracy: 1.000000\n",
            "[51: 64/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[51: 65/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[51: 66/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[51: 67/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[51: 68/268] train loss: 0.000179 accuracy: 1.000000\n",
            "[51: 69/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[51: 70/268] train loss: 0.000121 accuracy: 1.000000\n",
            "[51: 70/268] test loss: 0.000252 accuracy: 1.000000\n",
            "[51: 71/268] train loss: 0.002710 accuracy: 1.000000\n",
            "[51: 72/268] train loss: 0.271378 accuracy: 0.900000\n",
            "[51: 73/268] train loss: 0.000274 accuracy: 1.000000\n",
            "[51: 74/268] train loss: 0.003063 accuracy: 1.000000\n",
            "[51: 75/268] train loss: 0.000370 accuracy: 1.000000\n",
            "[51: 76/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[51: 77/268] train loss: 0.000251 accuracy: 1.000000\n",
            "[51: 78/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[51: 79/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[51: 80/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[51: 80/268] test loss: 0.000145 accuracy: 1.000000\n",
            "[51: 81/268] train loss: 0.000140 accuracy: 1.000000\n",
            "[51: 82/268] train loss: 0.001015 accuracy: 1.000000\n",
            "[51: 83/268] train loss: 0.000092 accuracy: 1.000000\n",
            "[51: 84/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[51: 85/268] train loss: 0.000590 accuracy: 1.000000\n",
            "[51: 86/268] train loss: 0.001620 accuracy: 1.000000\n",
            "[51: 87/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[51: 88/268] train loss: 0.002583 accuracy: 1.000000\n",
            "[51: 89/268] train loss: 0.001985 accuracy: 1.000000\n",
            "[51: 90/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[51: 90/268] test loss: 0.000070 accuracy: 1.000000\n",
            "[51: 91/268] train loss: 0.002144 accuracy: 1.000000\n",
            "[51: 92/268] train loss: 0.006981 accuracy: 1.000000\n",
            "[51: 93/268] train loss: 0.010999 accuracy: 1.000000\n",
            "[51: 94/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[51: 95/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[51: 96/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[51: 97/268] train loss: 0.000492 accuracy: 1.000000\n",
            "[51: 98/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[51: 99/268] train loss: 0.011058 accuracy: 1.000000\n",
            "[51: 100/268] train loss: 0.000536 accuracy: 1.000000\n",
            "[51: 100/268] test loss: 0.002769 accuracy: 1.000000\n",
            "[51: 101/268] train loss: 0.000261 accuracy: 1.000000\n",
            "[51: 102/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[51: 103/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[51: 104/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[51: 105/268] train loss: 0.001593 accuracy: 1.000000\n",
            "[51: 106/268] train loss: 0.000181 accuracy: 1.000000\n",
            "[51: 107/268] train loss: 0.000920 accuracy: 1.000000\n",
            "[51: 108/268] train loss: 0.000147 accuracy: 1.000000\n",
            "[51: 109/268] train loss: 0.001078 accuracy: 1.000000\n",
            "[51: 110/268] train loss: 0.001483 accuracy: 1.000000\n",
            "[51: 110/268] test loss: 0.000134 accuracy: 1.000000\n",
            "[51: 111/268] train loss: 0.002656 accuracy: 1.000000\n",
            "[51: 112/268] train loss: 0.000220 accuracy: 1.000000\n",
            "[51: 113/268] train loss: 0.000144 accuracy: 1.000000\n",
            "[51: 114/268] train loss: 0.001198 accuracy: 1.000000\n",
            "[51: 115/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[51: 116/268] train loss: 0.000178 accuracy: 1.000000\n",
            "[51: 117/268] train loss: 0.013685 accuracy: 1.000000\n",
            "[51: 118/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[51: 119/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[51: 120/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[51: 120/268] test loss: 0.003852 accuracy: 1.000000\n",
            "[51: 121/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[51: 122/268] train loss: 0.000288 accuracy: 1.000000\n",
            "[51: 123/268] train loss: 0.000163 accuracy: 1.000000\n",
            "[51: 124/268] train loss: 0.000139 accuracy: 1.000000\n",
            "[51: 125/268] train loss: 0.000222 accuracy: 1.000000\n",
            "[51: 126/268] train loss: 0.000106 accuracy: 1.000000\n",
            "[51: 127/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[51: 128/268] train loss: 0.002854 accuracy: 1.000000\n",
            "[51: 129/268] train loss: 0.000817 accuracy: 1.000000\n",
            "[51: 130/268] train loss: 0.001391 accuracy: 1.000000\n",
            "[51: 130/268] test loss: 0.000019 accuracy: 1.000000\n",
            "[51: 131/268] train loss: 0.007932 accuracy: 1.000000\n",
            "[51: 132/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[51: 133/268] train loss: 0.000417 accuracy: 1.000000\n",
            "[51: 134/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[51: 135/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[51: 136/268] train loss: 0.002262 accuracy: 1.000000\n",
            "[51: 137/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[51: 138/268] train loss: 0.000079 accuracy: 1.000000\n",
            "[51: 139/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[51: 140/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[51: 140/268] test loss: 0.000207 accuracy: 1.000000\n",
            "[51: 141/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[51: 142/268] train loss: 0.000118 accuracy: 1.000000\n",
            "[51: 143/268] train loss: 0.654176 accuracy: 0.900000\n",
            "[51: 144/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[51: 145/268] train loss: 0.000119 accuracy: 1.000000\n",
            "[51: 146/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[51: 147/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[51: 148/268] train loss: 0.000282 accuracy: 1.000000\n",
            "[51: 149/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[51: 150/268] train loss: 0.000128 accuracy: 1.000000\n",
            "[51: 150/268] test loss: 0.006136 accuracy: 1.000000\n",
            "[51: 151/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[51: 152/268] train loss: 0.000995 accuracy: 1.000000\n",
            "[51: 153/268] train loss: 0.002406 accuracy: 1.000000\n",
            "[51: 154/268] train loss: 0.018028 accuracy: 1.000000\n",
            "[51: 155/268] train loss: 0.000385 accuracy: 1.000000\n",
            "[51: 156/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[51: 157/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[51: 158/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[51: 159/268] train loss: 0.005157 accuracy: 1.000000\n",
            "[51: 160/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[51: 160/268] test loss: 0.000117 accuracy: 1.000000\n",
            "[51: 161/268] train loss: 0.000082 accuracy: 1.000000\n",
            "[51: 162/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[51: 163/268] train loss: 0.000088 accuracy: 1.000000\n",
            "[51: 164/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[51: 165/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[51: 166/268] train loss: 0.000085 accuracy: 1.000000\n",
            "[51: 167/268] train loss: 0.000432 accuracy: 1.000000\n",
            "[51: 168/268] train loss: 0.000165 accuracy: 1.000000\n",
            "[51: 169/268] train loss: 0.031032 accuracy: 1.000000\n",
            "[51: 170/268] train loss: 0.002338 accuracy: 1.000000\n",
            "[51: 170/268] test loss: 0.002275 accuracy: 1.000000\n",
            "[51: 171/268] train loss: 0.000310 accuracy: 1.000000\n",
            "[51: 172/268] train loss: 0.000210 accuracy: 1.000000\n",
            "[51: 173/268] train loss: 0.000177 accuracy: 1.000000\n",
            "[51: 174/268] train loss: 0.000131 accuracy: 1.000000\n",
            "[51: 175/268] train loss: 0.009925 accuracy: 1.000000\n",
            "[51: 176/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[51: 177/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[51: 178/268] train loss: 0.000287 accuracy: 1.000000\n",
            "[51: 179/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[51: 180/268] train loss: 0.000095 accuracy: 1.000000\n",
            "[51: 180/268] test loss: 0.001374 accuracy: 1.000000\n",
            "[51: 181/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[51: 182/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[51: 183/268] train loss: 0.075182 accuracy: 0.900000\n",
            "[51: 184/268] train loss: 0.000316 accuracy: 1.000000\n",
            "[51: 185/268] train loss: 0.000884 accuracy: 1.000000\n",
            "[51: 186/268] train loss: 0.000230 accuracy: 1.000000\n",
            "[51: 187/268] train loss: 0.001077 accuracy: 1.000000\n",
            "[51: 188/268] train loss: 0.000213 accuracy: 1.000000\n",
            "[51: 189/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[51: 190/268] train loss: 0.000134 accuracy: 1.000000\n",
            "[51: 190/268] test loss: 0.033772 accuracy: 1.000000\n",
            "[51: 191/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[51: 192/268] train loss: 0.002426 accuracy: 1.000000\n",
            "[51: 193/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[51: 194/268] train loss: 0.000819 accuracy: 1.000000\n",
            "[51: 195/268] train loss: 0.000132 accuracy: 1.000000\n",
            "[51: 196/268] train loss: 0.005782 accuracy: 1.000000\n",
            "[51: 197/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[51: 198/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[51: 199/268] train loss: 0.000216 accuracy: 1.000000\n",
            "[51: 200/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[51: 200/268] test loss: 0.000460 accuracy: 1.000000\n",
            "[51: 201/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[51: 202/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[51: 203/268] train loss: 0.000126 accuracy: 1.000000\n",
            "[51: 204/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[51: 205/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[51: 206/268] train loss: 0.000709 accuracy: 1.000000\n",
            "[51: 207/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[51: 208/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[51: 209/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[51: 210/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[51: 210/268] test loss: 0.000124 accuracy: 1.000000\n",
            "[51: 211/268] train loss: 0.000126 accuracy: 1.000000\n",
            "[51: 212/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[51: 213/268] train loss: 0.001537 accuracy: 1.000000\n",
            "[51: 214/268] train loss: 0.001146 accuracy: 1.000000\n",
            "[51: 215/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[51: 216/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[51: 217/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[51: 218/268] train loss: 0.004463 accuracy: 1.000000\n",
            "[51: 219/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[51: 220/268] train loss: 0.000414 accuracy: 1.000000\n",
            "[51: 220/268] test loss: 0.000184 accuracy: 1.000000\n",
            "[51: 221/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[51: 222/268] train loss: 0.000115 accuracy: 1.000000\n",
            "[51: 223/268] train loss: 0.001494 accuracy: 1.000000\n",
            "[51: 224/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[51: 225/268] train loss: 0.001082 accuracy: 1.000000\n",
            "[51: 226/268] train loss: 0.000432 accuracy: 1.000000\n",
            "[51: 227/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[51: 228/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[51: 229/268] train loss: 0.000190 accuracy: 1.000000\n",
            "[51: 230/268] train loss: 0.000917 accuracy: 1.000000\n",
            "[51: 230/268] test loss: 0.000399 accuracy: 1.000000\n",
            "[51: 231/268] train loss: 0.000484 accuracy: 1.000000\n",
            "[51: 232/268] train loss: 0.013091 accuracy: 1.000000\n",
            "[51: 233/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[51: 234/268] train loss: 0.000278 accuracy: 1.000000\n",
            "[51: 235/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[51: 236/268] train loss: 0.000135 accuracy: 1.000000\n",
            "[51: 237/268] train loss: 0.000127 accuracy: 1.000000\n",
            "[51: 238/268] train loss: 0.001385 accuracy: 1.000000\n",
            "[51: 239/268] train loss: 0.005287 accuracy: 1.000000\n",
            "[51: 240/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[51: 240/268] test loss: 0.000229 accuracy: 1.000000\n",
            "[51: 241/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[51: 242/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[51: 243/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[51: 244/268] train loss: 0.000302 accuracy: 1.000000\n",
            "[51: 245/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[51: 246/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[51: 247/268] train loss: 0.000476 accuracy: 1.000000\n",
            "[51: 248/268] train loss: 0.000433 accuracy: 1.000000\n",
            "[51: 249/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[51: 250/268] train loss: 0.000186 accuracy: 1.000000\n",
            "[51: 250/268] test loss: 0.001373 accuracy: 1.000000\n",
            "[51: 251/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[51: 252/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[51: 253/268] train loss: 0.000398 accuracy: 1.000000\n",
            "[51: 254/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[51: 255/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[51: 256/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[51: 257/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[51: 258/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[51: 259/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[51: 260/268] train loss: 0.001058 accuracy: 1.000000\n",
            "[51: 260/268] test loss: 0.000281 accuracy: 1.000000\n",
            "[51: 261/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[51: 262/268] train loss: 0.000341 accuracy: 1.000000\n",
            "[51: 263/268] train loss: 0.000250 accuracy: 1.000000\n",
            "[51: 264/268] train loss: 0.001908 accuracy: 1.000000\n",
            "[51: 265/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[51: 266/268] train loss: 0.000201 accuracy: 1.000000\n",
            "[51: 267/268] train loss: 0.000045 accuracy: 0.800000\n",
            "[52: 0/268] train loss: 0.000917 accuracy: 1.000000\n",
            "[52: 0/268] test loss: 0.003047 accuracy: 1.000000\n",
            "[52: 1/268] train loss: 0.000088 accuracy: 1.000000\n",
            "[52: 2/268] train loss: 0.000067 accuracy: 1.000000\n",
            "[52: 3/268] train loss: 0.000668 accuracy: 1.000000\n",
            "[52: 4/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[52: 5/268] train loss: 0.000272 accuracy: 1.000000\n",
            "[52: 6/268] train loss: 0.003724 accuracy: 1.000000\n",
            "[52: 7/268] train loss: 0.000219 accuracy: 1.000000\n",
            "[52: 8/268] train loss: 0.000090 accuracy: 1.000000\n",
            "[52: 9/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[52: 10/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[52: 10/268] test loss: 0.001697 accuracy: 1.000000\n",
            "[52: 11/268] train loss: 0.002063 accuracy: 1.000000\n",
            "[52: 12/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[52: 13/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[52: 14/268] train loss: 0.000151 accuracy: 1.000000\n",
            "[52: 15/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[52: 16/268] train loss: 0.000955 accuracy: 1.000000\n",
            "[52: 17/268] train loss: 0.005168 accuracy: 1.000000\n",
            "[52: 18/268] train loss: 0.001358 accuracy: 1.000000\n",
            "[52: 19/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[52: 20/268] train loss: 0.010785 accuracy: 1.000000\n",
            "[52: 20/268] test loss: 0.000107 accuracy: 1.000000\n",
            "[52: 21/268] train loss: 0.009145 accuracy: 1.000000\n",
            "[52: 22/268] train loss: 0.000309 accuracy: 1.000000\n",
            "[52: 23/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[52: 24/268] train loss: 0.000098 accuracy: 1.000000\n",
            "[52: 25/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[52: 26/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[52: 27/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[52: 28/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[52: 29/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[52: 30/268] train loss: 0.000188 accuracy: 1.000000\n",
            "[52: 30/268] test loss: 0.002334 accuracy: 1.000000\n",
            "[52: 31/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[52: 32/268] train loss: 0.007178 accuracy: 1.000000\n",
            "[52: 33/268] train loss: 0.005922 accuracy: 1.000000\n",
            "[52: 34/268] train loss: 0.000200 accuracy: 1.000000\n",
            "[52: 35/268] train loss: 0.000632 accuracy: 1.000000\n",
            "[52: 36/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[52: 37/268] train loss: 0.003835 accuracy: 1.000000\n",
            "[52: 38/268] train loss: 0.000108 accuracy: 1.000000\n",
            "[52: 39/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[52: 40/268] train loss: 0.001117 accuracy: 1.000000\n",
            "[52: 40/268] test loss: 0.000559 accuracy: 1.000000\n",
            "[52: 41/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[52: 42/268] train loss: 0.000278 accuracy: 1.000000\n",
            "[52: 43/268] train loss: 0.000081 accuracy: 1.000000\n",
            "[52: 44/268] train loss: 0.001505 accuracy: 1.000000\n",
            "[52: 45/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[52: 46/268] train loss: 0.000124 accuracy: 1.000000\n",
            "[52: 47/268] train loss: 0.000461 accuracy: 1.000000\n",
            "[52: 48/268] train loss: 0.000195 accuracy: 1.000000\n",
            "[52: 49/268] train loss: 0.001765 accuracy: 1.000000\n",
            "[52: 50/268] train loss: 0.000127 accuracy: 1.000000\n",
            "[52: 50/268] test loss: 0.000274 accuracy: 1.000000\n",
            "[52: 51/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[52: 52/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[52: 53/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[52: 54/268] train loss: 0.001212 accuracy: 1.000000\n",
            "[52: 55/268] train loss: 0.000649 accuracy: 1.000000\n",
            "[52: 56/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[52: 57/268] train loss: 0.004020 accuracy: 1.000000\n",
            "[52: 58/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[52: 59/268] train loss: 0.000071 accuracy: 1.000000\n",
            "[52: 60/268] train loss: 0.472444 accuracy: 0.900000\n",
            "[52: 60/268] test loss: 0.000249 accuracy: 1.000000\n",
            "[52: 61/268] train loss: 0.000123 accuracy: 1.000000\n",
            "[52: 62/268] train loss: 0.000192 accuracy: 1.000000\n",
            "[52: 63/268] train loss: 0.000178 accuracy: 1.000000\n",
            "[52: 64/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[52: 65/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[52: 66/268] train loss: 0.003085 accuracy: 1.000000\n",
            "[52: 67/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[52: 68/268] train loss: 0.008595 accuracy: 1.000000\n",
            "[52: 69/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[52: 70/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[52: 70/268] test loss: 0.019343 accuracy: 1.000000\n",
            "[52: 71/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[52: 72/268] train loss: 0.000177 accuracy: 1.000000\n",
            "[52: 73/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[52: 74/268] train loss: 0.001668 accuracy: 1.000000\n",
            "[52: 75/268] train loss: 0.000064 accuracy: 1.000000\n",
            "[52: 76/268] train loss: 0.000602 accuracy: 1.000000\n",
            "[52: 77/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[52: 78/268] train loss: 0.006178 accuracy: 1.000000\n",
            "[52: 79/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[52: 80/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[52: 80/268] test loss: 0.000723 accuracy: 1.000000\n",
            "[52: 81/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[52: 82/268] train loss: 0.000085 accuracy: 1.000000\n",
            "[52: 83/268] train loss: 0.048978 accuracy: 1.000000\n",
            "[52: 84/268] train loss: 0.000271 accuracy: 1.000000\n",
            "[52: 85/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[52: 86/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[52: 87/268] train loss: 0.000748 accuracy: 1.000000\n",
            "[52: 88/268] train loss: 0.000634 accuracy: 1.000000\n",
            "[52: 89/268] train loss: 0.000203 accuracy: 1.000000\n",
            "[52: 90/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[52: 90/268] test loss: 0.000790 accuracy: 1.000000\n",
            "[52: 91/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[52: 92/268] train loss: 0.003436 accuracy: 1.000000\n",
            "[52: 93/268] train loss: 0.000152 accuracy: 1.000000\n",
            "[52: 94/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[52: 95/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[52: 96/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[52: 97/268] train loss: 0.000082 accuracy: 1.000000\n",
            "[52: 98/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[52: 99/268] train loss: 0.000188 accuracy: 1.000000\n",
            "[52: 100/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[52: 100/268] test loss: 0.000110 accuracy: 1.000000\n",
            "[52: 101/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[52: 102/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[52: 103/268] train loss: 0.000426 accuracy: 1.000000\n",
            "[52: 104/268] train loss: 0.001050 accuracy: 1.000000\n",
            "[52: 105/268] train loss: 0.001064 accuracy: 1.000000\n",
            "[52: 106/268] train loss: 0.000148 accuracy: 1.000000\n",
            "[52: 107/268] train loss: 0.000153 accuracy: 1.000000\n",
            "[52: 108/268] train loss: 0.000204 accuracy: 1.000000\n",
            "[52: 109/268] train loss: 0.000055 accuracy: 1.000000\n",
            "[52: 110/268] train loss: 0.000628 accuracy: 1.000000\n",
            "[52: 110/268] test loss: 0.000134 accuracy: 1.000000\n",
            "[52: 111/268] train loss: 0.000105 accuracy: 1.000000\n",
            "[52: 112/268] train loss: 0.001530 accuracy: 1.000000\n",
            "[52: 113/268] train loss: 0.000261 accuracy: 1.000000\n",
            "[52: 114/268] train loss: 0.042190 accuracy: 1.000000\n",
            "[52: 115/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[52: 116/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[52: 117/268] train loss: 0.018433 accuracy: 1.000000\n",
            "[52: 118/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[52: 119/268] train loss: 0.000239 accuracy: 1.000000\n",
            "[52: 120/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[52: 120/268] test loss: 0.000589 accuracy: 1.000000\n",
            "[52: 121/268] train loss: 0.000520 accuracy: 1.000000\n",
            "[52: 122/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[52: 123/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[52: 124/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[52: 125/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[52: 126/268] train loss: 0.008688 accuracy: 1.000000\n",
            "[52: 127/268] train loss: 0.004043 accuracy: 1.000000\n",
            "[52: 128/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[52: 129/268] train loss: 0.000398 accuracy: 1.000000\n",
            "[52: 130/268] train loss: 0.000094 accuracy: 1.000000\n",
            "[52: 130/268] test loss: 0.000049 accuracy: 1.000000\n",
            "[52: 131/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[52: 132/268] train loss: 0.000157 accuracy: 1.000000\n",
            "[52: 133/268] train loss: 0.000209 accuracy: 1.000000\n",
            "[52: 134/268] train loss: 0.000221 accuracy: 1.000000\n",
            "[52: 135/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[52: 136/268] train loss: 0.000526 accuracy: 1.000000\n",
            "[52: 137/268] train loss: 0.030954 accuracy: 1.000000\n",
            "[52: 138/268] train loss: 0.000258 accuracy: 1.000000\n",
            "[52: 139/268] train loss: 0.000536 accuracy: 1.000000\n",
            "[52: 140/268] train loss: 0.000468 accuracy: 1.000000\n",
            "[52: 140/268] test loss: 0.003795 accuracy: 1.000000\n",
            "[52: 141/268] train loss: 0.000749 accuracy: 1.000000\n",
            "[52: 142/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[52: 143/268] train loss: 0.011745 accuracy: 1.000000\n",
            "[52: 144/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[52: 145/268] train loss: 0.002355 accuracy: 1.000000\n",
            "[52: 146/268] train loss: 0.000323 accuracy: 1.000000\n",
            "[52: 147/268] train loss: 0.000066 accuracy: 1.000000\n",
            "[52: 148/268] train loss: 0.002197 accuracy: 1.000000\n",
            "[52: 149/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[52: 150/268] train loss: 0.001484 accuracy: 1.000000\n",
            "[52: 150/268] test loss: 0.000097 accuracy: 1.000000\n",
            "[52: 151/268] train loss: 0.003870 accuracy: 1.000000\n",
            "[52: 152/268] train loss: 0.000449 accuracy: 1.000000\n",
            "[52: 153/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[52: 154/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[52: 155/268] train loss: 0.000119 accuracy: 1.000000\n",
            "[52: 156/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[52: 157/268] train loss: 0.001111 accuracy: 1.000000\n",
            "[52: 158/268] train loss: 0.000141 accuracy: 1.000000\n",
            "[52: 159/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[52: 160/268] train loss: 0.001906 accuracy: 1.000000\n",
            "[52: 160/268] test loss: 0.001298 accuracy: 1.000000\n",
            "[52: 161/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[52: 162/268] train loss: 0.002176 accuracy: 1.000000\n",
            "[52: 163/268] train loss: 0.000076 accuracy: 1.000000\n",
            "[52: 164/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[52: 165/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[52: 166/268] train loss: 0.010207 accuracy: 1.000000\n",
            "[52: 167/268] train loss: 0.000094 accuracy: 1.000000\n",
            "[52: 168/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[52: 169/268] train loss: 0.000159 accuracy: 1.000000\n",
            "[52: 170/268] train loss: 0.000256 accuracy: 1.000000\n",
            "[52: 170/268] test loss: 0.004358 accuracy: 1.000000\n",
            "[52: 171/268] train loss: 0.010202 accuracy: 1.000000\n",
            "[52: 172/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[52: 173/268] train loss: 0.000230 accuracy: 1.000000\n",
            "[52: 174/268] train loss: 0.006088 accuracy: 1.000000\n",
            "[52: 175/268] train loss: 0.001589 accuracy: 1.000000\n",
            "[52: 176/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[52: 177/268] train loss: 0.000745 accuracy: 1.000000\n",
            "[52: 178/268] train loss: 0.003576 accuracy: 1.000000\n",
            "[52: 179/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[52: 180/268] train loss: 0.001031 accuracy: 1.000000\n",
            "[52: 180/268] test loss: 0.000537 accuracy: 1.000000\n",
            "[52: 181/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[52: 182/268] train loss: 0.005506 accuracy: 1.000000\n",
            "[52: 183/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[52: 184/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[52: 185/268] train loss: 0.000400 accuracy: 1.000000\n",
            "[52: 186/268] train loss: 0.000234 accuracy: 1.000000\n",
            "[52: 187/268] train loss: 0.000131 accuracy: 1.000000\n",
            "[52: 188/268] train loss: 0.003335 accuracy: 1.000000\n",
            "[52: 189/268] train loss: 0.009856 accuracy: 1.000000\n",
            "[52: 190/268] train loss: 0.001755 accuracy: 1.000000\n",
            "[52: 190/268] test loss: 0.007329 accuracy: 1.000000\n",
            "[52: 191/268] train loss: 0.000432 accuracy: 1.000000\n",
            "[52: 192/268] train loss: 0.002584 accuracy: 1.000000\n",
            "[52: 193/268] train loss: 0.000742 accuracy: 1.000000\n",
            "[52: 194/268] train loss: 0.000171 accuracy: 1.000000\n",
            "[52: 195/268] train loss: 0.029215 accuracy: 1.000000\n",
            "[52: 196/268] train loss: 0.004199 accuracy: 1.000000\n",
            "[52: 197/268] train loss: 0.000064 accuracy: 1.000000\n",
            "[52: 198/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[52: 199/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[52: 200/268] train loss: 0.000612 accuracy: 1.000000\n",
            "[52: 200/268] test loss: 0.000193 accuracy: 1.000000\n",
            "[52: 201/268] train loss: 0.000801 accuracy: 1.000000\n",
            "[52: 202/268] train loss: 0.002444 accuracy: 1.000000\n",
            "[52: 203/268] train loss: 0.001121 accuracy: 1.000000\n",
            "[52: 204/268] train loss: 0.006794 accuracy: 1.000000\n",
            "[52: 205/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[52: 206/268] train loss: 0.000272 accuracy: 1.000000\n",
            "[52: 207/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[52: 208/268] train loss: 0.007876 accuracy: 1.000000\n",
            "[52: 209/268] train loss: 0.001259 accuracy: 1.000000\n",
            "[52: 210/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[52: 210/268] test loss: 0.000301 accuracy: 1.000000\n",
            "[52: 211/268] train loss: 0.003247 accuracy: 1.000000\n",
            "[52: 212/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[52: 213/268] train loss: 0.000292 accuracy: 1.000000\n",
            "[52: 214/268] train loss: 0.000919 accuracy: 1.000000\n",
            "[52: 215/268] train loss: 0.000129 accuracy: 1.000000\n",
            "[52: 216/268] train loss: 0.006138 accuracy: 1.000000\n",
            "[52: 217/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[52: 218/268] train loss: 0.000230 accuracy: 1.000000\n",
            "[52: 219/268] train loss: 0.000520 accuracy: 1.000000\n",
            "[52: 220/268] train loss: 0.005914 accuracy: 1.000000\n",
            "[52: 220/268] test loss: 0.000367 accuracy: 1.000000\n",
            "[52: 221/268] train loss: 0.000582 accuracy: 1.000000\n",
            "[52: 222/268] train loss: 0.000254 accuracy: 1.000000\n",
            "[52: 223/268] train loss: 0.000455 accuracy: 1.000000\n",
            "[52: 224/268] train loss: 0.000738 accuracy: 1.000000\n",
            "[52: 225/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[52: 226/268] train loss: 0.000122 accuracy: 1.000000\n",
            "[52: 227/268] train loss: 0.000118 accuracy: 1.000000\n",
            "[52: 228/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[52: 229/268] train loss: 0.000119 accuracy: 1.000000\n",
            "[52: 230/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[52: 230/268] test loss: 0.000711 accuracy: 1.000000\n",
            "[52: 231/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[52: 232/268] train loss: 0.000071 accuracy: 1.000000\n",
            "[52: 233/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[52: 234/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[52: 235/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[52: 236/268] train loss: 0.000352 accuracy: 1.000000\n",
            "[52: 237/268] train loss: 0.027464 accuracy: 1.000000\n",
            "[52: 238/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[52: 239/268] train loss: 0.000106 accuracy: 1.000000\n",
            "[52: 240/268] train loss: 0.046601 accuracy: 1.000000\n",
            "[52: 240/268] test loss: 0.000789 accuracy: 1.000000\n",
            "[52: 241/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[52: 242/268] train loss: 0.000098 accuracy: 1.000000\n",
            "[52: 243/268] train loss: 0.001929 accuracy: 1.000000\n",
            "[52: 244/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[52: 245/268] train loss: 0.000190 accuracy: 1.000000\n",
            "[52: 246/268] train loss: 0.000146 accuracy: 1.000000\n",
            "[52: 247/268] train loss: 0.000938 accuracy: 1.000000\n",
            "[52: 248/268] train loss: 0.011682 accuracy: 1.000000\n",
            "[52: 249/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[52: 250/268] train loss: 0.000232 accuracy: 1.000000\n",
            "[52: 250/268] test loss: 0.000355 accuracy: 1.000000\n",
            "[52: 251/268] train loss: 0.013032 accuracy: 1.000000\n",
            "[52: 252/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[52: 253/268] train loss: 0.000673 accuracy: 1.000000\n",
            "[52: 254/268] train loss: 0.001342 accuracy: 1.000000\n",
            "[52: 255/268] train loss: 0.038895 accuracy: 1.000000\n",
            "[52: 256/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[52: 257/268] train loss: 0.000090 accuracy: 1.000000\n",
            "[52: 258/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[52: 259/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[52: 260/268] train loss: 0.000492 accuracy: 1.000000\n",
            "[52: 260/268] test loss: 0.000563 accuracy: 1.000000\n",
            "[52: 261/268] train loss: 0.000067 accuracy: 1.000000\n",
            "[52: 262/268] train loss: 0.072769 accuracy: 1.000000\n",
            "[52: 263/268] train loss: 0.000223 accuracy: 1.000000\n",
            "[52: 264/268] train loss: 0.000256 accuracy: 1.000000\n",
            "[52: 265/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[52: 266/268] train loss: 0.000250 accuracy: 1.000000\n",
            "[52: 267/268] train loss: 0.000146 accuracy: 0.800000\n",
            "[53: 0/268] train loss: 0.003782 accuracy: 1.000000\n",
            "[53: 0/268] test loss: 0.002932 accuracy: 1.000000\n",
            "[53: 1/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[53: 2/268] train loss: 0.001618 accuracy: 1.000000\n",
            "[53: 3/268] train loss: 0.000272 accuracy: 1.000000\n",
            "[53: 4/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[53: 5/268] train loss: 0.000143 accuracy: 1.000000\n",
            "[53: 6/268] train loss: 0.001052 accuracy: 1.000000\n",
            "[53: 7/268] train loss: 0.000269 accuracy: 1.000000\n",
            "[53: 8/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[53: 9/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[53: 10/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[53: 10/268] test loss: 0.001210 accuracy: 1.000000\n",
            "[53: 11/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[53: 12/268] train loss: 0.000764 accuracy: 1.000000\n",
            "[53: 13/268] train loss: 0.001619 accuracy: 1.000000\n",
            "[53: 14/268] train loss: 0.000169 accuracy: 1.000000\n",
            "[53: 15/268] train loss: 0.000118 accuracy: 1.000000\n",
            "[53: 16/268] train loss: 0.000192 accuracy: 1.000000\n",
            "[53: 17/268] train loss: 0.001288 accuracy: 1.000000\n",
            "[53: 18/268] train loss: 0.000298 accuracy: 1.000000\n",
            "[53: 19/268] train loss: 0.000752 accuracy: 1.000000\n",
            "[53: 20/268] train loss: 0.000213 accuracy: 1.000000\n",
            "[53: 20/268] test loss: 0.000176 accuracy: 1.000000\n",
            "[53: 21/268] train loss: 0.001323 accuracy: 1.000000\n",
            "[53: 22/268] train loss: 0.002627 accuracy: 1.000000\n",
            "[53: 23/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[53: 24/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[53: 25/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[53: 26/268] train loss: 0.000468 accuracy: 1.000000\n",
            "[53: 27/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[53: 28/268] train loss: 0.000108 accuracy: 1.000000\n",
            "[53: 29/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[53: 30/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[53: 30/268] test loss: 0.000414 accuracy: 1.000000\n",
            "[53: 31/268] train loss: 0.000317 accuracy: 1.000000\n",
            "[53: 32/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[53: 33/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[53: 34/268] train loss: 0.000603 accuracy: 1.000000\n",
            "[53: 35/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[53: 36/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[53: 37/268] train loss: 0.000132 accuracy: 1.000000\n",
            "[53: 38/268] train loss: 0.000267 accuracy: 1.000000\n",
            "[53: 39/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[53: 40/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[53: 40/268] test loss: 0.000292 accuracy: 1.000000\n",
            "[53: 41/268] train loss: 0.002661 accuracy: 1.000000\n",
            "[53: 42/268] train loss: 0.000099 accuracy: 1.000000\n",
            "[53: 43/268] train loss: 0.000244 accuracy: 1.000000\n",
            "[53: 44/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[53: 45/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[53: 46/268] train loss: 0.000756 accuracy: 1.000000\n",
            "[53: 47/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[53: 48/268] train loss: 0.000327 accuracy: 1.000000\n",
            "[53: 49/268] train loss: 0.000186 accuracy: 1.000000\n",
            "[53: 50/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[53: 50/268] test loss: 0.000192 accuracy: 1.000000\n",
            "[53: 51/268] train loss: 0.000055 accuracy: 1.000000\n",
            "[53: 52/268] train loss: 0.000758 accuracy: 1.000000\n",
            "[53: 53/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[53: 54/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[53: 55/268] train loss: 0.000094 accuracy: 1.000000\n",
            "[53: 56/268] train loss: 0.000654 accuracy: 1.000000\n",
            "[53: 57/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[53: 58/268] train loss: 0.000215 accuracy: 1.000000\n",
            "[53: 59/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[53: 60/268] train loss: 0.000158 accuracy: 1.000000\n",
            "[53: 60/268] test loss: 0.000085 accuracy: 1.000000\n",
            "[53: 61/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[53: 62/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[53: 63/268] train loss: 0.046958 accuracy: 1.000000\n",
            "[53: 64/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[53: 65/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[53: 66/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[53: 67/268] train loss: 0.000280 accuracy: 1.000000\n",
            "[53: 68/268] train loss: 0.000344 accuracy: 1.000000\n",
            "[53: 69/268] train loss: 0.002091 accuracy: 1.000000\n",
            "[53: 70/268] train loss: 0.000137 accuracy: 1.000000\n",
            "[53: 70/268] test loss: 0.000131 accuracy: 1.000000\n",
            "[53: 71/268] train loss: 0.000105 accuracy: 1.000000\n",
            "[53: 72/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[53: 73/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[53: 74/268] train loss: 0.000213 accuracy: 1.000000\n",
            "[53: 75/268] train loss: 0.000278 accuracy: 1.000000\n",
            "[53: 76/268] train loss: 0.000067 accuracy: 1.000000\n",
            "[53: 77/268] train loss: 0.009914 accuracy: 1.000000\n",
            "[53: 78/268] train loss: 0.002575 accuracy: 1.000000\n",
            "[53: 79/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[53: 80/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[53: 80/268] test loss: 0.000729 accuracy: 1.000000\n",
            "[53: 81/268] train loss: 0.000095 accuracy: 1.000000\n",
            "[53: 82/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[53: 83/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[53: 84/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[53: 85/268] train loss: 0.000124 accuracy: 1.000000\n",
            "[53: 86/268] train loss: 0.000129 accuracy: 1.000000\n",
            "[53: 87/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[53: 88/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[53: 89/268] train loss: 0.025013 accuracy: 1.000000\n",
            "[53: 90/268] train loss: 0.000700 accuracy: 1.000000\n",
            "[53: 90/268] test loss: 0.003807 accuracy: 1.000000\n",
            "[53: 91/268] train loss: 0.000138 accuracy: 1.000000\n",
            "[53: 92/268] train loss: 0.059142 accuracy: 1.000000\n",
            "[53: 93/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[53: 94/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[53: 95/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[53: 96/268] train loss: 0.003807 accuracy: 1.000000\n",
            "[53: 97/268] train loss: 0.000700 accuracy: 1.000000\n",
            "[53: 98/268] train loss: 0.000096 accuracy: 1.000000\n",
            "[53: 99/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[53: 100/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[53: 100/268] test loss: 0.000268 accuracy: 1.000000\n",
            "[53: 101/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[53: 102/268] train loss: 0.000509 accuracy: 1.000000\n",
            "[53: 103/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[53: 104/268] train loss: 0.000418 accuracy: 1.000000\n",
            "[53: 105/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[53: 106/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[53: 107/268] train loss: 0.001057 accuracy: 1.000000\n",
            "[53: 108/268] train loss: 0.000420 accuracy: 1.000000\n",
            "[53: 109/268] train loss: 0.000644 accuracy: 1.000000\n",
            "[53: 110/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[53: 110/268] test loss: 0.000076 accuracy: 1.000000\n",
            "[53: 111/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[53: 112/268] train loss: 0.000208 accuracy: 1.000000\n",
            "[53: 113/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[53: 114/268] train loss: 0.003991 accuracy: 1.000000\n",
            "[53: 115/268] train loss: 0.025647 accuracy: 1.000000\n",
            "[53: 116/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[53: 117/268] train loss: 0.000390 accuracy: 1.000000\n",
            "[53: 118/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[53: 119/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[53: 120/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[53: 120/268] test loss: 0.000390 accuracy: 1.000000\n",
            "[53: 121/268] train loss: 0.000377 accuracy: 1.000000\n",
            "[53: 122/268] train loss: 0.076432 accuracy: 0.900000\n",
            "[53: 123/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[53: 124/268] train loss: 0.000137 accuracy: 1.000000\n",
            "[53: 125/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[53: 126/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[53: 127/268] train loss: 0.000218 accuracy: 1.000000\n",
            "[53: 128/268] train loss: 0.000097 accuracy: 1.000000\n",
            "[53: 129/268] train loss: 0.001454 accuracy: 1.000000\n",
            "[53: 130/268] train loss: 0.000103 accuracy: 1.000000\n",
            "[53: 130/268] test loss: 0.001582 accuracy: 1.000000\n",
            "[53: 131/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[53: 132/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[53: 133/268] train loss: 0.000130 accuracy: 1.000000\n",
            "[53: 134/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[53: 135/268] train loss: 0.000224 accuracy: 1.000000\n",
            "[53: 136/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[53: 137/268] train loss: 0.004456 accuracy: 1.000000\n",
            "[53: 138/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[53: 139/268] train loss: 0.000258 accuracy: 1.000000\n",
            "[53: 140/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[53: 140/268] test loss: 0.000279 accuracy: 1.000000\n",
            "[53: 141/268] train loss: 0.000076 accuracy: 1.000000\n",
            "[53: 142/268] train loss: 0.003765 accuracy: 1.000000\n",
            "[53: 143/268] train loss: 0.000208 accuracy: 1.000000\n",
            "[53: 144/268] train loss: 0.000071 accuracy: 1.000000\n",
            "[53: 145/268] train loss: 0.017620 accuracy: 1.000000\n",
            "[53: 146/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[53: 147/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[53: 148/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[53: 149/268] train loss: 0.000313 accuracy: 1.000000\n",
            "[53: 150/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[53: 150/268] test loss: 0.001117 accuracy: 1.000000\n",
            "[53: 151/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[53: 152/268] train loss: 0.007285 accuracy: 1.000000\n",
            "[53: 153/268] train loss: 0.000108 accuracy: 1.000000\n",
            "[53: 154/268] train loss: 0.018659 accuracy: 1.000000\n",
            "[53: 155/268] train loss: 0.000486 accuracy: 1.000000\n",
            "[53: 156/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[53: 157/268] train loss: 0.000207 accuracy: 1.000000\n",
            "[53: 158/268] train loss: 0.041672 accuracy: 1.000000\n",
            "[53: 159/268] train loss: 0.000120 accuracy: 1.000000\n",
            "[53: 160/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[53: 160/268] test loss: 0.000241 accuracy: 1.000000\n",
            "[53: 161/268] train loss: 0.002324 accuracy: 1.000000\n",
            "[53: 162/268] train loss: 0.000361 accuracy: 1.000000\n",
            "[53: 163/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[53: 164/268] train loss: 0.000656 accuracy: 1.000000\n",
            "[53: 165/268] train loss: 0.000123 accuracy: 1.000000\n",
            "[53: 166/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[53: 167/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[53: 168/268] train loss: 0.000816 accuracy: 1.000000\n",
            "[53: 169/268] train loss: 0.000172 accuracy: 1.000000\n",
            "[53: 170/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[53: 170/268] test loss: 0.000119 accuracy: 1.000000\n",
            "[53: 171/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[53: 172/268] train loss: 0.000325 accuracy: 1.000000\n",
            "[53: 173/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[53: 174/268] train loss: 0.003956 accuracy: 1.000000\n",
            "[53: 175/268] train loss: 0.000187 accuracy: 1.000000\n",
            "[53: 176/268] train loss: 0.002806 accuracy: 1.000000\n",
            "[53: 177/268] train loss: 0.000124 accuracy: 1.000000\n",
            "[53: 178/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[53: 179/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[53: 180/268] train loss: 0.000052 accuracy: 1.000000\n",
            "[53: 180/268] test loss: 0.000946 accuracy: 1.000000\n",
            "[53: 181/268] train loss: 0.000602 accuracy: 1.000000\n",
            "[53: 182/268] train loss: 0.000967 accuracy: 1.000000\n",
            "[53: 183/268] train loss: 0.000266 accuracy: 1.000000\n",
            "[53: 184/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[53: 185/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[53: 186/268] train loss: 0.000321 accuracy: 1.000000\n",
            "[53: 187/268] train loss: 0.000477 accuracy: 1.000000\n",
            "[53: 188/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[53: 189/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[53: 190/268] train loss: 0.000237 accuracy: 1.000000\n",
            "[53: 190/268] test loss: 0.000050 accuracy: 1.000000\n",
            "[53: 191/268] train loss: 0.007025 accuracy: 1.000000\n",
            "[53: 192/268] train loss: 0.000172 accuracy: 1.000000\n",
            "[53: 193/268] train loss: 0.000370 accuracy: 1.000000\n",
            "[53: 194/268] train loss: 0.001335 accuracy: 1.000000\n",
            "[53: 195/268] train loss: 0.000888 accuracy: 1.000000\n",
            "[53: 196/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[53: 197/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[53: 198/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[53: 199/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[53: 200/268] train loss: 0.000556 accuracy: 1.000000\n",
            "[53: 200/268] test loss: 0.000208 accuracy: 1.000000\n",
            "[53: 201/268] train loss: 0.000888 accuracy: 1.000000\n",
            "[53: 202/268] train loss: 0.000120 accuracy: 1.000000\n",
            "[53: 203/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[53: 204/268] train loss: 0.000305 accuracy: 1.000000\n",
            "[53: 205/268] train loss: 0.000721 accuracy: 1.000000\n",
            "[53: 206/268] train loss: 0.000636 accuracy: 1.000000\n",
            "[53: 207/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[53: 208/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[53: 209/268] train loss: 0.004316 accuracy: 1.000000\n",
            "[53: 210/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[53: 210/268] test loss: 0.000063 accuracy: 1.000000\n",
            "[53: 211/268] train loss: 0.000731 accuracy: 1.000000\n",
            "[53: 212/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[53: 213/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[53: 214/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[53: 215/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[53: 216/268] train loss: 0.002858 accuracy: 1.000000\n",
            "[53: 217/268] train loss: 0.000337 accuracy: 1.000000\n",
            "[53: 218/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[53: 219/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[53: 220/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[53: 220/268] test loss: 0.000386 accuracy: 1.000000\n",
            "[53: 221/268] train loss: 0.000101 accuracy: 1.000000\n",
            "[53: 222/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[53: 223/268] train loss: 0.000145 accuracy: 1.000000\n",
            "[53: 224/268] train loss: 0.001938 accuracy: 1.000000\n",
            "[53: 225/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[53: 226/268] train loss: 0.000055 accuracy: 1.000000\n",
            "[53: 227/268] train loss: 0.000137 accuracy: 1.000000\n",
            "[53: 228/268] train loss: 0.000334 accuracy: 1.000000\n",
            "[53: 229/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[53: 230/268] train loss: 0.000088 accuracy: 1.000000\n",
            "[53: 230/268] test loss: 0.001517 accuracy: 1.000000\n",
            "[53: 231/268] train loss: 0.006191 accuracy: 1.000000\n",
            "[53: 232/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[53: 233/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[53: 234/268] train loss: 0.000365 accuracy: 1.000000\n",
            "[53: 235/268] train loss: 0.000208 accuracy: 1.000000\n",
            "[53: 236/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[53: 237/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[53: 238/268] train loss: 0.000112 accuracy: 1.000000\n",
            "[53: 239/268] train loss: 0.003549 accuracy: 1.000000\n",
            "[53: 240/268] train loss: 0.011624 accuracy: 1.000000\n",
            "[53: 240/268] test loss: 0.000218 accuracy: 1.000000\n",
            "[53: 241/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[53: 242/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[53: 243/268] train loss: 0.000257 accuracy: 1.000000\n",
            "[53: 244/268] train loss: 0.000092 accuracy: 1.000000\n",
            "[53: 245/268] train loss: 0.001213 accuracy: 1.000000\n",
            "[53: 246/268] train loss: 0.000501 accuracy: 1.000000\n",
            "[53: 247/268] train loss: 0.000991 accuracy: 1.000000\n",
            "[53: 248/268] train loss: 0.000107 accuracy: 1.000000\n",
            "[53: 249/268] train loss: 0.000962 accuracy: 1.000000\n",
            "[53: 250/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[53: 250/268] test loss: 0.001455 accuracy: 1.000000\n",
            "[53: 251/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[53: 252/268] train loss: 0.000140 accuracy: 1.000000\n",
            "[53: 253/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[53: 254/268] train loss: 0.001092 accuracy: 1.000000\n",
            "[53: 255/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[53: 256/268] train loss: 0.000113 accuracy: 1.000000\n",
            "[53: 257/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[53: 258/268] train loss: 0.001603 accuracy: 1.000000\n",
            "[53: 259/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[53: 260/268] train loss: 0.000529 accuracy: 1.000000\n",
            "[53: 260/268] test loss: 0.000093 accuracy: 1.000000\n",
            "[53: 261/268] train loss: 0.000359 accuracy: 1.000000\n",
            "[53: 262/268] train loss: 0.000105 accuracy: 1.000000\n",
            "[53: 263/268] train loss: 0.013822 accuracy: 1.000000\n",
            "[53: 264/268] train loss: 0.050943 accuracy: 1.000000\n",
            "[53: 265/268] train loss: 0.000635 accuracy: 1.000000\n",
            "[53: 266/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[53: 267/268] train loss: 0.000012 accuracy: 0.800000\n",
            "[54: 0/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[54: 0/268] test loss: 0.000527 accuracy: 1.000000\n",
            "[54: 1/268] train loss: 0.000090 accuracy: 1.000000\n",
            "[54: 2/268] train loss: 0.000208 accuracy: 1.000000\n",
            "[54: 3/268] train loss: 0.000090 accuracy: 1.000000\n",
            "[54: 4/268] train loss: 0.001027 accuracy: 1.000000\n",
            "[54: 5/268] train loss: 0.000398 accuracy: 1.000000\n",
            "[54: 6/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[54: 7/268] train loss: 0.000252 accuracy: 1.000000\n",
            "[54: 8/268] train loss: 0.000179 accuracy: 1.000000\n",
            "[54: 9/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[54: 10/268] train loss: 0.000117 accuracy: 1.000000\n",
            "[54: 10/268] test loss: 0.000022 accuracy: 1.000000\n",
            "[54: 11/268] train loss: 0.000356 accuracy: 1.000000\n",
            "[54: 12/268] train loss: 0.004047 accuracy: 1.000000\n",
            "[54: 13/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[54: 14/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[54: 15/268] train loss: 0.000390 accuracy: 1.000000\n",
            "[54: 16/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[54: 17/268] train loss: 0.000159 accuracy: 1.000000\n",
            "[54: 18/268] train loss: 0.000532 accuracy: 1.000000\n",
            "[54: 19/268] train loss: 0.000168 accuracy: 1.000000\n",
            "[54: 20/268] train loss: 0.000220 accuracy: 1.000000\n",
            "[54: 20/268] test loss: 0.000113 accuracy: 1.000000\n",
            "[54: 21/268] train loss: 0.000052 accuracy: 1.000000\n",
            "[54: 22/268] train loss: 0.000266 accuracy: 1.000000\n",
            "[54: 23/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[54: 24/268] train loss: 0.000302 accuracy: 1.000000\n",
            "[54: 25/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[54: 26/268] train loss: 0.000109 accuracy: 1.000000\n",
            "[54: 27/268] train loss: 0.000490 accuracy: 1.000000\n",
            "[54: 28/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[54: 29/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[54: 30/268] train loss: 0.000001 accuracy: 1.000000\n",
            "[54: 30/268] test loss: 0.000046 accuracy: 1.000000\n",
            "[54: 31/268] train loss: 0.000139 accuracy: 1.000000\n",
            "[54: 32/268] train loss: 0.083631 accuracy: 0.900000\n",
            "[54: 33/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[54: 34/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[54: 35/268] train loss: 0.000331 accuracy: 1.000000\n",
            "[54: 36/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[54: 37/268] train loss: 0.000614 accuracy: 1.000000\n",
            "[54: 38/268] train loss: 0.000297 accuracy: 1.000000\n",
            "[54: 39/268] train loss: 0.022774 accuracy: 1.000000\n",
            "[54: 40/268] train loss: 0.000360 accuracy: 1.000000\n",
            "[54: 40/268] test loss: 0.000471 accuracy: 1.000000\n",
            "[54: 41/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[54: 42/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[54: 43/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[54: 44/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[54: 45/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[54: 46/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[54: 47/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[54: 48/268] train loss: 0.005477 accuracy: 1.000000\n",
            "[54: 49/268] train loss: 0.026247 accuracy: 1.000000\n",
            "[54: 50/268] train loss: 0.000096 accuracy: 1.000000\n",
            "[54: 50/268] test loss: 0.000189 accuracy: 1.000000\n",
            "[54: 51/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[54: 52/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[54: 53/268] train loss: 0.002306 accuracy: 1.000000\n",
            "[54: 54/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[54: 55/268] train loss: 0.000124 accuracy: 1.000000\n",
            "[54: 56/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[54: 57/268] train loss: 0.000177 accuracy: 1.000000\n",
            "[54: 58/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[54: 59/268] train loss: 0.000491 accuracy: 1.000000\n",
            "[54: 60/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[54: 60/268] test loss: 0.000298 accuracy: 1.000000\n",
            "[54: 61/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[54: 62/268] train loss: 0.001129 accuracy: 1.000000\n",
            "[54: 63/268] train loss: 0.002372 accuracy: 1.000000\n",
            "[54: 64/268] train loss: 0.000052 accuracy: 1.000000\n",
            "[54: 65/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[54: 66/268] train loss: 0.008587 accuracy: 1.000000\n",
            "[54: 67/268] train loss: 0.337861 accuracy: 0.900000\n",
            "[54: 68/268] train loss: 0.000152 accuracy: 1.000000\n",
            "[54: 69/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[54: 70/268] train loss: 0.000135 accuracy: 1.000000\n",
            "[54: 70/268] test loss: 0.000923 accuracy: 1.000000\n",
            "[54: 71/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[54: 72/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[54: 73/268] train loss: 0.000095 accuracy: 1.000000\n",
            "[54: 74/268] train loss: 0.000081 accuracy: 1.000000\n",
            "[54: 75/268] train loss: 0.000128 accuracy: 1.000000\n",
            "[54: 76/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[54: 77/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[54: 78/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[54: 79/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[54: 80/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[54: 80/268] test loss: 0.005771 accuracy: 1.000000\n",
            "[54: 81/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[54: 82/268] train loss: 0.004606 accuracy: 1.000000\n",
            "[54: 83/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[54: 84/268] train loss: 0.001414 accuracy: 1.000000\n",
            "[54: 85/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[54: 86/268] train loss: 0.000331 accuracy: 1.000000\n",
            "[54: 87/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[54: 88/268] train loss: 0.003736 accuracy: 1.000000\n",
            "[54: 89/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[54: 90/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[54: 90/268] test loss: 0.000081 accuracy: 1.000000\n",
            "[54: 91/268] train loss: 0.000222 accuracy: 1.000000\n",
            "[54: 92/268] train loss: 0.000530 accuracy: 1.000000\n",
            "[54: 93/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[54: 94/268] train loss: 0.000057 accuracy: 1.000000\n",
            "[54: 95/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[54: 96/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[54: 97/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[54: 98/268] train loss: 0.000178 accuracy: 1.000000\n",
            "[54: 99/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[54: 100/268] train loss: 0.019282 accuracy: 1.000000\n",
            "[54: 100/268] test loss: 0.200633 accuracy: 0.900000\n",
            "[54: 101/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[54: 102/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[54: 103/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[54: 104/268] train loss: 0.041333 accuracy: 1.000000\n",
            "[54: 105/268] train loss: 0.000207 accuracy: 1.000000\n",
            "[54: 106/268] train loss: 0.000097 accuracy: 1.000000\n",
            "[54: 107/268] train loss: 0.001277 accuracy: 1.000000\n",
            "[54: 108/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[54: 109/268] train loss: 0.003423 accuracy: 1.000000\n",
            "[54: 110/268] train loss: 0.000647 accuracy: 1.000000\n",
            "[54: 110/268] test loss: 0.002742 accuracy: 1.000000\n",
            "[54: 111/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[54: 112/268] train loss: 0.000296 accuracy: 1.000000\n",
            "[54: 113/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[54: 114/268] train loss: 0.000145 accuracy: 1.000000\n",
            "[54: 115/268] train loss: 0.002295 accuracy: 1.000000\n",
            "[54: 116/268] train loss: 0.000387 accuracy: 1.000000\n",
            "[54: 117/268] train loss: 0.000366 accuracy: 1.000000\n",
            "[54: 118/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[54: 119/268] train loss: 0.191321 accuracy: 0.900000\n",
            "[54: 120/268] train loss: 0.012553 accuracy: 1.000000\n",
            "[54: 120/268] test loss: 0.000436 accuracy: 1.000000\n",
            "[54: 121/268] train loss: 0.000118 accuracy: 1.000000\n",
            "[54: 122/268] train loss: 0.000127 accuracy: 1.000000\n",
            "[54: 123/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[54: 124/268] train loss: 0.000337 accuracy: 1.000000\n",
            "[54: 125/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[54: 126/268] train loss: 0.000136 accuracy: 1.000000\n",
            "[54: 127/268] train loss: 0.002926 accuracy: 1.000000\n",
            "[54: 128/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[54: 129/268] train loss: 0.006830 accuracy: 1.000000\n",
            "[54: 130/268] train loss: 0.000103 accuracy: 1.000000\n",
            "[54: 130/268] test loss: 0.000129 accuracy: 1.000000\n",
            "[54: 131/268] train loss: 0.000143 accuracy: 1.000000\n",
            "[54: 132/268] train loss: 0.001798 accuracy: 1.000000\n",
            "[54: 133/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[54: 134/268] train loss: 0.000197 accuracy: 1.000000\n",
            "[54: 135/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[54: 136/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[54: 137/268] train loss: 0.002109 accuracy: 1.000000\n",
            "[54: 138/268] train loss: 0.000477 accuracy: 1.000000\n",
            "[54: 139/268] train loss: 0.000288 accuracy: 1.000000\n",
            "[54: 140/268] train loss: 0.000552 accuracy: 1.000000\n",
            "[54: 140/268] test loss: 0.000650 accuracy: 1.000000\n",
            "[54: 141/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[54: 142/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[54: 143/268] train loss: 0.000277 accuracy: 1.000000\n",
            "[54: 144/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[54: 145/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[54: 146/268] train loss: 0.004021 accuracy: 1.000000\n",
            "[54: 147/268] train loss: 0.000140 accuracy: 1.000000\n",
            "[54: 148/268] train loss: 0.074157 accuracy: 0.900000\n",
            "[54: 149/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[54: 150/268] train loss: 0.000079 accuracy: 1.000000\n",
            "[54: 150/268] test loss: 0.000198 accuracy: 1.000000\n",
            "[54: 151/268] train loss: 0.001907 accuracy: 1.000000\n",
            "[54: 152/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[54: 153/268] train loss: 0.003826 accuracy: 1.000000\n",
            "[54: 154/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[54: 155/268] train loss: 0.001681 accuracy: 1.000000\n",
            "[54: 156/268] train loss: 0.010704 accuracy: 1.000000\n",
            "[54: 157/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[54: 158/268] train loss: 0.001173 accuracy: 1.000000\n",
            "[54: 159/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[54: 160/268] train loss: 0.000763 accuracy: 1.000000\n",
            "[54: 160/268] test loss: 0.000284 accuracy: 1.000000\n",
            "[54: 161/268] train loss: 0.019030 accuracy: 1.000000\n",
            "[54: 162/268] train loss: 0.004093 accuracy: 1.000000\n",
            "[54: 163/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[54: 164/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[54: 165/268] train loss: 0.000231 accuracy: 1.000000\n",
            "[54: 166/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[54: 167/268] train loss: 0.000386 accuracy: 1.000000\n",
            "[54: 168/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[54: 169/268] train loss: 0.000216 accuracy: 1.000000\n",
            "[54: 170/268] train loss: 0.000179 accuracy: 1.000000\n",
            "[54: 170/268] test loss: 0.000092 accuracy: 1.000000\n",
            "[54: 171/268] train loss: 0.000210 accuracy: 1.000000\n",
            "[54: 172/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[54: 173/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[54: 174/268] train loss: 0.000104 accuracy: 1.000000\n",
            "[54: 175/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[54: 176/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[54: 177/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[54: 178/268] train loss: 0.000130 accuracy: 1.000000\n",
            "[54: 179/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[54: 180/268] train loss: 0.011712 accuracy: 1.000000\n",
            "[54: 180/268] test loss: 0.000093 accuracy: 1.000000\n",
            "[54: 181/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[54: 182/268] train loss: 0.000167 accuracy: 1.000000\n",
            "[54: 183/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[54: 184/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[54: 185/268] train loss: 0.000130 accuracy: 1.000000\n",
            "[54: 186/268] train loss: 0.000085 accuracy: 1.000000\n",
            "[54: 187/268] train loss: 0.000271 accuracy: 1.000000\n",
            "[54: 188/268] train loss: 0.000303 accuracy: 1.000000\n",
            "[54: 189/268] train loss: 0.000132 accuracy: 1.000000\n",
            "[54: 190/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[54: 190/268] test loss: 0.001973 accuracy: 1.000000\n",
            "[54: 191/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[54: 192/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[54: 193/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[54: 194/268] train loss: 0.000303 accuracy: 1.000000\n",
            "[54: 195/268] train loss: 0.000499 accuracy: 1.000000\n",
            "[54: 196/268] train loss: 0.000094 accuracy: 1.000000\n",
            "[54: 197/268] train loss: 0.005523 accuracy: 1.000000\n",
            "[54: 198/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[54: 199/268] train loss: 0.007505 accuracy: 1.000000\n",
            "[54: 200/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[54: 200/268] test loss: 0.000398 accuracy: 1.000000\n",
            "[54: 201/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[54: 202/268] train loss: 0.000425 accuracy: 1.000000\n",
            "[54: 203/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[54: 204/268] train loss: 0.000792 accuracy: 1.000000\n",
            "[54: 205/268] train loss: 0.000279 accuracy: 1.000000\n",
            "[54: 206/268] train loss: 0.000200 accuracy: 1.000000\n",
            "[54: 207/268] train loss: 0.000155 accuracy: 1.000000\n",
            "[54: 208/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[54: 209/268] train loss: 0.001709 accuracy: 1.000000\n",
            "[54: 210/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[54: 210/268] test loss: 0.000168 accuracy: 1.000000\n",
            "[54: 211/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[54: 212/268] train loss: 0.541426 accuracy: 0.800000\n",
            "[54: 213/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[54: 214/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[54: 215/268] train loss: 0.044202 accuracy: 1.000000\n",
            "[54: 216/268] train loss: 0.000147 accuracy: 1.000000\n",
            "[54: 217/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[54: 218/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[54: 219/268] train loss: 0.000249 accuracy: 1.000000\n",
            "[54: 220/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[54: 220/268] test loss: 0.006889 accuracy: 1.000000\n",
            "[54: 221/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[54: 222/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[54: 223/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[54: 224/268] train loss: 0.000327 accuracy: 1.000000\n",
            "[54: 225/268] train loss: 0.000173 accuracy: 1.000000\n",
            "[54: 226/268] train loss: 0.001033 accuracy: 1.000000\n",
            "[54: 227/268] train loss: 0.001494 accuracy: 1.000000\n",
            "[54: 228/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[54: 229/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[54: 230/268] train loss: 0.169767 accuracy: 0.900000\n",
            "[54: 230/268] test loss: 0.012958 accuracy: 1.000000\n",
            "[54: 231/268] train loss: 0.001324 accuracy: 1.000000\n",
            "[54: 232/268] train loss: 0.000474 accuracy: 1.000000\n",
            "[54: 233/268] train loss: 0.000098 accuracy: 1.000000\n",
            "[54: 234/268] train loss: 0.001142 accuracy: 1.000000\n",
            "[54: 235/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[54: 236/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[54: 237/268] train loss: 0.009595 accuracy: 1.000000\n",
            "[54: 238/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[54: 239/268] train loss: 0.000105 accuracy: 1.000000\n",
            "[54: 240/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[54: 240/268] test loss: 0.000393 accuracy: 1.000000\n",
            "[54: 241/268] train loss: 0.007571 accuracy: 1.000000\n",
            "[54: 242/268] train loss: 0.018506 accuracy: 1.000000\n",
            "[54: 243/268] train loss: 0.000698 accuracy: 1.000000\n",
            "[54: 244/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[54: 245/268] train loss: 0.000115 accuracy: 1.000000\n",
            "[54: 246/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[54: 247/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[54: 248/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[54: 249/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[54: 250/268] train loss: 0.000787 accuracy: 1.000000\n",
            "[54: 250/268] test loss: 0.000381 accuracy: 1.000000\n",
            "[54: 251/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[54: 252/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[54: 253/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[54: 254/268] train loss: 0.367537 accuracy: 0.900000\n",
            "[54: 255/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[54: 256/268] train loss: 0.000582 accuracy: 1.000000\n",
            "[54: 257/268] train loss: 0.000120 accuracy: 1.000000\n",
            "[54: 258/268] train loss: 0.000120 accuracy: 1.000000\n",
            "[54: 259/268] train loss: 0.001216 accuracy: 1.000000\n",
            "[54: 260/268] train loss: 0.111892 accuracy: 0.900000\n",
            "[54: 260/268] test loss: 0.000407 accuracy: 1.000000\n",
            "[54: 261/268] train loss: 0.056280 accuracy: 1.000000\n",
            "[54: 262/268] train loss: 0.000150 accuracy: 1.000000\n",
            "[54: 263/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[54: 264/268] train loss: 0.046674 accuracy: 1.000000\n",
            "[54: 265/268] train loss: 0.000093 accuracy: 1.000000\n",
            "[54: 266/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[54: 267/268] train loss: 0.000054 accuracy: 0.800000\n",
            "[55: 0/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[55: 0/268] test loss: 0.005224 accuracy: 1.000000\n",
            "[55: 1/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[55: 2/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[55: 3/268] train loss: 0.000131 accuracy: 1.000000\n",
            "[55: 4/268] train loss: 0.276729 accuracy: 0.900000\n",
            "[55: 5/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[55: 6/268] train loss: 0.000154 accuracy: 1.000000\n",
            "[55: 7/268] train loss: 0.007791 accuracy: 1.000000\n",
            "[55: 8/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[55: 9/268] train loss: 0.000380 accuracy: 1.000000\n",
            "[55: 10/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[55: 10/268] test loss: 0.000071 accuracy: 1.000000\n",
            "[55: 11/268] train loss: 0.002790 accuracy: 1.000000\n",
            "[55: 12/268] train loss: 0.011831 accuracy: 1.000000\n",
            "[55: 13/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[55: 14/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[55: 15/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[55: 16/268] train loss: 0.000186 accuracy: 1.000000\n",
            "[55: 17/268] train loss: 0.000300 accuracy: 1.000000\n",
            "[55: 18/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[55: 19/268] train loss: 0.000351 accuracy: 1.000000\n",
            "[55: 20/268] train loss: 0.000229 accuracy: 1.000000\n",
            "[55: 20/268] test loss: 0.000789 accuracy: 1.000000\n",
            "[55: 21/268] train loss: 0.221334 accuracy: 0.900000\n",
            "[55: 22/268] train loss: 0.005971 accuracy: 1.000000\n",
            "[55: 23/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[55: 24/268] train loss: 0.011892 accuracy: 1.000000\n",
            "[55: 25/268] train loss: 0.000338 accuracy: 1.000000\n",
            "[55: 26/268] train loss: 0.001380 accuracy: 1.000000\n",
            "[55: 27/268] train loss: 0.000123 accuracy: 1.000000\n",
            "[55: 28/268] train loss: 0.008619 accuracy: 1.000000\n",
            "[55: 29/268] train loss: 0.268704 accuracy: 0.900000\n",
            "[55: 30/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[55: 30/268] test loss: 0.000304 accuracy: 1.000000\n",
            "[55: 31/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[55: 32/268] train loss: 0.001718 accuracy: 1.000000\n",
            "[55: 33/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[55: 34/268] train loss: 0.000122 accuracy: 1.000000\n",
            "[55: 35/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[55: 36/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[55: 37/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[55: 38/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[55: 39/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[55: 40/268] train loss: 0.001441 accuracy: 1.000000\n",
            "[55: 40/268] test loss: 0.000758 accuracy: 1.000000\n",
            "[55: 41/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[55: 42/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[55: 43/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[55: 44/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[55: 45/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[55: 46/268] train loss: 0.003951 accuracy: 1.000000\n",
            "[55: 47/268] train loss: 0.048336 accuracy: 1.000000\n",
            "[55: 48/268] train loss: 0.000573 accuracy: 1.000000\n",
            "[55: 49/268] train loss: 0.000168 accuracy: 1.000000\n",
            "[55: 50/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[55: 50/268] test loss: 0.000391 accuracy: 1.000000\n",
            "[55: 51/268] train loss: 0.001880 accuracy: 1.000000\n",
            "[55: 52/268] train loss: 0.000443 accuracy: 1.000000\n",
            "[55: 53/268] train loss: 0.001522 accuracy: 1.000000\n",
            "[55: 54/268] train loss: 0.000221 accuracy: 1.000000\n",
            "[55: 55/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[55: 56/268] train loss: 0.002710 accuracy: 1.000000\n",
            "[55: 57/268] train loss: 0.000476 accuracy: 1.000000\n",
            "[55: 58/268] train loss: 0.000115 accuracy: 1.000000\n",
            "[55: 59/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[55: 60/268] train loss: 0.000428 accuracy: 1.000000\n",
            "[55: 60/268] test loss: 0.000731 accuracy: 1.000000\n",
            "[55: 61/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[55: 62/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[55: 63/268] train loss: 0.000399 accuracy: 1.000000\n",
            "[55: 64/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[55: 65/268] train loss: 0.001556 accuracy: 1.000000\n",
            "[55: 66/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[55: 67/268] train loss: 0.000113 accuracy: 1.000000\n",
            "[55: 68/268] train loss: 0.001098 accuracy: 1.000000\n",
            "[55: 69/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[55: 70/268] train loss: 0.165777 accuracy: 0.900000\n",
            "[55: 70/268] test loss: 0.002204 accuracy: 1.000000\n",
            "[55: 71/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[55: 72/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[55: 73/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[55: 74/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[55: 75/268] train loss: 0.000734 accuracy: 1.000000\n",
            "[55: 76/268] train loss: 0.038582 accuracy: 1.000000\n",
            "[55: 77/268] train loss: 0.000406 accuracy: 1.000000\n",
            "[55: 78/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[55: 79/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[55: 80/268] train loss: 0.000126 accuracy: 1.000000\n",
            "[55: 80/268] test loss: 0.000471 accuracy: 1.000000\n",
            "[55: 81/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[55: 82/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[55: 83/268] train loss: 0.000657 accuracy: 1.000000\n",
            "[55: 84/268] train loss: 0.000362 accuracy: 1.000000\n",
            "[55: 85/268] train loss: 0.001331 accuracy: 1.000000\n",
            "[55: 86/268] train loss: 0.012597 accuracy: 1.000000\n",
            "[55: 87/268] train loss: 0.000179 accuracy: 1.000000\n",
            "[55: 88/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[55: 89/268] train loss: 0.001621 accuracy: 1.000000\n",
            "[55: 90/268] train loss: 0.000277 accuracy: 1.000000\n",
            "[55: 90/268] test loss: 0.005699 accuracy: 1.000000\n",
            "[55: 91/268] train loss: 0.000300 accuracy: 1.000000\n",
            "[55: 92/268] train loss: 0.000243 accuracy: 1.000000\n",
            "[55: 93/268] train loss: 0.000066 accuracy: 1.000000\n",
            "[55: 94/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[55: 95/268] train loss: 0.001104 accuracy: 1.000000\n",
            "[55: 96/268] train loss: 0.000392 accuracy: 1.000000\n",
            "[55: 97/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[55: 98/268] train loss: 0.000197 accuracy: 1.000000\n",
            "[55: 99/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[55: 100/268] train loss: 0.000849 accuracy: 1.000000\n",
            "[55: 100/268] test loss: 0.000430 accuracy: 1.000000\n",
            "[55: 101/268] train loss: 0.000074 accuracy: 1.000000\n",
            "[55: 102/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[55: 103/268] train loss: 0.000374 accuracy: 1.000000\n",
            "[55: 104/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[55: 105/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[55: 106/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[55: 107/268] train loss: 0.000198 accuracy: 1.000000\n",
            "[55: 108/268] train loss: 0.000110 accuracy: 1.000000\n",
            "[55: 109/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[55: 110/268] train loss: 0.000327 accuracy: 1.000000\n",
            "[55: 110/268] test loss: 0.000688 accuracy: 1.000000\n",
            "[55: 111/268] train loss: 0.008392 accuracy: 1.000000\n",
            "[55: 112/268] train loss: 0.000148 accuracy: 1.000000\n",
            "[55: 113/268] train loss: 0.020082 accuracy: 1.000000\n",
            "[55: 114/268] train loss: 0.000224 accuracy: 1.000000\n",
            "[55: 115/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[55: 116/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[55: 117/268] train loss: 0.000088 accuracy: 1.000000\n",
            "[55: 118/268] train loss: 0.000234 accuracy: 1.000000\n",
            "[55: 119/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[55: 120/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[55: 120/268] test loss: 0.000289 accuracy: 1.000000\n",
            "[55: 121/268] train loss: 0.013747 accuracy: 1.000000\n",
            "[55: 122/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[55: 123/268] train loss: 0.000374 accuracy: 1.000000\n",
            "[55: 124/268] train loss: 0.000184 accuracy: 1.000000\n",
            "[55: 125/268] train loss: 0.000112 accuracy: 1.000000\n",
            "[55: 126/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[55: 127/268] train loss: 0.000196 accuracy: 1.000000\n",
            "[55: 128/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[55: 129/268] train loss: 0.000093 accuracy: 1.000000\n",
            "[55: 130/268] train loss: 0.000557 accuracy: 1.000000\n",
            "[55: 130/268] test loss: 0.000386 accuracy: 1.000000\n",
            "[55: 131/268] train loss: 0.043512 accuracy: 1.000000\n",
            "[55: 132/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[55: 133/268] train loss: 0.001038 accuracy: 1.000000\n",
            "[55: 134/268] train loss: 0.005688 accuracy: 1.000000\n",
            "[55: 135/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[55: 136/268] train loss: 0.000632 accuracy: 1.000000\n",
            "[55: 137/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[55: 138/268] train loss: 0.000079 accuracy: 1.000000\n",
            "[55: 139/268] train loss: 0.000201 accuracy: 1.000000\n",
            "[55: 140/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[55: 140/268] test loss: 0.000703 accuracy: 1.000000\n",
            "[55: 141/268] train loss: 0.003020 accuracy: 1.000000\n",
            "[55: 142/268] train loss: 0.000243 accuracy: 1.000000\n",
            "[55: 143/268] train loss: 0.000261 accuracy: 1.000000\n",
            "[55: 144/268] train loss: 0.000195 accuracy: 1.000000\n",
            "[55: 145/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[55: 146/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[55: 147/268] train loss: 0.000128 accuracy: 1.000000\n",
            "[55: 148/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[55: 149/268] train loss: 0.004682 accuracy: 1.000000\n",
            "[55: 150/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[55: 150/268] test loss: 0.000748 accuracy: 1.000000\n",
            "[55: 151/268] train loss: 0.181818 accuracy: 0.900000\n",
            "[55: 152/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[55: 153/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[55: 154/268] train loss: 0.007777 accuracy: 1.000000\n",
            "[55: 155/268] train loss: 0.001777 accuracy: 1.000000\n",
            "[55: 156/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[55: 157/268] train loss: 0.000109 accuracy: 1.000000\n",
            "[55: 158/268] train loss: 0.006991 accuracy: 1.000000\n",
            "[55: 159/268] train loss: 0.000185 accuracy: 1.000000\n",
            "[55: 160/268] train loss: 0.000530 accuracy: 1.000000\n",
            "[55: 160/268] test loss: 0.000599 accuracy: 1.000000\n",
            "[55: 161/268] train loss: 0.000958 accuracy: 1.000000\n",
            "[55: 162/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[55: 163/268] train loss: 0.000202 accuracy: 1.000000\n",
            "[55: 164/268] train loss: 0.009658 accuracy: 1.000000\n",
            "[55: 165/268] train loss: 0.001862 accuracy: 1.000000\n",
            "[55: 166/268] train loss: 0.000531 accuracy: 1.000000\n",
            "[55: 167/268] train loss: 0.000228 accuracy: 1.000000\n",
            "[55: 168/268] train loss: 0.000324 accuracy: 1.000000\n",
            "[55: 169/268] train loss: 0.000226 accuracy: 1.000000\n",
            "[55: 170/268] train loss: 0.005121 accuracy: 1.000000\n",
            "[55: 170/268] test loss: 0.000555 accuracy: 1.000000\n",
            "[55: 171/268] train loss: 0.000140 accuracy: 1.000000\n",
            "[55: 172/268] train loss: 0.000323 accuracy: 1.000000\n",
            "[55: 173/268] train loss: 0.001416 accuracy: 1.000000\n",
            "[55: 174/268] train loss: 0.000150 accuracy: 1.000000\n",
            "[55: 175/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[55: 176/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[55: 177/268] train loss: 0.000196 accuracy: 1.000000\n",
            "[55: 178/268] train loss: 0.000132 accuracy: 1.000000\n",
            "[55: 179/268] train loss: 0.417922 accuracy: 0.900000\n",
            "[55: 180/268] train loss: 0.000183 accuracy: 1.000000\n",
            "[55: 180/268] test loss: 0.001716 accuracy: 1.000000\n",
            "[55: 181/268] train loss: 0.000620 accuracy: 1.000000\n",
            "[55: 182/268] train loss: 0.003642 accuracy: 1.000000\n",
            "[55: 183/268] train loss: 0.000117 accuracy: 1.000000\n",
            "[55: 184/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[55: 185/268] train loss: 0.000413 accuracy: 1.000000\n",
            "[55: 186/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[55: 187/268] train loss: 0.004849 accuracy: 1.000000\n",
            "[55: 188/268] train loss: 0.000181 accuracy: 1.000000\n",
            "[55: 189/268] train loss: 0.000163 accuracy: 1.000000\n",
            "[55: 190/268] train loss: 0.000184 accuracy: 1.000000\n",
            "[55: 190/268] test loss: 0.003824 accuracy: 1.000000\n",
            "[55: 191/268] train loss: 0.015381 accuracy: 1.000000\n",
            "[55: 192/268] train loss: 0.000588 accuracy: 1.000000\n",
            "[55: 193/268] train loss: 0.000078 accuracy: 1.000000\n",
            "[55: 194/268] train loss: 0.004682 accuracy: 1.000000\n",
            "[55: 195/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[55: 196/268] train loss: 0.016383 accuracy: 1.000000\n",
            "[55: 197/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[55: 198/268] train loss: 0.001235 accuracy: 1.000000\n",
            "[55: 199/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[55: 200/268] train loss: 0.000110 accuracy: 1.000000\n",
            "[55: 200/268] test loss: 0.000217 accuracy: 1.000000\n",
            "[55: 201/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[55: 202/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[55: 203/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[55: 204/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[55: 205/268] train loss: 0.001166 accuracy: 1.000000\n",
            "[55: 206/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[55: 207/268] train loss: 0.000817 accuracy: 1.000000\n",
            "[55: 208/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[55: 209/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[55: 210/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[55: 210/268] test loss: 0.000059 accuracy: 1.000000\n",
            "[55: 211/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[55: 212/268] train loss: 0.001433 accuracy: 1.000000\n",
            "[55: 213/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[55: 214/268] train loss: 0.050616 accuracy: 1.000000\n",
            "[55: 215/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[55: 216/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[55: 217/268] train loss: 0.001927 accuracy: 1.000000\n",
            "[55: 218/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[55: 219/268] train loss: 0.043234 accuracy: 1.000000\n",
            "[55: 220/268] train loss: 0.000094 accuracy: 1.000000\n",
            "[55: 220/268] test loss: 0.000457 accuracy: 1.000000\n",
            "[55: 221/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[55: 222/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[55: 223/268] train loss: 0.000550 accuracy: 1.000000\n",
            "[55: 224/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[55: 225/268] train loss: 0.000529 accuracy: 1.000000\n",
            "[55: 226/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[55: 227/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[55: 228/268] train loss: 0.005647 accuracy: 1.000000\n",
            "[55: 229/268] train loss: 0.000570 accuracy: 1.000000\n",
            "[55: 230/268] train loss: 0.000599 accuracy: 1.000000\n",
            "[55: 230/268] test loss: 0.000027 accuracy: 1.000000\n",
            "[55: 231/268] train loss: 0.000119 accuracy: 1.000000\n",
            "[55: 232/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[55: 233/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[55: 234/268] train loss: 0.005799 accuracy: 1.000000\n",
            "[55: 235/268] train loss: 0.009671 accuracy: 1.000000\n",
            "[55: 236/268] train loss: 0.000077 accuracy: 1.000000\n",
            "[55: 237/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[55: 238/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[55: 239/268] train loss: 0.000081 accuracy: 1.000000\n",
            "[55: 240/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[55: 240/268] test loss: 0.000381 accuracy: 1.000000\n",
            "[55: 241/268] train loss: 0.001440 accuracy: 1.000000\n",
            "[55: 242/268] train loss: 0.000089 accuracy: 1.000000\n",
            "[55: 243/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[55: 244/268] train loss: 0.000262 accuracy: 1.000000\n",
            "[55: 245/268] train loss: 0.006949 accuracy: 1.000000\n",
            "[55: 246/268] train loss: 0.000455 accuracy: 1.000000\n",
            "[55: 247/268] train loss: 0.006176 accuracy: 1.000000\n",
            "[55: 248/268] train loss: 0.000451 accuracy: 1.000000\n",
            "[55: 249/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[55: 250/268] train loss: 0.007542 accuracy: 1.000000\n",
            "[55: 250/268] test loss: 0.000582 accuracy: 1.000000\n",
            "[55: 251/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[55: 252/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[55: 253/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[55: 254/268] train loss: 0.001221 accuracy: 1.000000\n",
            "[55: 255/268] train loss: 0.000579 accuracy: 1.000000\n",
            "[55: 256/268] train loss: 0.000057 accuracy: 1.000000\n",
            "[55: 257/268] train loss: 0.003562 accuracy: 1.000000\n",
            "[55: 258/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[55: 259/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[55: 260/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[55: 260/268] test loss: 0.001128 accuracy: 1.000000\n",
            "[55: 261/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[55: 262/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[55: 263/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[55: 264/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[55: 265/268] train loss: 0.000111 accuracy: 1.000000\n",
            "[55: 266/268] train loss: 0.005346 accuracy: 1.000000\n",
            "[55: 267/268] train loss: 0.001326 accuracy: 0.800000\n",
            "[56: 0/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[56: 0/268] test loss: 0.000144 accuracy: 1.000000\n",
            "[56: 1/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[56: 2/268] train loss: 0.000124 accuracy: 1.000000\n",
            "[56: 3/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[56: 4/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[56: 5/268] train loss: 0.006048 accuracy: 1.000000\n",
            "[56: 6/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[56: 7/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[56: 8/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[56: 9/268] train loss: 0.000076 accuracy: 1.000000\n",
            "[56: 10/268] train loss: 0.000308 accuracy: 1.000000\n",
            "[56: 10/268] test loss: 0.000011 accuracy: 1.000000\n",
            "[56: 11/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[56: 12/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[56: 13/268] train loss: 0.048435 accuracy: 1.000000\n",
            "[56: 14/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[56: 15/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[56: 16/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[56: 17/268] train loss: 0.000190 accuracy: 1.000000\n",
            "[56: 18/268] train loss: 0.017350 accuracy: 1.000000\n",
            "[56: 19/268] train loss: 0.000093 accuracy: 1.000000\n",
            "[56: 20/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[56: 20/268] test loss: 0.003759 accuracy: 1.000000\n",
            "[56: 21/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[56: 22/268] train loss: 0.000505 accuracy: 1.000000\n",
            "[56: 23/268] train loss: 0.000119 accuracy: 1.000000\n",
            "[56: 24/268] train loss: 0.000192 accuracy: 1.000000\n",
            "[56: 25/268] train loss: 0.000212 accuracy: 1.000000\n",
            "[56: 26/268] train loss: 0.000681 accuracy: 1.000000\n",
            "[56: 27/268] train loss: 0.022860 accuracy: 1.000000\n",
            "[56: 28/268] train loss: 0.196376 accuracy: 0.900000\n",
            "[56: 29/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[56: 30/268] train loss: 0.000697 accuracy: 1.000000\n",
            "[56: 30/268] test loss: 0.000114 accuracy: 1.000000\n",
            "[56: 31/268] train loss: 0.033403 accuracy: 1.000000\n",
            "[56: 32/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[56: 33/268] train loss: 0.002631 accuracy: 1.000000\n",
            "[56: 34/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[56: 35/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[56: 36/268] train loss: 0.000611 accuracy: 1.000000\n",
            "[56: 37/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[56: 38/268] train loss: 0.012596 accuracy: 1.000000\n",
            "[56: 39/268] train loss: 0.000710 accuracy: 1.000000\n",
            "[56: 40/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[56: 40/268] test loss: 0.000428 accuracy: 1.000000\n",
            "[56: 41/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[56: 42/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[56: 43/268] train loss: 0.172019 accuracy: 0.900000\n",
            "[56: 44/268] train loss: 0.000259 accuracy: 1.000000\n",
            "[56: 45/268] train loss: 0.009690 accuracy: 1.000000\n",
            "[56: 46/268] train loss: 0.001516 accuracy: 1.000000\n",
            "[56: 47/268] train loss: 0.009489 accuracy: 1.000000\n",
            "[56: 48/268] train loss: 0.125391 accuracy: 0.900000\n",
            "[56: 49/268] train loss: 0.000098 accuracy: 1.000000\n",
            "[56: 50/268] train loss: 0.000934 accuracy: 1.000000\n",
            "[56: 50/268] test loss: 0.022342 accuracy: 1.000000\n",
            "[56: 51/268] train loss: 0.004385 accuracy: 1.000000\n",
            "[56: 52/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[56: 53/268] train loss: 0.001262 accuracy: 1.000000\n",
            "[56: 54/268] train loss: 0.006874 accuracy: 1.000000\n",
            "[56: 55/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[56: 56/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[56: 57/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[56: 58/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[56: 59/268] train loss: 0.000251 accuracy: 1.000000\n",
            "[56: 60/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[56: 60/268] test loss: 0.000153 accuracy: 1.000000\n",
            "[56: 61/268] train loss: 0.001150 accuracy: 1.000000\n",
            "[56: 62/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[56: 63/268] train loss: 0.000220 accuracy: 1.000000\n",
            "[56: 64/268] train loss: 0.001605 accuracy: 1.000000\n",
            "[56: 65/268] train loss: 0.001158 accuracy: 1.000000\n",
            "[56: 66/268] train loss: 0.000315 accuracy: 1.000000\n",
            "[56: 67/268] train loss: 0.000993 accuracy: 1.000000\n",
            "[56: 68/268] train loss: 0.000135 accuracy: 1.000000\n",
            "[56: 69/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[56: 70/268] train loss: 0.000552 accuracy: 1.000000\n",
            "[56: 70/268] test loss: 0.020373 accuracy: 1.000000\n",
            "[56: 71/268] train loss: 0.000125 accuracy: 1.000000\n",
            "[56: 72/268] train loss: 0.000434 accuracy: 1.000000\n",
            "[56: 73/268] train loss: 0.000064 accuracy: 1.000000\n",
            "[56: 74/268] train loss: 0.005638 accuracy: 1.000000\n",
            "[56: 75/268] train loss: 0.000187 accuracy: 1.000000\n",
            "[56: 76/268] train loss: 0.452277 accuracy: 0.900000\n",
            "[56: 77/268] train loss: 0.107064 accuracy: 0.900000\n",
            "[56: 78/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[56: 79/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[56: 80/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[56: 80/268] test loss: 0.001085 accuracy: 1.000000\n",
            "[56: 81/268] train loss: 0.000066 accuracy: 1.000000\n",
            "[56: 82/268] train loss: 0.000163 accuracy: 1.000000\n",
            "[56: 83/268] train loss: 0.002710 accuracy: 1.000000\n",
            "[56: 84/268] train loss: 0.000111 accuracy: 1.000000\n",
            "[56: 85/268] train loss: 0.000994 accuracy: 1.000000\n",
            "[56: 86/268] train loss: 0.000551 accuracy: 1.000000\n",
            "[56: 87/268] train loss: 0.000579 accuracy: 1.000000\n",
            "[56: 88/268] train loss: 0.000071 accuracy: 1.000000\n",
            "[56: 89/268] train loss: 0.001480 accuracy: 1.000000\n",
            "[56: 90/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[56: 90/268] test loss: 0.000066 accuracy: 1.000000\n",
            "[56: 91/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[56: 92/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[56: 93/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[56: 94/268] train loss: 0.000446 accuracy: 1.000000\n",
            "[56: 95/268] train loss: 0.000245 accuracy: 1.000000\n",
            "[56: 96/268] train loss: 0.246718 accuracy: 0.900000\n",
            "[56: 97/268] train loss: 0.012585 accuracy: 1.000000\n",
            "[56: 98/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[56: 99/268] train loss: 0.027899 accuracy: 1.000000\n",
            "[56: 100/268] train loss: 0.006480 accuracy: 1.000000\n",
            "[56: 100/268] test loss: 0.001990 accuracy: 1.000000\n",
            "[56: 101/268] train loss: 0.001513 accuracy: 1.000000\n",
            "[56: 102/268] train loss: 0.000534 accuracy: 1.000000\n",
            "[56: 103/268] train loss: 0.000700 accuracy: 1.000000\n",
            "[56: 104/268] train loss: 0.016932 accuracy: 1.000000\n",
            "[56: 105/268] train loss: 0.003262 accuracy: 1.000000\n",
            "[56: 106/268] train loss: 0.000262 accuracy: 1.000000\n",
            "[56: 107/268] train loss: 0.004627 accuracy: 1.000000\n",
            "[56: 108/268] train loss: 0.000915 accuracy: 1.000000\n",
            "[56: 109/268] train loss: 0.000092 accuracy: 1.000000\n",
            "[56: 110/268] train loss: 0.000123 accuracy: 1.000000\n",
            "[56: 110/268] test loss: 0.000861 accuracy: 1.000000\n",
            "[56: 111/268] train loss: 0.073441 accuracy: 1.000000\n",
            "[56: 112/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[56: 113/268] train loss: 0.020382 accuracy: 1.000000\n",
            "[56: 114/268] train loss: 0.000340 accuracy: 1.000000\n",
            "[56: 115/268] train loss: 0.000358 accuracy: 1.000000\n",
            "[56: 116/268] train loss: 0.000388 accuracy: 1.000000\n",
            "[56: 117/268] train loss: 0.006461 accuracy: 1.000000\n",
            "[56: 118/268] train loss: 0.007026 accuracy: 1.000000\n",
            "[56: 119/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[56: 120/268] train loss: 0.001263 accuracy: 1.000000\n",
            "[56: 120/268] test loss: 0.110937 accuracy: 0.900000\n",
            "[56: 121/268] train loss: 0.001095 accuracy: 1.000000\n",
            "[56: 122/268] train loss: 0.000407 accuracy: 1.000000\n",
            "[56: 123/268] train loss: 0.000637 accuracy: 1.000000\n",
            "[56: 124/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[56: 125/268] train loss: 0.000599 accuracy: 1.000000\n",
            "[56: 126/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[56: 127/268] train loss: 0.003403 accuracy: 1.000000\n",
            "[56: 128/268] train loss: 0.314471 accuracy: 0.900000\n",
            "[56: 129/268] train loss: 0.000569 accuracy: 1.000000\n",
            "[56: 130/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[56: 130/268] test loss: 0.005253 accuracy: 1.000000\n",
            "[56: 131/268] train loss: 0.000797 accuracy: 1.000000\n",
            "[56: 132/268] train loss: 0.014948 accuracy: 1.000000\n",
            "[56: 133/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[56: 134/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[56: 135/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[56: 136/268] train loss: 0.018436 accuracy: 1.000000\n",
            "[56: 137/268] train loss: 0.000389 accuracy: 1.000000\n",
            "[56: 138/268] train loss: 0.000375 accuracy: 1.000000\n",
            "[56: 139/268] train loss: 0.000370 accuracy: 1.000000\n",
            "[56: 140/268] train loss: 0.001222 accuracy: 1.000000\n",
            "[56: 140/268] test loss: 0.003041 accuracy: 1.000000\n",
            "[56: 141/268] train loss: 0.000130 accuracy: 1.000000\n",
            "[56: 142/268] train loss: 0.017904 accuracy: 1.000000\n",
            "[56: 143/268] train loss: 0.008909 accuracy: 1.000000\n",
            "[56: 144/268] train loss: 0.002989 accuracy: 1.000000\n",
            "[56: 145/268] train loss: 0.426258 accuracy: 0.900000\n",
            "[56: 146/268] train loss: 0.084499 accuracy: 0.900000\n",
            "[56: 147/268] train loss: 0.001512 accuracy: 1.000000\n",
            "[56: 148/268] train loss: 0.000126 accuracy: 1.000000\n",
            "[56: 149/268] train loss: 0.000378 accuracy: 1.000000\n",
            "[56: 150/268] train loss: 0.000399 accuracy: 1.000000\n",
            "[56: 150/268] test loss: 0.000990 accuracy: 1.000000\n",
            "[56: 151/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[56: 152/268] train loss: 0.000299 accuracy: 1.000000\n",
            "[56: 153/268] train loss: 0.000108 accuracy: 1.000000\n",
            "[56: 154/268] train loss: 0.000806 accuracy: 1.000000\n",
            "[56: 155/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[56: 156/268] train loss: 0.000171 accuracy: 1.000000\n",
            "[56: 157/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[56: 158/268] train loss: 0.000242 accuracy: 1.000000\n",
            "[56: 159/268] train loss: 0.015347 accuracy: 1.000000\n",
            "[56: 160/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[56: 160/268] test loss: 0.000025 accuracy: 1.000000\n",
            "[56: 161/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[56: 162/268] train loss: 0.003771 accuracy: 1.000000\n",
            "[56: 163/268] train loss: 0.000577 accuracy: 1.000000\n",
            "[56: 164/268] train loss: 0.007474 accuracy: 1.000000\n",
            "[56: 165/268] train loss: 0.000383 accuracy: 1.000000\n",
            "[56: 166/268] train loss: 0.001068 accuracy: 1.000000\n",
            "[56: 167/268] train loss: 0.003145 accuracy: 1.000000\n",
            "[56: 168/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[56: 169/268] train loss: 0.026066 accuracy: 1.000000\n",
            "[56: 170/268] train loss: 0.001205 accuracy: 1.000000\n",
            "[56: 170/268] test loss: 0.000266 accuracy: 1.000000\n",
            "[56: 171/268] train loss: 0.246552 accuracy: 0.900000\n",
            "[56: 172/268] train loss: 0.000094 accuracy: 1.000000\n",
            "[56: 173/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[56: 174/268] train loss: 0.000174 accuracy: 1.000000\n",
            "[56: 175/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[56: 176/268] train loss: 0.000446 accuracy: 1.000000\n",
            "[56: 177/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[56: 178/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[56: 179/268] train loss: 0.000580 accuracy: 1.000000\n",
            "[56: 180/268] train loss: 0.000092 accuracy: 1.000000\n",
            "[56: 180/268] test loss: 0.000029 accuracy: 1.000000\n",
            "[56: 181/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[56: 182/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[56: 183/268] train loss: 0.000287 accuracy: 1.000000\n",
            "[56: 184/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[56: 185/268] train loss: 0.002180 accuracy: 1.000000\n",
            "[56: 186/268] train loss: 0.000166 accuracy: 1.000000\n",
            "[56: 187/268] train loss: 0.048599 accuracy: 1.000000\n",
            "[56: 188/268] train loss: 0.002024 accuracy: 1.000000\n",
            "[56: 189/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[56: 190/268] train loss: 0.000484 accuracy: 1.000000\n",
            "[56: 190/268] test loss: 0.000032 accuracy: 1.000000\n",
            "[56: 191/268] train loss: 0.002143 accuracy: 1.000000\n",
            "[56: 192/268] train loss: 0.000096 accuracy: 1.000000\n",
            "[56: 193/268] train loss: 0.000322 accuracy: 1.000000\n",
            "[56: 194/268] train loss: 0.001641 accuracy: 1.000000\n",
            "[56: 195/268] train loss: 0.000354 accuracy: 1.000000\n",
            "[56: 196/268] train loss: 0.001399 accuracy: 1.000000\n",
            "[56: 197/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[56: 198/268] train loss: 0.000415 accuracy: 1.000000\n",
            "[56: 199/268] train loss: 0.000211 accuracy: 1.000000\n",
            "[56: 200/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[56: 200/268] test loss: 0.000449 accuracy: 1.000000\n",
            "[56: 201/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[56: 202/268] train loss: 0.001332 accuracy: 1.000000\n",
            "[56: 203/268] train loss: 0.032451 accuracy: 1.000000\n",
            "[56: 204/268] train loss: 0.000837 accuracy: 1.000000\n",
            "[56: 205/268] train loss: 0.001684 accuracy: 1.000000\n",
            "[56: 206/268] train loss: 0.000057 accuracy: 1.000000\n",
            "[56: 207/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[56: 208/268] train loss: 0.000082 accuracy: 1.000000\n",
            "[56: 209/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[56: 210/268] train loss: 0.000178 accuracy: 1.000000\n",
            "[56: 210/268] test loss: 0.000124 accuracy: 1.000000\n",
            "[56: 211/268] train loss: 0.001327 accuracy: 1.000000\n",
            "[56: 212/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[56: 213/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[56: 214/268] train loss: 0.029454 accuracy: 1.000000\n",
            "[56: 215/268] train loss: 0.000506 accuracy: 1.000000\n",
            "[56: 216/268] train loss: 0.000177 accuracy: 1.000000\n",
            "[56: 217/268] train loss: 0.000153 accuracy: 1.000000\n",
            "[56: 218/268] train loss: 0.000354 accuracy: 1.000000\n",
            "[56: 219/268] train loss: 0.000300 accuracy: 1.000000\n",
            "[56: 220/268] train loss: 0.002108 accuracy: 1.000000\n",
            "[56: 220/268] test loss: 0.001928 accuracy: 1.000000\n",
            "[56: 221/268] train loss: 0.000330 accuracy: 1.000000\n",
            "[56: 222/268] train loss: 0.041306 accuracy: 1.000000\n",
            "[56: 223/268] train loss: 0.060870 accuracy: 1.000000\n",
            "[56: 224/268] train loss: 0.001053 accuracy: 1.000000\n",
            "[56: 225/268] train loss: 0.000569 accuracy: 1.000000\n",
            "[56: 226/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[56: 227/268] train loss: 0.005208 accuracy: 1.000000\n",
            "[56: 228/268] train loss: 0.003531 accuracy: 1.000000\n",
            "[56: 229/268] train loss: 0.004947 accuracy: 1.000000\n",
            "[56: 230/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[56: 230/268] test loss: 0.000018 accuracy: 1.000000\n",
            "[56: 231/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[56: 232/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[56: 233/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[56: 234/268] train loss: 0.000111 accuracy: 1.000000\n",
            "[56: 235/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[56: 236/268] train loss: 0.000299 accuracy: 1.000000\n",
            "[56: 237/268] train loss: 0.000984 accuracy: 1.000000\n",
            "[56: 238/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[56: 239/268] train loss: 0.000481 accuracy: 1.000000\n",
            "[56: 240/268] train loss: 0.002050 accuracy: 1.000000\n",
            "[56: 240/268] test loss: 0.000036 accuracy: 1.000000\n",
            "[56: 241/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[56: 242/268] train loss: 0.000812 accuracy: 1.000000\n",
            "[56: 243/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[56: 244/268] train loss: 0.000641 accuracy: 1.000000\n",
            "[56: 245/268] train loss: 0.000112 accuracy: 1.000000\n",
            "[56: 246/268] train loss: 0.000127 accuracy: 1.000000\n",
            "[56: 247/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[56: 248/268] train loss: 0.000447 accuracy: 1.000000\n",
            "[56: 249/268] train loss: 0.001060 accuracy: 1.000000\n",
            "[56: 250/268] train loss: 0.000120 accuracy: 1.000000\n",
            "[56: 250/268] test loss: 0.000080 accuracy: 1.000000\n",
            "[56: 251/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[56: 252/268] train loss: 0.000286 accuracy: 1.000000\n",
            "[56: 253/268] train loss: 0.000057 accuracy: 1.000000\n",
            "[56: 254/268] train loss: 0.000052 accuracy: 1.000000\n",
            "[56: 255/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[56: 256/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[56: 257/268] train loss: 0.005284 accuracy: 1.000000\n",
            "[56: 258/268] train loss: 0.000441 accuracy: 1.000000\n",
            "[56: 259/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[56: 260/268] train loss: 0.000544 accuracy: 1.000000\n",
            "[56: 260/268] test loss: 0.031833 accuracy: 1.000000\n",
            "[56: 261/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[56: 262/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[56: 263/268] train loss: 0.000445 accuracy: 1.000000\n",
            "[56: 264/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[56: 265/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[56: 266/268] train loss: 0.000316 accuracy: 1.000000\n",
            "[56: 267/268] train loss: 0.000072 accuracy: 0.800000\n",
            "[57: 0/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[57: 0/268] test loss: 0.000009 accuracy: 1.000000\n",
            "[57: 1/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[57: 2/268] train loss: 0.000057 accuracy: 1.000000\n",
            "[57: 3/268] train loss: 0.000180 accuracy: 1.000000\n",
            "[57: 4/268] train loss: 0.000322 accuracy: 1.000000\n",
            "[57: 5/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[57: 6/268] train loss: 0.000249 accuracy: 1.000000\n",
            "[57: 7/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[57: 8/268] train loss: 0.000878 accuracy: 1.000000\n",
            "[57: 9/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[57: 10/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[57: 10/268] test loss: 0.003049 accuracy: 1.000000\n",
            "[57: 11/268] train loss: 0.001171 accuracy: 1.000000\n",
            "[57: 12/268] train loss: 0.000055 accuracy: 1.000000\n",
            "[57: 13/268] train loss: 0.000283 accuracy: 1.000000\n",
            "[57: 14/268] train loss: 0.000123 accuracy: 1.000000\n",
            "[57: 15/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[57: 16/268] train loss: 0.001419 accuracy: 1.000000\n",
            "[57: 17/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[57: 18/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[57: 19/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[57: 20/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[57: 20/268] test loss: 0.002286 accuracy: 1.000000\n",
            "[57: 21/268] train loss: 0.000201 accuracy: 1.000000\n",
            "[57: 22/268] train loss: 0.010329 accuracy: 1.000000\n",
            "[57: 23/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[57: 24/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[57: 25/268] train loss: 0.000156 accuracy: 1.000000\n",
            "[57: 26/268] train loss: 0.001847 accuracy: 1.000000\n",
            "[57: 27/268] train loss: 0.000469 accuracy: 1.000000\n",
            "[57: 28/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[57: 29/268] train loss: 0.009776 accuracy: 1.000000\n",
            "[57: 30/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[57: 30/268] test loss: 0.013142 accuracy: 1.000000\n",
            "[57: 31/268] train loss: 0.000123 accuracy: 1.000000\n",
            "[57: 32/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[57: 33/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[57: 34/268] train loss: 0.000079 accuracy: 1.000000\n",
            "[57: 35/268] train loss: 0.003451 accuracy: 1.000000\n",
            "[57: 36/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[57: 37/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[57: 38/268] train loss: 0.000236 accuracy: 1.000000\n",
            "[57: 39/268] train loss: 0.000145 accuracy: 1.000000\n",
            "[57: 40/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[57: 40/268] test loss: 0.001085 accuracy: 1.000000\n",
            "[57: 41/268] train loss: 0.025163 accuracy: 1.000000\n",
            "[57: 42/268] train loss: 0.013986 accuracy: 1.000000\n",
            "[57: 43/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[57: 44/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[57: 45/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[57: 46/268] train loss: 0.000223 accuracy: 1.000000\n",
            "[57: 47/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[57: 48/268] train loss: 0.016458 accuracy: 1.000000\n",
            "[57: 49/268] train loss: 0.009439 accuracy: 1.000000\n",
            "[57: 50/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[57: 50/268] test loss: 0.000189 accuracy: 1.000000\n",
            "[57: 51/268] train loss: 0.002010 accuracy: 1.000000\n",
            "[57: 52/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[57: 53/268] train loss: 0.000089 accuracy: 1.000000\n",
            "[57: 54/268] train loss: 0.000074 accuracy: 1.000000\n",
            "[57: 55/268] train loss: 0.014183 accuracy: 1.000000\n",
            "[57: 56/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[57: 57/268] train loss: 0.006716 accuracy: 1.000000\n",
            "[57: 58/268] train loss: 0.001584 accuracy: 1.000000\n",
            "[57: 59/268] train loss: 0.000795 accuracy: 1.000000\n",
            "[57: 60/268] train loss: 0.000202 accuracy: 1.000000\n",
            "[57: 60/268] test loss: 0.000518 accuracy: 1.000000\n",
            "[57: 61/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[57: 62/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[57: 63/268] train loss: 0.000103 accuracy: 1.000000\n",
            "[57: 64/268] train loss: 0.004476 accuracy: 1.000000\n",
            "[57: 65/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[57: 66/268] train loss: 0.000137 accuracy: 1.000000\n",
            "[57: 67/268] train loss: 0.002272 accuracy: 1.000000\n",
            "[57: 68/268] train loss: 0.000200 accuracy: 1.000000\n",
            "[57: 69/268] train loss: 0.000749 accuracy: 1.000000\n",
            "[57: 70/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[57: 70/268] test loss: 0.002936 accuracy: 1.000000\n",
            "[57: 71/268] train loss: 0.000119 accuracy: 1.000000\n",
            "[57: 72/268] train loss: 0.001185 accuracy: 1.000000\n",
            "[57: 73/268] train loss: 0.000768 accuracy: 1.000000\n",
            "[57: 74/268] train loss: 0.002379 accuracy: 1.000000\n",
            "[57: 75/268] train loss: 0.000330 accuracy: 1.000000\n",
            "[57: 76/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[57: 77/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[57: 78/268] train loss: 0.000252 accuracy: 1.000000\n",
            "[57: 79/268] train loss: 0.000452 accuracy: 1.000000\n",
            "[57: 80/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[57: 80/268] test loss: 0.001140 accuracy: 1.000000\n",
            "[57: 81/268] train loss: 0.001498 accuracy: 1.000000\n",
            "[57: 82/268] train loss: 0.000216 accuracy: 1.000000\n",
            "[57: 83/268] train loss: 0.004237 accuracy: 1.000000\n",
            "[57: 84/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[57: 85/268] train loss: 0.000137 accuracy: 1.000000\n",
            "[57: 86/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[57: 87/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[57: 88/268] train loss: 0.000079 accuracy: 1.000000\n",
            "[57: 89/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[57: 90/268] train loss: 0.000260 accuracy: 1.000000\n",
            "[57: 90/268] test loss: 0.000591 accuracy: 1.000000\n",
            "[57: 91/268] train loss: 0.000229 accuracy: 1.000000\n",
            "[57: 92/268] train loss: 2.189995 accuracy: 0.600000\n",
            "[57: 93/268] train loss: 0.000136 accuracy: 1.000000\n",
            "[57: 94/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[57: 95/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[57: 96/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[57: 97/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[57: 98/268] train loss: 0.053665 accuracy: 1.000000\n",
            "[57: 99/268] train loss: 0.004138 accuracy: 1.000000\n",
            "[57: 100/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[57: 100/268] test loss: 0.000116 accuracy: 1.000000\n",
            "[57: 101/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[57: 102/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[57: 103/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[57: 104/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[57: 105/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[57: 106/268] train loss: 0.000131 accuracy: 1.000000\n",
            "[57: 107/268] train loss: 0.000385 accuracy: 1.000000\n",
            "[57: 108/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[57: 109/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[57: 110/268] train loss: 0.000067 accuracy: 1.000000\n",
            "[57: 110/268] test loss: 0.000860 accuracy: 1.000000\n",
            "[57: 111/268] train loss: 0.000729 accuracy: 1.000000\n",
            "[57: 112/268] train loss: 0.000252 accuracy: 1.000000\n",
            "[57: 113/268] train loss: 0.000402 accuracy: 1.000000\n",
            "[57: 114/268] train loss: 0.018706 accuracy: 1.000000\n",
            "[57: 115/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[57: 116/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[57: 117/268] train loss: 0.002224 accuracy: 1.000000\n",
            "[57: 118/268] train loss: 0.001308 accuracy: 1.000000\n",
            "[57: 119/268] train loss: 0.000182 accuracy: 1.000000\n",
            "[57: 120/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[57: 120/268] test loss: 0.002381 accuracy: 1.000000\n",
            "[57: 121/268] train loss: 0.002745 accuracy: 1.000000\n",
            "[57: 122/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[57: 123/268] train loss: 0.000146 accuracy: 1.000000\n",
            "[57: 124/268] train loss: 0.000055 accuracy: 1.000000\n",
            "[57: 125/268] train loss: 0.004097 accuracy: 1.000000\n",
            "[57: 126/268] train loss: 0.000212 accuracy: 1.000000\n",
            "[57: 127/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[57: 128/268] train loss: 0.000538 accuracy: 1.000000\n",
            "[57: 129/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[57: 130/268] train loss: 0.001399 accuracy: 1.000000\n",
            "[57: 130/268] test loss: 0.000578 accuracy: 1.000000\n",
            "[57: 131/268] train loss: 0.000565 accuracy: 1.000000\n",
            "[57: 132/268] train loss: 0.000877 accuracy: 1.000000\n",
            "[57: 133/268] train loss: 0.000342 accuracy: 1.000000\n",
            "[57: 134/268] train loss: 0.000088 accuracy: 1.000000\n",
            "[57: 135/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[57: 136/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[57: 137/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[57: 138/268] train loss: 0.000392 accuracy: 1.000000\n",
            "[57: 139/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[57: 140/268] train loss: 0.000433 accuracy: 1.000000\n",
            "[57: 140/268] test loss: 0.000066 accuracy: 1.000000\n",
            "[57: 141/268] train loss: 0.137065 accuracy: 1.000000\n",
            "[57: 142/268] train loss: 0.000346 accuracy: 1.000000\n",
            "[57: 143/268] train loss: 0.000085 accuracy: 1.000000\n",
            "[57: 144/268] train loss: 0.000196 accuracy: 1.000000\n",
            "[57: 145/268] train loss: 0.001663 accuracy: 1.000000\n",
            "[57: 146/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[57: 147/268] train loss: 0.000310 accuracy: 1.000000\n",
            "[57: 148/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[57: 149/268] train loss: 0.000111 accuracy: 1.000000\n",
            "[57: 150/268] train loss: 0.000830 accuracy: 1.000000\n",
            "[57: 150/268] test loss: 0.001051 accuracy: 1.000000\n",
            "[57: 151/268] train loss: 0.000472 accuracy: 1.000000\n",
            "[57: 152/268] train loss: 0.000477 accuracy: 1.000000\n",
            "[57: 153/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[57: 154/268] train loss: 0.000431 accuracy: 1.000000\n",
            "[57: 155/268] train loss: 0.063809 accuracy: 1.000000\n",
            "[57: 156/268] train loss: 0.003439 accuracy: 1.000000\n",
            "[57: 157/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[57: 158/268] train loss: 0.000418 accuracy: 1.000000\n",
            "[57: 159/268] train loss: 0.157268 accuracy: 0.900000\n",
            "[57: 160/268] train loss: 0.000212 accuracy: 1.000000\n",
            "[57: 160/268] test loss: 0.001511 accuracy: 1.000000\n",
            "[57: 161/268] train loss: 0.010519 accuracy: 1.000000\n",
            "[57: 162/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[57: 163/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[57: 164/268] train loss: 0.000640 accuracy: 1.000000\n",
            "[57: 165/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[57: 166/268] train loss: 0.002828 accuracy: 1.000000\n",
            "[57: 167/268] train loss: 0.001303 accuracy: 1.000000\n",
            "[57: 168/268] train loss: 0.000612 accuracy: 1.000000\n",
            "[57: 169/268] train loss: 0.000288 accuracy: 1.000000\n",
            "[57: 170/268] train loss: 0.000255 accuracy: 1.000000\n",
            "[57: 170/268] test loss: 0.001394 accuracy: 1.000000\n",
            "[57: 171/268] train loss: 0.000092 accuracy: 1.000000\n",
            "[57: 172/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[57: 173/268] train loss: 0.000105 accuracy: 1.000000\n",
            "[57: 174/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[57: 175/268] train loss: 0.000165 accuracy: 1.000000\n",
            "[57: 176/268] train loss: 0.135449 accuracy: 0.900000\n",
            "[57: 177/268] train loss: 0.000689 accuracy: 1.000000\n",
            "[57: 178/268] train loss: 0.000105 accuracy: 1.000000\n",
            "[57: 179/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[57: 180/268] train loss: 0.003591 accuracy: 1.000000\n",
            "[57: 180/268] test loss: 0.011285 accuracy: 1.000000\n",
            "[57: 181/268] train loss: 0.000094 accuracy: 1.000000\n",
            "[57: 182/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[57: 183/268] train loss: 0.000076 accuracy: 1.000000\n",
            "[57: 184/268] train loss: 0.000066 accuracy: 1.000000\n",
            "[57: 185/268] train loss: 0.000128 accuracy: 1.000000\n",
            "[57: 186/268] train loss: 0.000095 accuracy: 1.000000\n",
            "[57: 187/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[57: 188/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[57: 189/268] train loss: 0.000635 accuracy: 1.000000\n",
            "[57: 190/268] train loss: 0.007494 accuracy: 1.000000\n",
            "[57: 190/268] test loss: 0.000224 accuracy: 1.000000\n",
            "[57: 191/268] train loss: 0.000119 accuracy: 1.000000\n",
            "[57: 192/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[57: 193/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[57: 194/268] train loss: 0.000752 accuracy: 1.000000\n",
            "[57: 195/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[57: 196/268] train loss: 0.000140 accuracy: 1.000000\n",
            "[57: 197/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[57: 198/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[57: 199/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[57: 200/268] train loss: 0.006394 accuracy: 1.000000\n",
            "[57: 200/268] test loss: 0.001586 accuracy: 1.000000\n",
            "[57: 201/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[57: 202/268] train loss: 0.002267 accuracy: 1.000000\n",
            "[57: 203/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[57: 204/268] train loss: 0.000632 accuracy: 1.000000\n",
            "[57: 205/268] train loss: 0.020857 accuracy: 1.000000\n",
            "[57: 206/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[57: 207/268] train loss: 0.033166 accuracy: 1.000000\n",
            "[57: 208/268] train loss: 0.026982 accuracy: 1.000000\n",
            "[57: 209/268] train loss: 0.000077 accuracy: 1.000000\n",
            "[57: 210/268] train loss: 0.000094 accuracy: 1.000000\n",
            "[57: 210/268] test loss: 0.000544 accuracy: 1.000000\n",
            "[57: 211/268] train loss: 0.000076 accuracy: 1.000000\n",
            "[57: 212/268] train loss: 0.026039 accuracy: 1.000000\n",
            "[57: 213/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[57: 214/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[57: 215/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[57: 216/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[57: 217/268] train loss: 0.000421 accuracy: 1.000000\n",
            "[57: 218/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[57: 219/268] train loss: 0.012992 accuracy: 1.000000\n",
            "[57: 220/268] train loss: 0.001038 accuracy: 1.000000\n",
            "[57: 220/268] test loss: 0.000037 accuracy: 1.000000\n",
            "[57: 221/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[57: 222/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[57: 223/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[57: 224/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[57: 225/268] train loss: 0.003936 accuracy: 1.000000\n",
            "[57: 226/268] train loss: 0.000246 accuracy: 1.000000\n",
            "[57: 227/268] train loss: 0.000707 accuracy: 1.000000\n",
            "[57: 228/268] train loss: 0.000458 accuracy: 1.000000\n",
            "[57: 229/268] train loss: 0.000098 accuracy: 1.000000\n",
            "[57: 230/268] train loss: 0.000590 accuracy: 1.000000\n",
            "[57: 230/268] test loss: 0.000341 accuracy: 1.000000\n",
            "[57: 231/268] train loss: 0.000129 accuracy: 1.000000\n",
            "[57: 232/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[57: 233/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[57: 234/268] train loss: 0.000104 accuracy: 1.000000\n",
            "[57: 235/268] train loss: 0.021312 accuracy: 1.000000\n",
            "[57: 236/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[57: 237/268] train loss: 0.000487 accuracy: 1.000000\n",
            "[57: 238/268] train loss: 0.222200 accuracy: 0.900000\n",
            "[57: 239/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[57: 240/268] train loss: 0.000228 accuracy: 1.000000\n",
            "[57: 240/268] test loss: 0.000182 accuracy: 1.000000\n",
            "[57: 241/268] train loss: 0.000410 accuracy: 1.000000\n",
            "[57: 242/268] train loss: 0.003832 accuracy: 1.000000\n",
            "[57: 243/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[57: 244/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[57: 245/268] train loss: 0.001616 accuracy: 1.000000\n",
            "[57: 246/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[57: 247/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[57: 248/268] train loss: 0.000637 accuracy: 1.000000\n",
            "[57: 249/268] train loss: 0.000713 accuracy: 1.000000\n",
            "[57: 250/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[57: 250/268] test loss: 0.000205 accuracy: 1.000000\n",
            "[57: 251/268] train loss: 0.000717 accuracy: 1.000000\n",
            "[57: 252/268] train loss: 0.005546 accuracy: 1.000000\n",
            "[57: 253/268] train loss: 0.000160 accuracy: 1.000000\n",
            "[57: 254/268] train loss: 0.000934 accuracy: 1.000000\n",
            "[57: 255/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[57: 256/268] train loss: 0.000098 accuracy: 1.000000\n",
            "[57: 257/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[57: 258/268] train loss: 0.000752 accuracy: 1.000000\n",
            "[57: 259/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[57: 260/268] train loss: 0.001607 accuracy: 1.000000\n",
            "[57: 260/268] test loss: 0.009182 accuracy: 1.000000\n",
            "[57: 261/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[57: 262/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[57: 263/268] train loss: 0.000238 accuracy: 1.000000\n",
            "[57: 264/268] train loss: 0.002794 accuracy: 1.000000\n",
            "[57: 265/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[57: 266/268] train loss: 0.000781 accuracy: 1.000000\n",
            "[57: 267/268] train loss: 0.000006 accuracy: 0.800000\n",
            "[58: 0/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[58: 0/268] test loss: 0.000064 accuracy: 1.000000\n",
            "[58: 1/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[58: 2/268] train loss: 0.004249 accuracy: 1.000000\n",
            "[58: 3/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[58: 4/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[58: 5/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[58: 6/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[58: 7/268] train loss: 0.000105 accuracy: 1.000000\n",
            "[58: 8/268] train loss: 0.019317 accuracy: 1.000000\n",
            "[58: 9/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[58: 10/268] train loss: 0.000149 accuracy: 1.000000\n",
            "[58: 10/268] test loss: 0.002147 accuracy: 1.000000\n",
            "[58: 11/268] train loss: 0.000105 accuracy: 1.000000\n",
            "[58: 12/268] train loss: 0.000109 accuracy: 1.000000\n",
            "[58: 13/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[58: 14/268] train loss: 0.000139 accuracy: 1.000000\n",
            "[58: 15/268] train loss: 0.000347 accuracy: 1.000000\n",
            "[58: 16/268] train loss: 0.000090 accuracy: 1.000000\n",
            "[58: 17/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[58: 18/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[58: 19/268] train loss: 0.000796 accuracy: 1.000000\n",
            "[58: 20/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[58: 20/268] test loss: 0.000109 accuracy: 1.000000\n",
            "[58: 21/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[58: 22/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[58: 23/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[58: 24/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[58: 25/268] train loss: 0.001775 accuracy: 1.000000\n",
            "[58: 26/268] train loss: 0.000052 accuracy: 1.000000\n",
            "[58: 27/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[58: 28/268] train loss: 0.000224 accuracy: 1.000000\n",
            "[58: 29/268] train loss: 0.000245 accuracy: 1.000000\n",
            "[58: 30/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[58: 30/268] test loss: 0.000027 accuracy: 1.000000\n",
            "[58: 31/268] train loss: 0.000228 accuracy: 1.000000\n",
            "[58: 32/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[58: 33/268] train loss: 0.000290 accuracy: 1.000000\n",
            "[58: 34/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[58: 35/268] train loss: 0.000211 accuracy: 1.000000\n",
            "[58: 36/268] train loss: 0.003801 accuracy: 1.000000\n",
            "[58: 37/268] train loss: 0.000133 accuracy: 1.000000\n",
            "[58: 38/268] train loss: 0.000264 accuracy: 1.000000\n",
            "[58: 39/268] train loss: 0.000077 accuracy: 1.000000\n",
            "[58: 40/268] train loss: 0.000209 accuracy: 1.000000\n",
            "[58: 40/268] test loss: 0.000107 accuracy: 1.000000\n",
            "[58: 41/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[58: 42/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[58: 43/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[58: 44/268] train loss: 0.000692 accuracy: 1.000000\n",
            "[58: 45/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[58: 46/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[58: 47/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[58: 48/268] train loss: 0.000152 accuracy: 1.000000\n",
            "[58: 49/268] train loss: 0.000566 accuracy: 1.000000\n",
            "[58: 50/268] train loss: 0.000268 accuracy: 1.000000\n",
            "[58: 50/268] test loss: 0.000312 accuracy: 1.000000\n",
            "[58: 51/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[58: 52/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[58: 53/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[58: 54/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[58: 55/268] train loss: 0.000121 accuracy: 1.000000\n",
            "[58: 56/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[58: 57/268] train loss: 0.000354 accuracy: 1.000000\n",
            "[58: 58/268] train loss: 0.000423 accuracy: 1.000000\n",
            "[58: 59/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[58: 60/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[58: 60/268] test loss: 0.000249 accuracy: 1.000000\n",
            "[58: 61/268] train loss: 0.001387 accuracy: 1.000000\n",
            "[58: 62/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[58: 63/268] train loss: 0.000120 accuracy: 1.000000\n",
            "[58: 64/268] train loss: 0.000584 accuracy: 1.000000\n",
            "[58: 65/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[58: 66/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[58: 67/268] train loss: 0.000097 accuracy: 1.000000\n",
            "[58: 68/268] train loss: 0.000312 accuracy: 1.000000\n",
            "[58: 69/268] train loss: 0.017794 accuracy: 1.000000\n",
            "[58: 70/268] train loss: 0.000197 accuracy: 1.000000\n",
            "[58: 70/268] test loss: 0.000262 accuracy: 1.000000\n",
            "[58: 71/268] train loss: 0.000419 accuracy: 1.000000\n",
            "[58: 72/268] train loss: 0.000320 accuracy: 1.000000\n",
            "[58: 73/268] train loss: 0.000001 accuracy: 1.000000\n",
            "[58: 74/268] train loss: 0.000483 accuracy: 1.000000\n",
            "[58: 75/268] train loss: 0.000632 accuracy: 1.000000\n",
            "[58: 76/268] train loss: 0.005301 accuracy: 1.000000\n",
            "[58: 77/268] train loss: 0.002298 accuracy: 1.000000\n",
            "[58: 78/268] train loss: 0.034866 accuracy: 1.000000\n",
            "[58: 79/268] train loss: 0.002126 accuracy: 1.000000\n",
            "[58: 80/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[58: 80/268] test loss: 0.000324 accuracy: 1.000000\n",
            "[58: 81/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[58: 82/268] train loss: 0.002540 accuracy: 1.000000\n",
            "[58: 83/268] train loss: 0.509424 accuracy: 0.800000\n",
            "[58: 84/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[58: 85/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[58: 86/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[58: 87/268] train loss: 0.000209 accuracy: 1.000000\n",
            "[58: 88/268] train loss: 0.000239 accuracy: 1.000000\n",
            "[58: 89/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[58: 90/268] train loss: 0.000417 accuracy: 1.000000\n",
            "[58: 90/268] test loss: 0.003018 accuracy: 1.000000\n",
            "[58: 91/268] train loss: 0.000625 accuracy: 1.000000\n",
            "[58: 92/268] train loss: 0.000367 accuracy: 1.000000\n",
            "[58: 93/268] train loss: 0.016191 accuracy: 1.000000\n",
            "[58: 94/268] train loss: 0.243209 accuracy: 0.900000\n",
            "[58: 95/268] train loss: 0.000094 accuracy: 1.000000\n",
            "[58: 96/268] train loss: 0.001385 accuracy: 1.000000\n",
            "[58: 97/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[58: 98/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[58: 99/268] train loss: 0.003090 accuracy: 1.000000\n",
            "[58: 100/268] train loss: 0.000414 accuracy: 1.000000\n",
            "[58: 100/268] test loss: 0.000145 accuracy: 1.000000\n",
            "[58: 101/268] train loss: 0.000275 accuracy: 1.000000\n",
            "[58: 102/268] train loss: 0.001686 accuracy: 1.000000\n",
            "[58: 103/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[58: 104/268] train loss: 0.000731 accuracy: 1.000000\n",
            "[58: 105/268] train loss: 0.000205 accuracy: 1.000000\n",
            "[58: 106/268] train loss: 0.000212 accuracy: 1.000000\n",
            "[58: 107/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[58: 108/268] train loss: 0.000821 accuracy: 1.000000\n",
            "[58: 109/268] train loss: 0.000864 accuracy: 1.000000\n",
            "[58: 110/268] train loss: 0.000214 accuracy: 1.000000\n",
            "[58: 110/268] test loss: 0.001146 accuracy: 1.000000\n",
            "[58: 111/268] train loss: 0.003608 accuracy: 1.000000\n",
            "[58: 112/268] train loss: 0.000147 accuracy: 1.000000\n",
            "[58: 113/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[58: 114/268] train loss: 0.000758 accuracy: 1.000000\n",
            "[58: 115/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[58: 116/268] train loss: 0.006654 accuracy: 1.000000\n",
            "[58: 117/268] train loss: 0.000393 accuracy: 1.000000\n",
            "[58: 118/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[58: 119/268] train loss: 0.002049 accuracy: 1.000000\n",
            "[58: 120/268] train loss: 0.018465 accuracy: 1.000000\n",
            "[58: 120/268] test loss: 0.000785 accuracy: 1.000000\n",
            "[58: 121/268] train loss: 0.027797 accuracy: 1.000000\n",
            "[58: 122/268] train loss: 0.000459 accuracy: 1.000000\n",
            "[58: 123/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[58: 124/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[58: 125/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[58: 126/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[58: 127/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[58: 128/268] train loss: 0.000052 accuracy: 1.000000\n",
            "[58: 129/268] train loss: 0.000897 accuracy: 1.000000\n",
            "[58: 130/268] train loss: 0.001178 accuracy: 1.000000\n",
            "[58: 130/268] test loss: 0.001325 accuracy: 1.000000\n",
            "[58: 131/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[58: 132/268] train loss: 0.001430 accuracy: 1.000000\n",
            "[58: 133/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[58: 134/268] train loss: 0.001256 accuracy: 1.000000\n",
            "[58: 135/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[58: 136/268] train loss: 0.000196 accuracy: 1.000000\n",
            "[58: 137/268] train loss: 0.001775 accuracy: 1.000000\n",
            "[58: 138/268] train loss: 0.000217 accuracy: 1.000000\n",
            "[58: 139/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[58: 140/268] train loss: 0.024900 accuracy: 1.000000\n",
            "[58: 140/268] test loss: 0.008749 accuracy: 1.000000\n",
            "[58: 141/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[58: 142/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[58: 143/268] train loss: 0.000963 accuracy: 1.000000\n",
            "[58: 144/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[58: 145/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[58: 146/268] train loss: 0.000936 accuracy: 1.000000\n",
            "[58: 147/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[58: 148/268] train loss: 0.000254 accuracy: 1.000000\n",
            "[58: 149/268] train loss: 0.000397 accuracy: 1.000000\n",
            "[58: 150/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[58: 150/268] test loss: 0.000462 accuracy: 1.000000\n",
            "[58: 151/268] train loss: 0.000227 accuracy: 1.000000\n",
            "[58: 152/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[58: 153/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[58: 154/268] train loss: 0.000399 accuracy: 1.000000\n",
            "[58: 155/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[58: 156/268] train loss: 0.260803 accuracy: 0.900000\n",
            "[58: 157/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[58: 158/268] train loss: 0.003706 accuracy: 1.000000\n",
            "[58: 159/268] train loss: 0.001489 accuracy: 1.000000\n",
            "[58: 160/268] train loss: 0.000140 accuracy: 1.000000\n",
            "[58: 160/268] test loss: 0.002862 accuracy: 1.000000\n",
            "[58: 161/268] train loss: 0.000081 accuracy: 1.000000\n",
            "[58: 162/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[58: 163/268] train loss: 0.000096 accuracy: 1.000000\n",
            "[58: 164/268] train loss: 0.000644 accuracy: 1.000000\n",
            "[58: 165/268] train loss: 0.000147 accuracy: 1.000000\n",
            "[58: 166/268] train loss: 0.002976 accuracy: 1.000000\n",
            "[58: 167/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[58: 168/268] train loss: 0.000078 accuracy: 1.000000\n",
            "[58: 169/268] train loss: 0.027457 accuracy: 1.000000\n",
            "[58: 170/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[58: 170/268] test loss: 0.000588 accuracy: 1.000000\n",
            "[58: 171/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[58: 172/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[58: 173/268] train loss: 0.670215 accuracy: 0.900000\n",
            "[58: 174/268] train loss: 0.000421 accuracy: 1.000000\n",
            "[58: 175/268] train loss: 0.000556 accuracy: 1.000000\n",
            "[58: 176/268] train loss: 0.000445 accuracy: 1.000000\n",
            "[58: 177/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[58: 178/268] train loss: 0.000343 accuracy: 1.000000\n",
            "[58: 179/268] train loss: 0.004702 accuracy: 1.000000\n",
            "[58: 180/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[58: 180/268] test loss: 0.000050 accuracy: 1.000000\n",
            "[58: 181/268] train loss: 0.000224 accuracy: 1.000000\n",
            "[58: 182/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[58: 183/268] train loss: 0.000001 accuracy: 1.000000\n",
            "[58: 184/268] train loss: 0.002124 accuracy: 1.000000\n",
            "[58: 185/268] train loss: 0.000375 accuracy: 1.000000\n",
            "[58: 186/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[58: 187/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[58: 188/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[58: 189/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[58: 190/268] train loss: 0.000066 accuracy: 1.000000\n",
            "[58: 190/268] test loss: 0.001025 accuracy: 1.000000\n",
            "[58: 191/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[58: 192/268] train loss: 0.000095 accuracy: 1.000000\n",
            "[58: 193/268] train loss: 0.000114 accuracy: 1.000000\n",
            "[58: 194/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[58: 195/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[58: 196/268] train loss: 0.000093 accuracy: 1.000000\n",
            "[58: 197/268] train loss: 0.000181 accuracy: 1.000000\n",
            "[58: 198/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[58: 199/268] train loss: 0.004778 accuracy: 1.000000\n",
            "[58: 200/268] train loss: 0.000591 accuracy: 1.000000\n",
            "[58: 200/268] test loss: 0.000172 accuracy: 1.000000\n",
            "[58: 201/268] train loss: 0.001948 accuracy: 1.000000\n",
            "[58: 202/268] train loss: 0.002180 accuracy: 1.000000\n",
            "[58: 203/268] train loss: 0.000122 accuracy: 1.000000\n",
            "[58: 204/268] train loss: 0.001275 accuracy: 1.000000\n",
            "[58: 205/268] train loss: 0.000914 accuracy: 1.000000\n",
            "[58: 206/268] train loss: 0.003921 accuracy: 1.000000\n",
            "[58: 207/268] train loss: 0.001455 accuracy: 1.000000\n",
            "[58: 208/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[58: 209/268] train loss: 0.010222 accuracy: 1.000000\n",
            "[58: 210/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[58: 210/268] test loss: 0.001627 accuracy: 1.000000\n",
            "[58: 211/268] train loss: 0.001353 accuracy: 1.000000\n",
            "[58: 212/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[58: 213/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[58: 214/268] train loss: 0.000829 accuracy: 1.000000\n",
            "[58: 215/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[58: 216/268] train loss: 0.000087 accuracy: 1.000000\n",
            "[58: 217/268] train loss: 0.000654 accuracy: 1.000000\n",
            "[58: 218/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[58: 219/268] train loss: 0.008365 accuracy: 1.000000\n",
            "[58: 220/268] train loss: 0.000264 accuracy: 1.000000\n",
            "[58: 220/268] test loss: 0.000752 accuracy: 1.000000\n",
            "[58: 221/268] train loss: 0.001153 accuracy: 1.000000\n",
            "[58: 222/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[58: 223/268] train loss: 0.001236 accuracy: 1.000000\n",
            "[58: 224/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[58: 225/268] train loss: 0.000110 accuracy: 1.000000\n",
            "[58: 226/268] train loss: 0.000956 accuracy: 1.000000\n",
            "[58: 227/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[58: 228/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[58: 229/268] train loss: 0.000209 accuracy: 1.000000\n",
            "[58: 230/268] train loss: 0.000734 accuracy: 1.000000\n",
            "[58: 230/268] test loss: 0.001331 accuracy: 1.000000\n",
            "[58: 231/268] train loss: 0.000288 accuracy: 1.000000\n",
            "[58: 232/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[58: 233/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[58: 234/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[58: 235/268] train loss: 0.000307 accuracy: 1.000000\n",
            "[58: 236/268] train loss: 0.073755 accuracy: 0.900000\n",
            "[58: 237/268] train loss: 0.003252 accuracy: 1.000000\n",
            "[58: 238/268] train loss: 0.000055 accuracy: 1.000000\n",
            "[58: 239/268] train loss: 0.003761 accuracy: 1.000000\n",
            "[58: 240/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[58: 240/268] test loss: 0.000967 accuracy: 1.000000\n",
            "[58: 241/268] train loss: 0.000430 accuracy: 1.000000\n",
            "[58: 242/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[58: 243/268] train loss: 0.000525 accuracy: 1.000000\n",
            "[58: 244/268] train loss: 0.004804 accuracy: 1.000000\n",
            "[58: 245/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[58: 246/268] train loss: 0.000578 accuracy: 1.000000\n",
            "[58: 247/268] train loss: 0.000178 accuracy: 1.000000\n",
            "[58: 248/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[58: 249/268] train loss: 0.000110 accuracy: 1.000000\n",
            "[58: 250/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[58: 250/268] test loss: 0.000515 accuracy: 1.000000\n",
            "[58: 251/268] train loss: 0.000606 accuracy: 1.000000\n",
            "[58: 252/268] train loss: 0.000809 accuracy: 1.000000\n",
            "[58: 253/268] train loss: 0.039551 accuracy: 1.000000\n",
            "[58: 254/268] train loss: 0.118831 accuracy: 0.900000\n",
            "[58: 255/268] train loss: 0.003754 accuracy: 1.000000\n",
            "[58: 256/268] train loss: 0.000389 accuracy: 1.000000\n",
            "[58: 257/268] train loss: 0.000099 accuracy: 1.000000\n",
            "[58: 258/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[58: 259/268] train loss: 0.000134 accuracy: 1.000000\n",
            "[58: 260/268] train loss: 0.003380 accuracy: 1.000000\n",
            "[58: 260/268] test loss: 0.000249 accuracy: 1.000000\n",
            "[58: 261/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[58: 262/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[58: 263/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[58: 264/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[58: 265/268] train loss: 0.004347 accuracy: 1.000000\n",
            "[58: 266/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[58: 267/268] train loss: 0.050861 accuracy: 0.800000\n",
            "[59: 0/268] train loss: 0.001067 accuracy: 1.000000\n",
            "[59: 0/268] test loss: 0.000083 accuracy: 1.000000\n",
            "[59: 1/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[59: 2/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[59: 3/268] train loss: 0.000064 accuracy: 1.000000\n",
            "[59: 4/268] train loss: 0.001342 accuracy: 1.000000\n",
            "[59: 5/268] train loss: 0.000293 accuracy: 1.000000\n",
            "[59: 6/268] train loss: 0.000600 accuracy: 1.000000\n",
            "[59: 7/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[59: 8/268] train loss: 0.000401 accuracy: 1.000000\n",
            "[59: 9/268] train loss: 0.001178 accuracy: 1.000000\n",
            "[59: 10/268] train loss: 0.006511 accuracy: 1.000000\n",
            "[59: 10/268] test loss: 0.000415 accuracy: 1.000000\n",
            "[59: 11/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[59: 12/268] train loss: 0.000260 accuracy: 1.000000\n",
            "[59: 13/268] train loss: 0.000118 accuracy: 1.000000\n",
            "[59: 14/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[59: 15/268] train loss: 0.000087 accuracy: 1.000000\n",
            "[59: 16/268] train loss: 0.000264 accuracy: 1.000000\n",
            "[59: 17/268] train loss: 0.000478 accuracy: 1.000000\n",
            "[59: 18/268] train loss: 0.000428 accuracy: 1.000000\n",
            "[59: 19/268] train loss: 0.000066 accuracy: 1.000000\n",
            "[59: 20/268] train loss: 0.000182 accuracy: 1.000000\n",
            "[59: 20/268] test loss: 0.013707 accuracy: 1.000000\n",
            "[59: 21/268] train loss: 0.000203 accuracy: 1.000000\n",
            "[59: 22/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[59: 23/268] train loss: 0.000107 accuracy: 1.000000\n",
            "[59: 24/268] train loss: 0.000097 accuracy: 1.000000\n",
            "[59: 25/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[59: 26/268] train loss: 0.000196 accuracy: 1.000000\n",
            "[59: 27/268] train loss: 0.001076 accuracy: 1.000000\n",
            "[59: 28/268] train loss: 0.000588 accuracy: 1.000000\n",
            "[59: 29/268] train loss: 0.007973 accuracy: 1.000000\n",
            "[59: 30/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[59: 30/268] test loss: 0.000016 accuracy: 1.000000\n",
            "[59: 31/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[59: 32/268] train loss: 0.002676 accuracy: 1.000000\n",
            "[59: 33/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[59: 34/268] train loss: 0.000707 accuracy: 1.000000\n",
            "[59: 35/268] train loss: 0.000055 accuracy: 1.000000\n",
            "[59: 36/268] train loss: 0.000707 accuracy: 1.000000\n",
            "[59: 37/268] train loss: 0.000197 accuracy: 1.000000\n",
            "[59: 38/268] train loss: 0.000313 accuracy: 1.000000\n",
            "[59: 39/268] train loss: 0.002286 accuracy: 1.000000\n",
            "[59: 40/268] train loss: 0.001815 accuracy: 1.000000\n",
            "[59: 40/268] test loss: 0.000484 accuracy: 1.000000\n",
            "[59: 41/268] train loss: 0.000162 accuracy: 1.000000\n",
            "[59: 42/268] train loss: 0.000170 accuracy: 1.000000\n",
            "[59: 43/268] train loss: 0.000198 accuracy: 1.000000\n",
            "[59: 44/268] train loss: 0.000182 accuracy: 1.000000\n",
            "[59: 45/268] train loss: 0.001849 accuracy: 1.000000\n",
            "[59: 46/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[59: 47/268] train loss: 0.000452 accuracy: 1.000000\n",
            "[59: 48/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[59: 49/268] train loss: 0.000247 accuracy: 1.000000\n",
            "[59: 50/268] train loss: 0.001219 accuracy: 1.000000\n",
            "[59: 50/268] test loss: 0.001053 accuracy: 1.000000\n",
            "[59: 51/268] train loss: 0.000195 accuracy: 1.000000\n",
            "[59: 52/268] train loss: 0.000196 accuracy: 1.000000\n",
            "[59: 53/268] train loss: 0.002832 accuracy: 1.000000\n",
            "[59: 54/268] train loss: 0.002500 accuracy: 1.000000\n",
            "[59: 55/268] train loss: 0.026915 accuracy: 1.000000\n",
            "[59: 56/268] train loss: 0.038277 accuracy: 1.000000\n",
            "[59: 57/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[59: 58/268] train loss: 0.000133 accuracy: 1.000000\n",
            "[59: 59/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[59: 60/268] train loss: 0.000241 accuracy: 1.000000\n",
            "[59: 60/268] test loss: 0.000462 accuracy: 1.000000\n",
            "[59: 61/268] train loss: 0.000195 accuracy: 1.000000\n",
            "[59: 62/268] train loss: 0.001188 accuracy: 1.000000\n",
            "[59: 63/268] train loss: 0.000055 accuracy: 1.000000\n",
            "[59: 64/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[59: 65/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[59: 66/268] train loss: 0.000155 accuracy: 1.000000\n",
            "[59: 67/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[59: 68/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[59: 69/268] train loss: 0.000379 accuracy: 1.000000\n",
            "[59: 70/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[59: 70/268] test loss: 0.000198 accuracy: 1.000000\n",
            "[59: 71/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[59: 72/268] train loss: 0.000440 accuracy: 1.000000\n",
            "[59: 73/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[59: 74/268] train loss: 0.001808 accuracy: 1.000000\n",
            "[59: 75/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[59: 76/268] train loss: 0.011914 accuracy: 1.000000\n",
            "[59: 77/268] train loss: 0.000433 accuracy: 1.000000\n",
            "[59: 78/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[59: 79/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[59: 80/268] train loss: 0.000074 accuracy: 1.000000\n",
            "[59: 80/268] test loss: 0.000065 accuracy: 1.000000\n",
            "[59: 81/268] train loss: 0.000192 accuracy: 1.000000\n",
            "[59: 82/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[59: 83/268] train loss: 0.001936 accuracy: 1.000000\n",
            "[59: 84/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[59: 85/268] train loss: 0.000099 accuracy: 1.000000\n",
            "[59: 86/268] train loss: 0.005448 accuracy: 1.000000\n",
            "[59: 87/268] train loss: 0.000147 accuracy: 1.000000\n",
            "[59: 88/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[59: 89/268] train loss: 0.003119 accuracy: 1.000000\n",
            "[59: 90/268] train loss: 0.000343 accuracy: 1.000000\n",
            "[59: 90/268] test loss: 0.000202 accuracy: 1.000000\n",
            "[59: 91/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[59: 92/268] train loss: 0.000078 accuracy: 1.000000\n",
            "[59: 93/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[59: 94/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[59: 95/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[59: 96/268] train loss: 0.000328 accuracy: 1.000000\n",
            "[59: 97/268] train loss: 0.000448 accuracy: 1.000000\n",
            "[59: 98/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[59: 99/268] train loss: 0.006885 accuracy: 1.000000\n",
            "[59: 100/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[59: 100/268] test loss: 0.001515 accuracy: 1.000000\n",
            "[59: 101/268] train loss: 0.000492 accuracy: 1.000000\n",
            "[59: 102/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[59: 103/268] train loss: 0.000319 accuracy: 1.000000\n",
            "[59: 104/268] train loss: 0.000386 accuracy: 1.000000\n",
            "[59: 105/268] train loss: 0.001904 accuracy: 1.000000\n",
            "[59: 106/268] train loss: 0.000275 accuracy: 1.000000\n",
            "[59: 107/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[59: 108/268] train loss: 0.000076 accuracy: 1.000000\n",
            "[59: 109/268] train loss: 0.001771 accuracy: 1.000000\n",
            "[59: 110/268] train loss: 0.001527 accuracy: 1.000000\n",
            "[59: 110/268] test loss: 0.000973 accuracy: 1.000000\n",
            "[59: 111/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[59: 112/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[59: 113/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[59: 114/268] train loss: 0.000078 accuracy: 1.000000\n",
            "[59: 115/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[59: 116/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[59: 117/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[59: 118/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[59: 119/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[59: 120/268] train loss: 0.000163 accuracy: 1.000000\n",
            "[59: 120/268] test loss: 0.000057 accuracy: 1.000000\n",
            "[59: 121/268] train loss: 0.000071 accuracy: 1.000000\n",
            "[59: 122/268] train loss: 0.000150 accuracy: 1.000000\n",
            "[59: 123/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[59: 124/268] train loss: 0.001027 accuracy: 1.000000\n",
            "[59: 125/268] train loss: 0.012816 accuracy: 1.000000\n",
            "[59: 126/268] train loss: 0.003190 accuracy: 1.000000\n",
            "[59: 127/268] train loss: 0.022214 accuracy: 1.000000\n",
            "[59: 128/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[59: 129/268] train loss: 0.018079 accuracy: 1.000000\n",
            "[59: 130/268] train loss: 0.002179 accuracy: 1.000000\n",
            "[59: 130/268] test loss: 0.000181 accuracy: 1.000000\n",
            "[59: 131/268] train loss: 0.001733 accuracy: 1.000000\n",
            "[59: 132/268] train loss: 0.000452 accuracy: 1.000000\n",
            "[59: 133/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[59: 134/268] train loss: 0.002067 accuracy: 1.000000\n",
            "[59: 135/268] train loss: 0.000591 accuracy: 1.000000\n",
            "[59: 136/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[59: 137/268] train loss: 0.004491 accuracy: 1.000000\n",
            "[59: 138/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[59: 139/268] train loss: 0.000788 accuracy: 1.000000\n",
            "[59: 140/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[59: 140/268] test loss: 0.001544 accuracy: 1.000000\n",
            "[59: 141/268] train loss: 0.006005 accuracy: 1.000000\n",
            "[59: 142/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[59: 143/268] train loss: 0.000862 accuracy: 1.000000\n",
            "[59: 144/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[59: 145/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[59: 146/268] train loss: 0.000074 accuracy: 1.000000\n",
            "[59: 147/268] train loss: 0.009591 accuracy: 1.000000\n",
            "[59: 148/268] train loss: 0.002967 accuracy: 1.000000\n",
            "[59: 149/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[59: 150/268] train loss: 0.002914 accuracy: 1.000000\n",
            "[59: 150/268] test loss: 0.000268 accuracy: 1.000000\n",
            "[59: 151/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[59: 152/268] train loss: 0.000064 accuracy: 1.000000\n",
            "[59: 153/268] train loss: 0.000738 accuracy: 1.000000\n",
            "[59: 154/268] train loss: 0.000505 accuracy: 1.000000\n",
            "[59: 155/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[59: 156/268] train loss: 0.000485 accuracy: 1.000000\n",
            "[59: 157/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[59: 158/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[59: 159/268] train loss: 0.000103 accuracy: 1.000000\n",
            "[59: 160/268] train loss: 0.000113 accuracy: 1.000000\n",
            "[59: 160/268] test loss: 0.000717 accuracy: 1.000000\n",
            "[59: 161/268] train loss: 0.000117 accuracy: 1.000000\n",
            "[59: 162/268] train loss: 0.015098 accuracy: 1.000000\n",
            "[59: 163/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[59: 164/268] train loss: 0.000180 accuracy: 1.000000\n",
            "[59: 165/268] train loss: 0.000264 accuracy: 1.000000\n",
            "[59: 166/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[59: 167/268] train loss: 0.001619 accuracy: 1.000000\n",
            "[59: 168/268] train loss: 0.000277 accuracy: 1.000000\n",
            "[59: 169/268] train loss: 0.283613 accuracy: 0.900000\n",
            "[59: 170/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[59: 170/268] test loss: 0.000377 accuracy: 1.000000\n",
            "[59: 171/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[59: 172/268] train loss: 0.000979 accuracy: 1.000000\n",
            "[59: 173/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[59: 174/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[59: 175/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[59: 176/268] train loss: 0.000151 accuracy: 1.000000\n",
            "[59: 177/268] train loss: 0.004640 accuracy: 1.000000\n",
            "[59: 178/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[59: 179/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[59: 180/268] train loss: 0.005546 accuracy: 1.000000\n",
            "[59: 180/268] test loss: 0.000815 accuracy: 1.000000\n",
            "[59: 181/268] train loss: 0.000647 accuracy: 1.000000\n",
            "[59: 182/268] train loss: 0.000166 accuracy: 1.000000\n",
            "[59: 183/268] train loss: 0.000098 accuracy: 1.000000\n",
            "[59: 184/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[59: 185/268] train loss: 0.000669 accuracy: 1.000000\n",
            "[59: 186/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[59: 187/268] train loss: 0.000109 accuracy: 1.000000\n",
            "[59: 188/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[59: 189/268] train loss: 0.000281 accuracy: 1.000000\n",
            "[59: 190/268] train loss: 0.001203 accuracy: 1.000000\n",
            "[59: 190/268] test loss: 0.000981 accuracy: 1.000000\n",
            "[59: 191/268] train loss: 0.000253 accuracy: 1.000000\n",
            "[59: 192/268] train loss: 0.000125 accuracy: 1.000000\n",
            "[59: 193/268] train loss: 0.000141 accuracy: 1.000000\n",
            "[59: 194/268] train loss: 0.004240 accuracy: 1.000000\n",
            "[59: 195/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[59: 196/268] train loss: 0.000222 accuracy: 1.000000\n",
            "[59: 197/268] train loss: 0.000895 accuracy: 1.000000\n",
            "[59: 198/268] train loss: 0.000457 accuracy: 1.000000\n",
            "[59: 199/268] train loss: 0.000081 accuracy: 1.000000\n",
            "[59: 200/268] train loss: 0.000335 accuracy: 1.000000\n",
            "[59: 200/268] test loss: 0.000049 accuracy: 1.000000\n",
            "[59: 201/268] train loss: 0.000078 accuracy: 1.000000\n",
            "[59: 202/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[59: 203/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[59: 204/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[59: 205/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[59: 206/268] train loss: 0.000176 accuracy: 1.000000\n",
            "[59: 207/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[59: 208/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[59: 209/268] train loss: 0.000339 accuracy: 1.000000\n",
            "[59: 210/268] train loss: 0.001105 accuracy: 1.000000\n",
            "[59: 210/268] test loss: 0.000010 accuracy: 1.000000\n",
            "[59: 211/268] train loss: 0.010189 accuracy: 1.000000\n",
            "[59: 212/268] train loss: 0.000360 accuracy: 1.000000\n",
            "[59: 213/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[59: 214/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[59: 215/268] train loss: 0.000522 accuracy: 1.000000\n",
            "[59: 216/268] train loss: 0.038570 accuracy: 1.000000\n",
            "[59: 217/268] train loss: 0.000131 accuracy: 1.000000\n",
            "[59: 218/268] train loss: 0.011715 accuracy: 1.000000\n",
            "[59: 219/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[59: 220/268] train loss: 0.000186 accuracy: 1.000000\n",
            "[59: 220/268] test loss: 0.000530 accuracy: 1.000000\n",
            "[59: 221/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[59: 222/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[59: 223/268] train loss: 0.002328 accuracy: 1.000000\n",
            "[59: 224/268] train loss: 0.000093 accuracy: 1.000000\n",
            "[59: 225/268] train loss: 0.004999 accuracy: 1.000000\n",
            "[59: 226/268] train loss: 0.007994 accuracy: 1.000000\n",
            "[59: 227/268] train loss: 0.000691 accuracy: 1.000000\n",
            "[59: 228/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[59: 229/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[59: 230/268] train loss: 0.006756 accuracy: 1.000000\n",
            "[59: 230/268] test loss: 0.000302 accuracy: 1.000000\n",
            "[59: 231/268] train loss: 0.000289 accuracy: 1.000000\n",
            "[59: 232/268] train loss: 0.001068 accuracy: 1.000000\n",
            "[59: 233/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[59: 234/268] train loss: 0.059426 accuracy: 1.000000\n",
            "[59: 235/268] train loss: 0.000107 accuracy: 1.000000\n",
            "[59: 236/268] train loss: 0.000074 accuracy: 1.000000\n",
            "[59: 237/268] train loss: 0.010403 accuracy: 1.000000\n",
            "[59: 238/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[59: 239/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[59: 240/268] train loss: 0.000868 accuracy: 1.000000\n",
            "[59: 240/268] test loss: 0.000777 accuracy: 1.000000\n",
            "[59: 241/268] train loss: 0.000178 accuracy: 1.000000\n",
            "[59: 242/268] train loss: 0.000167 accuracy: 1.000000\n",
            "[59: 243/268] train loss: 0.001143 accuracy: 1.000000\n",
            "[59: 244/268] train loss: 0.003427 accuracy: 1.000000\n",
            "[59: 245/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[59: 246/268] train loss: 0.000295 accuracy: 1.000000\n",
            "[59: 247/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[59: 248/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[59: 249/268] train loss: 0.000646 accuracy: 1.000000\n",
            "[59: 250/268] train loss: 0.000567 accuracy: 1.000000\n",
            "[59: 250/268] test loss: 0.000059 accuracy: 1.000000\n",
            "[59: 251/268] train loss: 0.001695 accuracy: 1.000000\n",
            "[59: 252/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[59: 253/268] train loss: 0.000700 accuracy: 1.000000\n",
            "[59: 254/268] train loss: 0.000584 accuracy: 1.000000\n",
            "[59: 255/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[59: 256/268] train loss: 0.000104 accuracy: 1.000000\n",
            "[59: 257/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[59: 258/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[59: 259/268] train loss: 0.000119 accuracy: 1.000000\n",
            "[59: 260/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[59: 260/268] test loss: 0.000910 accuracy: 1.000000\n",
            "[59: 261/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[59: 262/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[59: 263/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[59: 264/268] train loss: 0.006286 accuracy: 1.000000\n",
            "[59: 265/268] train loss: 0.000233 accuracy: 1.000000\n",
            "[59: 266/268] train loss: 0.000145 accuracy: 1.000000\n",
            "[59: 267/268] train loss: 0.000083 accuracy: 0.800000\n",
            "[60: 0/268] train loss: 0.000290 accuracy: 1.000000\n",
            "[60: 0/268] test loss: 0.002416 accuracy: 1.000000\n",
            "[60: 1/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[60: 2/268] train loss: 0.001086 accuracy: 1.000000\n",
            "[60: 3/268] train loss: 0.001057 accuracy: 1.000000\n",
            "[60: 4/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[60: 5/268] train loss: 0.000369 accuracy: 1.000000\n",
            "[60: 6/268] train loss: 0.000546 accuracy: 1.000000\n",
            "[60: 7/268] train loss: 0.000302 accuracy: 1.000000\n",
            "[60: 8/268] train loss: 0.000137 accuracy: 1.000000\n",
            "[60: 9/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[60: 10/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[60: 10/268] test loss: 0.000356 accuracy: 1.000000\n",
            "[60: 11/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[60: 12/268] train loss: 0.000499 accuracy: 1.000000\n",
            "[60: 13/268] train loss: 0.000259 accuracy: 1.000000\n",
            "[60: 14/268] train loss: 0.000166 accuracy: 1.000000\n",
            "[60: 15/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[60: 16/268] train loss: 0.000539 accuracy: 1.000000\n",
            "[60: 17/268] train loss: 0.000182 accuracy: 1.000000\n",
            "[60: 18/268] train loss: 0.000792 accuracy: 1.000000\n",
            "[60: 19/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[60: 20/268] train loss: 0.000157 accuracy: 1.000000\n",
            "[60: 20/268] test loss: 0.000162 accuracy: 1.000000\n",
            "[60: 21/268] train loss: 0.001203 accuracy: 1.000000\n",
            "[60: 22/268] train loss: 0.000168 accuracy: 1.000000\n",
            "[60: 23/268] train loss: 0.002849 accuracy: 1.000000\n",
            "[60: 24/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[60: 25/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[60: 26/268] train loss: 0.000117 accuracy: 1.000000\n",
            "[60: 27/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[60: 28/268] train loss: 0.015965 accuracy: 1.000000\n",
            "[60: 29/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[60: 30/268] train loss: 0.000104 accuracy: 1.000000\n",
            "[60: 30/268] test loss: 0.000179 accuracy: 1.000000\n",
            "[60: 31/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[60: 32/268] train loss: 0.000393 accuracy: 1.000000\n",
            "[60: 33/268] train loss: 0.000305 accuracy: 1.000000\n",
            "[60: 34/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[60: 35/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[60: 36/268] train loss: 0.030858 accuracy: 1.000000\n",
            "[60: 37/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[60: 38/268] train loss: 0.000380 accuracy: 1.000000\n",
            "[60: 39/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[60: 40/268] train loss: 0.004217 accuracy: 1.000000\n",
            "[60: 40/268] test loss: 0.000227 accuracy: 1.000000\n",
            "[60: 41/268] train loss: 0.000223 accuracy: 1.000000\n",
            "[60: 42/268] train loss: 0.000349 accuracy: 1.000000\n",
            "[60: 43/268] train loss: 0.000118 accuracy: 1.000000\n",
            "[60: 44/268] train loss: 0.000122 accuracy: 1.000000\n",
            "[60: 45/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[60: 46/268] train loss: 0.002213 accuracy: 1.000000\n",
            "[60: 47/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[60: 48/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[60: 49/268] train loss: 0.000229 accuracy: 1.000000\n",
            "[60: 50/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[60: 50/268] test loss: 0.000101 accuracy: 1.000000\n",
            "[60: 51/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[60: 52/268] train loss: 0.000291 accuracy: 1.000000\n",
            "[60: 53/268] train loss: 0.006914 accuracy: 1.000000\n",
            "[60: 54/268] train loss: 0.002731 accuracy: 1.000000\n",
            "[60: 55/268] train loss: 0.000252 accuracy: 1.000000\n",
            "[60: 56/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[60: 57/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[60: 58/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[60: 59/268] train loss: 0.001362 accuracy: 1.000000\n",
            "[60: 60/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[60: 60/268] test loss: 0.002182 accuracy: 1.000000\n",
            "[60: 61/268] train loss: 0.000695 accuracy: 1.000000\n",
            "[60: 62/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[60: 63/268] train loss: 0.000343 accuracy: 1.000000\n",
            "[60: 64/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[60: 65/268] train loss: 0.000101 accuracy: 1.000000\n",
            "[60: 66/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[60: 67/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[60: 68/268] train loss: 0.000681 accuracy: 1.000000\n",
            "[60: 69/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[60: 70/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[60: 70/268] test loss: 0.000355 accuracy: 1.000000\n",
            "[60: 71/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[60: 72/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[60: 73/268] train loss: 0.041391 accuracy: 1.000000\n",
            "[60: 74/268] train loss: 0.000868 accuracy: 1.000000\n",
            "[60: 75/268] train loss: 0.001538 accuracy: 1.000000\n",
            "[60: 76/268] train loss: 0.016442 accuracy: 1.000000\n",
            "[60: 77/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[60: 78/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[60: 79/268] train loss: 0.000139 accuracy: 1.000000\n",
            "[60: 80/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[60: 80/268] test loss: 0.003779 accuracy: 1.000000\n",
            "[60: 81/268] train loss: 0.000786 accuracy: 1.000000\n",
            "[60: 82/268] train loss: 0.000774 accuracy: 1.000000\n",
            "[60: 83/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[60: 84/268] train loss: 0.000052 accuracy: 1.000000\n",
            "[60: 85/268] train loss: 0.001550 accuracy: 1.000000\n",
            "[60: 86/268] train loss: 0.000330 accuracy: 1.000000\n",
            "[60: 87/268] train loss: 0.000426 accuracy: 1.000000\n",
            "[60: 88/268] train loss: 0.003236 accuracy: 1.000000\n",
            "[60: 89/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[60: 90/268] train loss: 0.000171 accuracy: 1.000000\n",
            "[60: 90/268] test loss: 0.000050 accuracy: 1.000000\n",
            "[60: 91/268] train loss: 0.000456 accuracy: 1.000000\n",
            "[60: 92/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[60: 93/268] train loss: 0.000245 accuracy: 1.000000\n",
            "[60: 94/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[60: 95/268] train loss: 0.000114 accuracy: 1.000000\n",
            "[60: 96/268] train loss: 0.000170 accuracy: 1.000000\n",
            "[60: 97/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[60: 98/268] train loss: 0.002490 accuracy: 1.000000\n",
            "[60: 99/268] train loss: 0.060418 accuracy: 1.000000\n",
            "[60: 100/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[60: 100/268] test loss: 0.001777 accuracy: 1.000000\n",
            "[60: 101/268] train loss: 0.000231 accuracy: 1.000000\n",
            "[60: 102/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[60: 103/268] train loss: 0.000064 accuracy: 1.000000\n",
            "[60: 104/268] train loss: 0.000300 accuracy: 1.000000\n",
            "[60: 105/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[60: 106/268] train loss: 0.000001 accuracy: 1.000000\n",
            "[60: 107/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[60: 108/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[60: 109/268] train loss: 0.001129 accuracy: 1.000000\n",
            "[60: 110/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[60: 110/268] test loss: 0.000353 accuracy: 1.000000\n",
            "[60: 111/268] train loss: 0.003464 accuracy: 1.000000\n",
            "[60: 112/268] train loss: 0.000488 accuracy: 1.000000\n",
            "[60: 113/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[60: 114/268] train loss: 0.001125 accuracy: 1.000000\n",
            "[60: 115/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[60: 116/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[60: 117/268] train loss: 0.002641 accuracy: 1.000000\n",
            "[60: 118/268] train loss: 0.000290 accuracy: 1.000000\n",
            "[60: 119/268] train loss: 0.003618 accuracy: 1.000000\n",
            "[60: 120/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[60: 120/268] test loss: 0.001020 accuracy: 1.000000\n",
            "[60: 121/268] train loss: 0.015087 accuracy: 1.000000\n",
            "[60: 122/268] train loss: 0.000109 accuracy: 1.000000\n",
            "[60: 123/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[60: 124/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[60: 125/268] train loss: 0.000088 accuracy: 1.000000\n",
            "[60: 126/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[60: 127/268] train loss: 0.000590 accuracy: 1.000000\n",
            "[60: 128/268] train loss: 0.000391 accuracy: 1.000000\n",
            "[60: 129/268] train loss: 0.000099 accuracy: 1.000000\n",
            "[60: 130/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[60: 130/268] test loss: 0.000068 accuracy: 1.000000\n",
            "[60: 131/268] train loss: 0.000088 accuracy: 1.000000\n",
            "[60: 132/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[60: 133/268] train loss: 0.000079 accuracy: 1.000000\n",
            "[60: 134/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[60: 135/268] train loss: 0.000313 accuracy: 1.000000\n",
            "[60: 136/268] train loss: 0.000516 accuracy: 1.000000\n",
            "[60: 137/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[60: 138/268] train loss: 0.001494 accuracy: 1.000000\n",
            "[60: 139/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[60: 140/268] train loss: 0.000118 accuracy: 1.000000\n",
            "[60: 140/268] test loss: 0.000098 accuracy: 1.000000\n",
            "[60: 141/268] train loss: 0.000081 accuracy: 1.000000\n",
            "[60: 142/268] train loss: 0.000186 accuracy: 1.000000\n",
            "[60: 143/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[60: 144/268] train loss: 0.000359 accuracy: 1.000000\n",
            "[60: 145/268] train loss: 0.000285 accuracy: 1.000000\n",
            "[60: 146/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[60: 147/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[60: 148/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[60: 149/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[60: 150/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[60: 150/268] test loss: 0.000526 accuracy: 1.000000\n",
            "[60: 151/268] train loss: 0.000104 accuracy: 1.000000\n",
            "[60: 152/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[60: 153/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[60: 154/268] train loss: 0.000325 accuracy: 1.000000\n",
            "[60: 155/268] train loss: 0.002089 accuracy: 1.000000\n",
            "[60: 156/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[60: 157/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[60: 158/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[60: 159/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[60: 160/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[60: 160/268] test loss: 0.001518 accuracy: 1.000000\n",
            "[60: 161/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[60: 162/268] train loss: 0.000401 accuracy: 1.000000\n",
            "[60: 163/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[60: 164/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[60: 165/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[60: 166/268] train loss: 0.000452 accuracy: 1.000000\n",
            "[60: 167/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[60: 168/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[60: 169/268] train loss: 0.000322 accuracy: 1.000000\n",
            "[60: 170/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[60: 170/268] test loss: 0.000019 accuracy: 1.000000\n",
            "[60: 171/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[60: 172/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[60: 173/268] train loss: 0.001924 accuracy: 1.000000\n",
            "[60: 174/268] train loss: 0.000928 accuracy: 1.000000\n",
            "[60: 175/268] train loss: 0.000066 accuracy: 1.000000\n",
            "[60: 176/268] train loss: 0.002801 accuracy: 1.000000\n",
            "[60: 177/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[60: 178/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[60: 179/268] train loss: 0.000941 accuracy: 1.000000\n",
            "[60: 180/268] train loss: 0.004807 accuracy: 1.000000\n",
            "[60: 180/268] test loss: 0.000120 accuracy: 1.000000\n",
            "[60: 181/268] train loss: 0.001042 accuracy: 1.000000\n",
            "[60: 182/268] train loss: 0.000871 accuracy: 1.000000\n",
            "[60: 183/268] train loss: 0.000585 accuracy: 1.000000\n",
            "[60: 184/268] train loss: 0.000909 accuracy: 1.000000\n",
            "[60: 185/268] train loss: 0.000248 accuracy: 1.000000\n",
            "[60: 186/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[60: 187/268] train loss: 0.000132 accuracy: 1.000000\n",
            "[60: 188/268] train loss: 0.000465 accuracy: 1.000000\n",
            "[60: 189/268] train loss: 0.000121 accuracy: 1.000000\n",
            "[60: 190/268] train loss: 0.000288 accuracy: 1.000000\n",
            "[60: 190/268] test loss: 0.000743 accuracy: 1.000000\n",
            "[60: 191/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[60: 192/268] train loss: 0.000160 accuracy: 1.000000\n",
            "[60: 193/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[60: 194/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[60: 195/268] train loss: 0.000074 accuracy: 1.000000\n",
            "[60: 196/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[60: 197/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[60: 198/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[60: 199/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[60: 200/268] train loss: 0.057856 accuracy: 1.000000\n",
            "[60: 200/268] test loss: 0.004917 accuracy: 1.000000\n",
            "[60: 201/268] train loss: 0.000147 accuracy: 1.000000\n",
            "[60: 202/268] train loss: 0.000126 accuracy: 1.000000\n",
            "[60: 203/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[60: 204/268] train loss: 0.000102 accuracy: 1.000000\n",
            "[60: 205/268] train loss: 0.000187 accuracy: 1.000000\n",
            "[60: 206/268] train loss: 0.001067 accuracy: 1.000000\n",
            "[60: 207/268] train loss: 0.000127 accuracy: 1.000000\n",
            "[60: 208/268] train loss: 0.000177 accuracy: 1.000000\n",
            "[60: 209/268] train loss: 0.000212 accuracy: 1.000000\n",
            "[60: 210/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[60: 210/268] test loss: 0.000662 accuracy: 1.000000\n",
            "[60: 211/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[60: 212/268] train loss: 0.000280 accuracy: 1.000000\n",
            "[60: 213/268] train loss: 0.000566 accuracy: 1.000000\n",
            "[60: 214/268] train loss: 0.004455 accuracy: 1.000000\n",
            "[60: 215/268] train loss: 0.028568 accuracy: 1.000000\n",
            "[60: 216/268] train loss: 0.000240 accuracy: 1.000000\n",
            "[60: 217/268] train loss: 0.000076 accuracy: 1.000000\n",
            "[60: 218/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[60: 219/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[60: 220/268] train loss: 0.000095 accuracy: 1.000000\n",
            "[60: 220/268] test loss: 0.000717 accuracy: 1.000000\n",
            "[60: 221/268] train loss: 0.000161 accuracy: 1.000000\n",
            "[60: 222/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[60: 223/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[60: 224/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[60: 225/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[60: 226/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[60: 227/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[60: 228/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[60: 229/268] train loss: 0.031716 accuracy: 1.000000\n",
            "[60: 230/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[60: 230/268] test loss: 0.000111 accuracy: 1.000000\n",
            "[60: 231/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[60: 232/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[60: 233/268] train loss: 0.000096 accuracy: 1.000000\n",
            "[60: 234/268] train loss: 0.002345 accuracy: 1.000000\n",
            "[60: 235/268] train loss: 0.000740 accuracy: 1.000000\n",
            "[60: 236/268] train loss: 0.076515 accuracy: 0.900000\n",
            "[60: 237/268] train loss: 0.001167 accuracy: 1.000000\n",
            "[60: 238/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[60: 239/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[60: 240/268] train loss: 0.001276 accuracy: 1.000000\n",
            "[60: 240/268] test loss: 0.000311 accuracy: 1.000000\n",
            "[60: 241/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[60: 242/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[60: 243/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[60: 244/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[60: 245/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[60: 246/268] train loss: 0.000089 accuracy: 1.000000\n",
            "[60: 247/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[60: 248/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[60: 249/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[60: 250/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[60: 250/268] test loss: 0.004510 accuracy: 1.000000\n",
            "[60: 251/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[60: 252/268] train loss: 0.000112 accuracy: 1.000000\n",
            "[60: 253/268] train loss: 0.003756 accuracy: 1.000000\n",
            "[60: 254/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[60: 255/268] train loss: 0.000099 accuracy: 1.000000\n",
            "[60: 256/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[60: 257/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[60: 258/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[60: 259/268] train loss: 0.003798 accuracy: 1.000000\n",
            "[60: 260/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[60: 260/268] test loss: 0.000239 accuracy: 1.000000\n",
            "[60: 261/268] train loss: 0.000479 accuracy: 1.000000\n",
            "[60: 262/268] train loss: 0.000174 accuracy: 1.000000\n",
            "[60: 263/268] train loss: 0.000114 accuracy: 1.000000\n",
            "[60: 264/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[60: 265/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[60: 266/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[60: 267/268] train loss: 0.000086 accuracy: 0.800000\n",
            "[61: 0/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[61: 0/268] test loss: 0.003168 accuracy: 1.000000\n",
            "[61: 1/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[61: 2/268] train loss: 0.000066 accuracy: 1.000000\n",
            "[61: 3/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[61: 4/268] train loss: 0.000609 accuracy: 1.000000\n",
            "[61: 5/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[61: 6/268] train loss: 0.001202 accuracy: 1.000000\n",
            "[61: 7/268] train loss: 0.001763 accuracy: 1.000000\n",
            "[61: 8/268] train loss: 0.000154 accuracy: 1.000000\n",
            "[61: 9/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[61: 10/268] train loss: 0.000180 accuracy: 1.000000\n",
            "[61: 10/268] test loss: 0.000172 accuracy: 1.000000\n",
            "[61: 11/268] train loss: 0.000096 accuracy: 1.000000\n",
            "[61: 12/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[61: 13/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[61: 14/268] train loss: 0.000177 accuracy: 1.000000\n",
            "[61: 15/268] train loss: 0.000983 accuracy: 1.000000\n",
            "[61: 16/268] train loss: 0.000123 accuracy: 1.000000\n",
            "[61: 17/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[61: 18/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[61: 19/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[61: 20/268] train loss: 0.001603 accuracy: 1.000000\n",
            "[61: 20/268] test loss: 0.000696 accuracy: 1.000000\n",
            "[61: 21/268] train loss: 0.000283 accuracy: 1.000000\n",
            "[61: 22/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[61: 23/268] train loss: 0.000460 accuracy: 1.000000\n",
            "[61: 24/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[61: 25/268] train loss: 0.000208 accuracy: 1.000000\n",
            "[61: 26/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[61: 27/268] train loss: 0.656863 accuracy: 0.900000\n",
            "[61: 28/268] train loss: 0.000369 accuracy: 1.000000\n",
            "[61: 29/268] train loss: 0.000905 accuracy: 1.000000\n",
            "[61: 30/268] train loss: 0.000503 accuracy: 1.000000\n",
            "[61: 30/268] test loss: 0.000225 accuracy: 1.000000\n",
            "[61: 31/268] train loss: 0.000174 accuracy: 1.000000\n",
            "[61: 32/268] train loss: 0.000273 accuracy: 1.000000\n",
            "[61: 33/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[61: 34/268] train loss: 0.000631 accuracy: 1.000000\n",
            "[61: 35/268] train loss: 0.000181 accuracy: 1.000000\n",
            "[61: 36/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[61: 37/268] train loss: 0.000117 accuracy: 1.000000\n",
            "[61: 38/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[61: 39/268] train loss: 0.000957 accuracy: 1.000000\n",
            "[61: 40/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[61: 40/268] test loss: 0.000283 accuracy: 1.000000\n",
            "[61: 41/268] train loss: 0.000093 accuracy: 1.000000\n",
            "[61: 42/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[61: 43/268] train loss: 0.000303 accuracy: 1.000000\n",
            "[61: 44/268] train loss: 0.000314 accuracy: 1.000000\n",
            "[61: 45/268] train loss: 0.000250 accuracy: 1.000000\n",
            "[61: 46/268] train loss: 0.003041 accuracy: 1.000000\n",
            "[61: 47/268] train loss: 0.000375 accuracy: 1.000000\n",
            "[61: 48/268] train loss: 0.000968 accuracy: 1.000000\n",
            "[61: 49/268] train loss: 0.000112 accuracy: 1.000000\n",
            "[61: 50/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[61: 50/268] test loss: 0.000486 accuracy: 1.000000\n",
            "[61: 51/268] train loss: 0.000085 accuracy: 1.000000\n",
            "[61: 52/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[61: 53/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[61: 54/268] train loss: 0.000156 accuracy: 1.000000\n",
            "[61: 55/268] train loss: 0.000129 accuracy: 1.000000\n",
            "[61: 56/268] train loss: 0.000150 accuracy: 1.000000\n",
            "[61: 57/268] train loss: 0.000132 accuracy: 1.000000\n",
            "[61: 58/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[61: 59/268] train loss: 0.011691 accuracy: 1.000000\n",
            "[61: 60/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[61: 60/268] test loss: 0.003173 accuracy: 1.000000\n",
            "[61: 61/268] train loss: 0.000135 accuracy: 1.000000\n",
            "[61: 62/268] train loss: 0.002652 accuracy: 1.000000\n",
            "[61: 63/268] train loss: 0.001670 accuracy: 1.000000\n",
            "[61: 64/268] train loss: 0.000085 accuracy: 1.000000\n",
            "[61: 65/268] train loss: 0.000644 accuracy: 1.000000\n",
            "[61: 66/268] train loss: 0.014289 accuracy: 1.000000\n",
            "[61: 67/268] train loss: 0.121665 accuracy: 0.900000\n",
            "[61: 68/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[61: 69/268] train loss: 0.018390 accuracy: 1.000000\n",
            "[61: 70/268] train loss: 0.000183 accuracy: 1.000000\n",
            "[61: 70/268] test loss: 0.000079 accuracy: 1.000000\n",
            "[61: 71/268] train loss: 0.005691 accuracy: 1.000000\n",
            "[61: 72/268] train loss: 0.000064 accuracy: 1.000000\n",
            "[61: 73/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[61: 74/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[61: 75/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[61: 76/268] train loss: 0.000127 accuracy: 1.000000\n",
            "[61: 77/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[61: 78/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[61: 79/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[61: 80/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[61: 80/268] test loss: 0.000957 accuracy: 1.000000\n",
            "[61: 81/268] train loss: 0.000176 accuracy: 1.000000\n",
            "[61: 82/268] train loss: 0.000156 accuracy: 1.000000\n",
            "[61: 83/268] train loss: 0.003333 accuracy: 1.000000\n",
            "[61: 84/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[61: 85/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[61: 86/268] train loss: 0.001673 accuracy: 1.000000\n",
            "[61: 87/268] train loss: 0.000526 accuracy: 1.000000\n",
            "[61: 88/268] train loss: 0.000168 accuracy: 1.000000\n",
            "[61: 89/268] train loss: 0.001390 accuracy: 1.000000\n",
            "[61: 90/268] train loss: 0.001332 accuracy: 1.000000\n",
            "[61: 90/268] test loss: 0.000673 accuracy: 1.000000\n",
            "[61: 91/268] train loss: 0.000145 accuracy: 1.000000\n",
            "[61: 92/268] train loss: 0.000152 accuracy: 1.000000\n",
            "[61: 93/268] train loss: 0.000133 accuracy: 1.000000\n",
            "[61: 94/268] train loss: 0.000141 accuracy: 1.000000\n",
            "[61: 95/268] train loss: 0.001571 accuracy: 1.000000\n",
            "[61: 96/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[61: 97/268] train loss: 0.002269 accuracy: 1.000000\n",
            "[61: 98/268] train loss: 0.000847 accuracy: 1.000000\n",
            "[61: 99/268] train loss: 0.001610 accuracy: 1.000000\n",
            "[61: 100/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[61: 100/268] test loss: 0.000212 accuracy: 1.000000\n",
            "[61: 101/268] train loss: 0.000199 accuracy: 1.000000\n",
            "[61: 102/268] train loss: 0.000428 accuracy: 1.000000\n",
            "[61: 103/268] train loss: 0.000514 accuracy: 1.000000\n",
            "[61: 104/268] train loss: 0.000137 accuracy: 1.000000\n",
            "[61: 105/268] train loss: 0.000271 accuracy: 1.000000\n",
            "[61: 106/268] train loss: 0.000082 accuracy: 1.000000\n",
            "[61: 107/268] train loss: 0.002070 accuracy: 1.000000\n",
            "[61: 108/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[61: 109/268] train loss: 0.000182 accuracy: 1.000000\n",
            "[61: 110/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[61: 110/268] test loss: 0.000354 accuracy: 1.000000\n",
            "[61: 111/268] train loss: 0.006258 accuracy: 1.000000\n",
            "[61: 112/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[61: 113/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[61: 114/268] train loss: 0.092234 accuracy: 0.900000\n",
            "[61: 115/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[61: 116/268] train loss: 0.001649 accuracy: 1.000000\n",
            "[61: 117/268] train loss: 0.004952 accuracy: 1.000000\n",
            "[61: 118/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[61: 119/268] train loss: 0.000722 accuracy: 1.000000\n",
            "[61: 120/268] train loss: 0.000137 accuracy: 1.000000\n",
            "[61: 120/268] test loss: 0.000862 accuracy: 1.000000\n",
            "[61: 121/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[61: 122/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[61: 123/268] train loss: 0.000105 accuracy: 1.000000\n",
            "[61: 124/268] train loss: 0.000418 accuracy: 1.000000\n",
            "[61: 125/268] train loss: 0.000067 accuracy: 1.000000\n",
            "[61: 126/268] train loss: 0.019491 accuracy: 1.000000\n",
            "[61: 127/268] train loss: 0.002360 accuracy: 1.000000\n",
            "[61: 128/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[61: 129/268] train loss: 0.000252 accuracy: 1.000000\n",
            "[61: 130/268] train loss: 0.003716 accuracy: 1.000000\n",
            "[61: 130/268] test loss: 0.000548 accuracy: 1.000000\n",
            "[61: 131/268] train loss: 0.000532 accuracy: 1.000000\n",
            "[61: 132/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[61: 133/268] train loss: 0.001355 accuracy: 1.000000\n",
            "[61: 134/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[61: 135/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[61: 136/268] train loss: 0.000146 accuracy: 1.000000\n",
            "[61: 137/268] train loss: 0.000235 accuracy: 1.000000\n",
            "[61: 138/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[61: 139/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[61: 140/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[61: 140/268] test loss: 0.000353 accuracy: 1.000000\n",
            "[61: 141/268] train loss: 0.000257 accuracy: 1.000000\n",
            "[61: 142/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[61: 143/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[61: 144/268] train loss: 0.000071 accuracy: 1.000000\n",
            "[61: 145/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[61: 146/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[61: 147/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[61: 148/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[61: 149/268] train loss: 0.004167 accuracy: 1.000000\n",
            "[61: 150/268] train loss: 0.000184 accuracy: 1.000000\n",
            "[61: 150/268] test loss: 0.000658 accuracy: 1.000000\n",
            "[61: 151/268] train loss: 0.000805 accuracy: 1.000000\n",
            "[61: 152/268] train loss: 0.000286 accuracy: 1.000000\n",
            "[61: 153/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[61: 154/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[61: 155/268] train loss: 0.000497 accuracy: 1.000000\n",
            "[61: 156/268] train loss: 0.000123 accuracy: 1.000000\n",
            "[61: 157/268] train loss: 0.002118 accuracy: 1.000000\n",
            "[61: 158/268] train loss: 0.000420 accuracy: 1.000000\n",
            "[61: 159/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[61: 160/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[61: 160/268] test loss: 0.000644 accuracy: 1.000000\n",
            "[61: 161/268] train loss: 0.000477 accuracy: 1.000000\n",
            "[61: 162/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[61: 163/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[61: 164/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[61: 165/268] train loss: 0.000176 accuracy: 1.000000\n",
            "[61: 166/268] train loss: 0.000178 accuracy: 1.000000\n",
            "[61: 167/268] train loss: 0.010370 accuracy: 1.000000\n",
            "[61: 168/268] train loss: 0.002850 accuracy: 1.000000\n",
            "[61: 169/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[61: 170/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[61: 170/268] test loss: 0.000453 accuracy: 1.000000\n",
            "[61: 171/268] train loss: 0.000101 accuracy: 1.000000\n",
            "[61: 172/268] train loss: 0.016700 accuracy: 1.000000\n",
            "[61: 173/268] train loss: 0.002293 accuracy: 1.000000\n",
            "[61: 174/268] train loss: 0.000474 accuracy: 1.000000\n",
            "[61: 175/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[61: 176/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[61: 177/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[61: 178/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[61: 179/268] train loss: 0.000547 accuracy: 1.000000\n",
            "[61: 180/268] train loss: 0.002394 accuracy: 1.000000\n",
            "[61: 180/268] test loss: 0.002323 accuracy: 1.000000\n",
            "[61: 181/268] train loss: 0.000102 accuracy: 1.000000\n",
            "[61: 182/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[61: 183/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[61: 184/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[61: 185/268] train loss: 0.000426 accuracy: 1.000000\n",
            "[61: 186/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[61: 187/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[61: 188/268] train loss: 0.001448 accuracy: 1.000000\n",
            "[61: 189/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[61: 190/268] train loss: 0.000094 accuracy: 1.000000\n",
            "[61: 190/268] test loss: 0.000598 accuracy: 1.000000\n",
            "[61: 191/268] train loss: 0.000237 accuracy: 1.000000\n",
            "[61: 192/268] train loss: 0.000183 accuracy: 1.000000\n",
            "[61: 193/268] train loss: 0.000506 accuracy: 1.000000\n",
            "[61: 194/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[61: 195/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[61: 196/268] train loss: 0.001644 accuracy: 1.000000\n",
            "[61: 197/268] train loss: 0.003355 accuracy: 1.000000\n",
            "[61: 198/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[61: 199/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[61: 200/268] train loss: 0.002017 accuracy: 1.000000\n",
            "[61: 200/268] test loss: 0.000025 accuracy: 1.000000\n",
            "[61: 201/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[61: 202/268] train loss: 0.000093 accuracy: 1.000000\n",
            "[61: 203/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[61: 204/268] train loss: 0.048400 accuracy: 1.000000\n",
            "[61: 205/268] train loss: 0.001260 accuracy: 1.000000\n",
            "[61: 206/268] train loss: 0.363810 accuracy: 0.800000\n",
            "[61: 207/268] train loss: 0.000377 accuracy: 1.000000\n",
            "[61: 208/268] train loss: 0.000130 accuracy: 1.000000\n",
            "[61: 209/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[61: 210/268] train loss: 0.000329 accuracy: 1.000000\n",
            "[61: 210/268] test loss: 0.000086 accuracy: 1.000000\n",
            "[61: 211/268] train loss: 0.000664 accuracy: 1.000000\n",
            "[61: 212/268] train loss: 0.000986 accuracy: 1.000000\n",
            "[61: 213/268] train loss: 0.001780 accuracy: 1.000000\n",
            "[61: 214/268] train loss: 0.000710 accuracy: 1.000000\n",
            "[61: 215/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[61: 216/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[61: 217/268] train loss: 0.002676 accuracy: 1.000000\n",
            "[61: 218/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[61: 219/268] train loss: 0.000204 accuracy: 1.000000\n",
            "[61: 220/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[61: 220/268] test loss: 0.000650 accuracy: 1.000000\n",
            "[61: 221/268] train loss: 0.000685 accuracy: 1.000000\n",
            "[61: 222/268] train loss: 0.000363 accuracy: 1.000000\n",
            "[61: 223/268] train loss: 0.002129 accuracy: 1.000000\n",
            "[61: 224/268] train loss: 0.001075 accuracy: 1.000000\n",
            "[61: 225/268] train loss: 0.000273 accuracy: 1.000000\n",
            "[61: 226/268] train loss: 0.044718 accuracy: 1.000000\n",
            "[61: 227/268] train loss: 0.023483 accuracy: 1.000000\n",
            "[61: 228/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[61: 229/268] train loss: 0.000078 accuracy: 1.000000\n",
            "[61: 230/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[61: 230/268] test loss: 0.010979 accuracy: 1.000000\n",
            "[61: 231/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[61: 232/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[61: 233/268] train loss: 0.000103 accuracy: 1.000000\n",
            "[61: 234/268] train loss: 0.000155 accuracy: 1.000000\n",
            "[61: 235/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[61: 236/268] train loss: 0.000304 accuracy: 1.000000\n",
            "[61: 237/268] train loss: 0.001081 accuracy: 1.000000\n",
            "[61: 238/268] train loss: 0.000078 accuracy: 1.000000\n",
            "[61: 239/268] train loss: 0.001432 accuracy: 1.000000\n",
            "[61: 240/268] train loss: 0.000313 accuracy: 1.000000\n",
            "[61: 240/268] test loss: 0.000815 accuracy: 1.000000\n",
            "[61: 241/268] train loss: 0.000078 accuracy: 1.000000\n",
            "[61: 242/268] train loss: 0.003763 accuracy: 1.000000\n",
            "[61: 243/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[61: 244/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[61: 245/268] train loss: 0.000089 accuracy: 1.000000\n",
            "[61: 246/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[61: 247/268] train loss: 0.000244 accuracy: 1.000000\n",
            "[61: 248/268] train loss: 0.032142 accuracy: 1.000000\n",
            "[61: 249/268] train loss: 0.000191 accuracy: 1.000000\n",
            "[61: 250/268] train loss: 0.000127 accuracy: 1.000000\n",
            "[61: 250/268] test loss: 0.000565 accuracy: 1.000000\n",
            "[61: 251/268] train loss: 0.000157 accuracy: 1.000000\n",
            "[61: 252/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[61: 253/268] train loss: 0.000422 accuracy: 1.000000\n",
            "[61: 254/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[61: 255/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[61: 256/268] train loss: 0.000131 accuracy: 1.000000\n",
            "[61: 257/268] train loss: 0.000143 accuracy: 1.000000\n",
            "[61: 258/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[61: 259/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[61: 260/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[61: 260/268] test loss: 0.000094 accuracy: 1.000000\n",
            "[61: 261/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[61: 262/268] train loss: 0.009207 accuracy: 1.000000\n",
            "[61: 263/268] train loss: 0.084153 accuracy: 0.900000\n",
            "[61: 264/268] train loss: 0.000112 accuracy: 1.000000\n",
            "[61: 265/268] train loss: 0.002427 accuracy: 1.000000\n",
            "[61: 266/268] train loss: 0.131799 accuracy: 0.900000\n",
            "[61: 267/268] train loss: 0.000069 accuracy: 0.800000\n",
            "[62: 0/268] train loss: 0.001253 accuracy: 1.000000\n",
            "[62: 0/268] test loss: 0.000043 accuracy: 1.000000\n",
            "[62: 1/268] train loss: 0.000533 accuracy: 1.000000\n",
            "[62: 2/268] train loss: 0.066707 accuracy: 1.000000\n",
            "[62: 3/268] train loss: 0.000289 accuracy: 1.000000\n",
            "[62: 4/268] train loss: 0.000494 accuracy: 1.000000\n",
            "[62: 5/268] train loss: 0.000143 accuracy: 1.000000\n",
            "[62: 6/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[62: 7/268] train loss: 0.008129 accuracy: 1.000000\n",
            "[62: 8/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[62: 9/268] train loss: 0.001794 accuracy: 1.000000\n",
            "[62: 10/268] train loss: 0.005296 accuracy: 1.000000\n",
            "[62: 10/268] test loss: 0.000185 accuracy: 1.000000\n",
            "[62: 11/268] train loss: 0.023157 accuracy: 1.000000\n",
            "[62: 12/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[62: 13/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[62: 14/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[62: 15/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[62: 16/268] train loss: 0.000120 accuracy: 1.000000\n",
            "[62: 17/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[62: 18/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[62: 19/268] train loss: 0.000812 accuracy: 1.000000\n",
            "[62: 20/268] train loss: 0.001964 accuracy: 1.000000\n",
            "[62: 20/268] test loss: 0.000224 accuracy: 1.000000\n",
            "[62: 21/268] train loss: 0.000808 accuracy: 1.000000\n",
            "[62: 22/268] train loss: 0.000156 accuracy: 1.000000\n",
            "[62: 23/268] train loss: 0.001202 accuracy: 1.000000\n",
            "[62: 24/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[62: 25/268] train loss: 0.001786 accuracy: 1.000000\n",
            "[62: 26/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[62: 27/268] train loss: 0.000243 accuracy: 1.000000\n",
            "[62: 28/268] train loss: 0.002886 accuracy: 1.000000\n",
            "[62: 29/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[62: 30/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[62: 30/268] test loss: 0.001715 accuracy: 1.000000\n",
            "[62: 31/268] train loss: 0.000337 accuracy: 1.000000\n",
            "[62: 32/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[62: 33/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[62: 34/268] train loss: 0.000180 accuracy: 1.000000\n",
            "[62: 35/268] train loss: 0.001760 accuracy: 1.000000\n",
            "[62: 36/268] train loss: 0.011347 accuracy: 1.000000\n",
            "[62: 37/268] train loss: 0.005770 accuracy: 1.000000\n",
            "[62: 38/268] train loss: 0.000346 accuracy: 1.000000\n",
            "[62: 39/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[62: 40/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[62: 40/268] test loss: 0.001377 accuracy: 1.000000\n",
            "[62: 41/268] train loss: 0.000369 accuracy: 1.000000\n",
            "[62: 42/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[62: 43/268] train loss: 0.000313 accuracy: 1.000000\n",
            "[62: 44/268] train loss: 0.000266 accuracy: 1.000000\n",
            "[62: 45/268] train loss: 0.000215 accuracy: 1.000000\n",
            "[62: 46/268] train loss: 0.011616 accuracy: 1.000000\n",
            "[62: 47/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[62: 48/268] train loss: 0.002541 accuracy: 1.000000\n",
            "[62: 49/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[62: 50/268] train loss: 0.000108 accuracy: 1.000000\n",
            "[62: 50/268] test loss: 0.000721 accuracy: 1.000000\n",
            "[62: 51/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[62: 52/268] train loss: 0.000889 accuracy: 1.000000\n",
            "[62: 53/268] train loss: 0.000141 accuracy: 1.000000\n",
            "[62: 54/268] train loss: 0.000087 accuracy: 1.000000\n",
            "[62: 55/268] train loss: 0.000131 accuracy: 1.000000\n",
            "[62: 56/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[62: 57/268] train loss: 0.002234 accuracy: 1.000000\n",
            "[62: 58/268] train loss: 0.002187 accuracy: 1.000000\n",
            "[62: 59/268] train loss: 0.000293 accuracy: 1.000000\n",
            "[62: 60/268] train loss: 0.000628 accuracy: 1.000000\n",
            "[62: 60/268] test loss: 0.000683 accuracy: 1.000000\n",
            "[62: 61/268] train loss: 0.000130 accuracy: 1.000000\n",
            "[62: 62/268] train loss: 0.000126 accuracy: 1.000000\n",
            "[62: 63/268] train loss: 0.000174 accuracy: 1.000000\n",
            "[62: 64/268] train loss: 0.001925 accuracy: 1.000000\n",
            "[62: 65/268] train loss: 0.000165 accuracy: 1.000000\n",
            "[62: 66/268] train loss: 0.000082 accuracy: 1.000000\n",
            "[62: 67/268] train loss: 0.000317 accuracy: 1.000000\n",
            "[62: 68/268] train loss: 0.000093 accuracy: 1.000000\n",
            "[62: 69/268] train loss: 0.001221 accuracy: 1.000000\n",
            "[62: 70/268] train loss: 0.005278 accuracy: 1.000000\n",
            "[62: 70/268] test loss: 0.000316 accuracy: 1.000000\n",
            "[62: 71/268] train loss: 0.000077 accuracy: 1.000000\n",
            "[62: 72/268] train loss: 0.022315 accuracy: 1.000000\n",
            "[62: 73/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[62: 74/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[62: 75/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[62: 76/268] train loss: 0.009725 accuracy: 1.000000\n",
            "[62: 77/268] train loss: 0.000429 accuracy: 1.000000\n",
            "[62: 78/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[62: 79/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[62: 80/268] train loss: 0.000095 accuracy: 1.000000\n",
            "[62: 80/268] test loss: 0.002350 accuracy: 1.000000\n",
            "[62: 81/268] train loss: 0.000076 accuracy: 1.000000\n",
            "[62: 82/268] train loss: 0.000321 accuracy: 1.000000\n",
            "[62: 83/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[62: 84/268] train loss: 0.001637 accuracy: 1.000000\n",
            "[62: 85/268] train loss: 0.001099 accuracy: 1.000000\n",
            "[62: 86/268] train loss: 0.002017 accuracy: 1.000000\n",
            "[62: 87/268] train loss: 0.000112 accuracy: 1.000000\n",
            "[62: 88/268] train loss: 0.000082 accuracy: 1.000000\n",
            "[62: 89/268] train loss: 0.000093 accuracy: 1.000000\n",
            "[62: 90/268] train loss: 0.000121 accuracy: 1.000000\n",
            "[62: 90/268] test loss: 0.001226 accuracy: 1.000000\n",
            "[62: 91/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[62: 92/268] train loss: 0.000273 accuracy: 1.000000\n",
            "[62: 93/268] train loss: 0.000377 accuracy: 1.000000\n",
            "[62: 94/268] train loss: 0.000098 accuracy: 1.000000\n",
            "[62: 95/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[62: 96/268] train loss: 0.000183 accuracy: 1.000000\n",
            "[62: 97/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[62: 98/268] train loss: 0.000313 accuracy: 1.000000\n",
            "[62: 99/268] train loss: 0.019344 accuracy: 1.000000\n",
            "[62: 100/268] train loss: 0.000306 accuracy: 1.000000\n",
            "[62: 100/268] test loss: 0.000283 accuracy: 1.000000\n",
            "[62: 101/268] train loss: 0.000052 accuracy: 1.000000\n",
            "[62: 102/268] train loss: 0.000208 accuracy: 1.000000\n",
            "[62: 103/268] train loss: 0.001402 accuracy: 1.000000\n",
            "[62: 104/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[62: 105/268] train loss: 0.001090 accuracy: 1.000000\n",
            "[62: 106/268] train loss: 0.000103 accuracy: 1.000000\n",
            "[62: 107/268] train loss: 0.001843 accuracy: 1.000000\n",
            "[62: 108/268] train loss: 0.003230 accuracy: 1.000000\n",
            "[62: 109/268] train loss: 0.013801 accuracy: 1.000000\n",
            "[62: 110/268] train loss: 0.003735 accuracy: 1.000000\n",
            "[62: 110/268] test loss: 0.031478 accuracy: 1.000000\n",
            "[62: 111/268] train loss: 0.008892 accuracy: 1.000000\n",
            "[62: 112/268] train loss: 0.000139 accuracy: 1.000000\n",
            "[62: 113/268] train loss: 0.000997 accuracy: 1.000000\n",
            "[62: 114/268] train loss: 0.000104 accuracy: 1.000000\n",
            "[62: 115/268] train loss: 0.000241 accuracy: 1.000000\n",
            "[62: 116/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[62: 117/268] train loss: 0.000130 accuracy: 1.000000\n",
            "[62: 118/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[62: 119/268] train loss: 0.000089 accuracy: 1.000000\n",
            "[62: 120/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[62: 120/268] test loss: 0.000168 accuracy: 1.000000\n",
            "[62: 121/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[62: 122/268] train loss: 0.000194 accuracy: 1.000000\n",
            "[62: 123/268] train loss: 0.069902 accuracy: 0.900000\n",
            "[62: 124/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[62: 125/268] train loss: 0.000691 accuracy: 1.000000\n",
            "[62: 126/268] train loss: 0.000183 accuracy: 1.000000\n",
            "[62: 127/268] train loss: 0.000307 accuracy: 1.000000\n",
            "[62: 128/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[62: 129/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[62: 130/268] train loss: 0.000081 accuracy: 1.000000\n",
            "[62: 130/268] test loss: 0.001645 accuracy: 1.000000\n",
            "[62: 131/268] train loss: 0.000096 accuracy: 1.000000\n",
            "[62: 132/268] train loss: 0.000213 accuracy: 1.000000\n",
            "[62: 133/268] train loss: 0.000167 accuracy: 1.000000\n",
            "[62: 134/268] train loss: 0.000092 accuracy: 1.000000\n",
            "[62: 135/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[62: 136/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[62: 137/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[62: 138/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[62: 139/268] train loss: 0.001171 accuracy: 1.000000\n",
            "[62: 140/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[62: 140/268] test loss: 0.000100 accuracy: 1.000000\n",
            "[62: 141/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[62: 142/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[62: 143/268] train loss: 0.000188 accuracy: 1.000000\n",
            "[62: 144/268] train loss: 0.000368 accuracy: 1.000000\n",
            "[62: 145/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[62: 146/268] train loss: 0.000633 accuracy: 1.000000\n",
            "[62: 147/268] train loss: 0.000119 accuracy: 1.000000\n",
            "[62: 148/268] train loss: 0.000278 accuracy: 1.000000\n",
            "[62: 149/268] train loss: 0.000097 accuracy: 1.000000\n",
            "[62: 150/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[62: 150/268] test loss: 0.000683 accuracy: 1.000000\n",
            "[62: 151/268] train loss: 0.000134 accuracy: 1.000000\n",
            "[62: 152/268] train loss: 0.000287 accuracy: 1.000000\n",
            "[62: 153/268] train loss: 0.000158 accuracy: 1.000000\n",
            "[62: 154/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[62: 155/268] train loss: 0.000346 accuracy: 1.000000\n",
            "[62: 156/268] train loss: 0.000103 accuracy: 1.000000\n",
            "[62: 157/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[62: 158/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[62: 159/268] train loss: 0.000067 accuracy: 1.000000\n",
            "[62: 160/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[62: 160/268] test loss: 0.010422 accuracy: 1.000000\n",
            "[62: 161/268] train loss: 0.000168 accuracy: 1.000000\n",
            "[62: 162/268] train loss: 0.003642 accuracy: 1.000000\n",
            "[62: 163/268] train loss: 0.001247 accuracy: 1.000000\n",
            "[62: 164/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[62: 165/268] train loss: 0.000871 accuracy: 1.000000\n",
            "[62: 166/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[62: 167/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[62: 168/268] train loss: 0.000293 accuracy: 1.000000\n",
            "[62: 169/268] train loss: 0.000087 accuracy: 1.000000\n",
            "[62: 170/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[62: 170/268] test loss: 0.000589 accuracy: 1.000000\n",
            "[62: 171/268] train loss: 0.000353 accuracy: 1.000000\n",
            "[62: 172/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[62: 173/268] train loss: 0.000162 accuracy: 1.000000\n",
            "[62: 174/268] train loss: 0.000122 accuracy: 1.000000\n",
            "[62: 175/268] train loss: 0.000114 accuracy: 1.000000\n",
            "[62: 176/268] train loss: 0.000312 accuracy: 1.000000\n",
            "[62: 177/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[62: 178/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[62: 179/268] train loss: 0.000508 accuracy: 1.000000\n",
            "[62: 180/268] train loss: 0.000200 accuracy: 1.000000\n",
            "[62: 180/268] test loss: 0.000400 accuracy: 1.000000\n",
            "[62: 181/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[62: 182/268] train loss: 0.001270 accuracy: 1.000000\n",
            "[62: 183/268] train loss: 0.002253 accuracy: 1.000000\n",
            "[62: 184/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[62: 185/268] train loss: 0.008161 accuracy: 1.000000\n",
            "[62: 186/268] train loss: 0.000285 accuracy: 1.000000\n",
            "[62: 187/268] train loss: 0.000122 accuracy: 1.000000\n",
            "[62: 188/268] train loss: 0.000432 accuracy: 1.000000\n",
            "[62: 189/268] train loss: 0.000272 accuracy: 1.000000\n",
            "[62: 190/268] train loss: 0.000210 accuracy: 1.000000\n",
            "[62: 190/268] test loss: 0.001028 accuracy: 1.000000\n",
            "[62: 191/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[62: 192/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[62: 193/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[62: 194/268] train loss: 0.000264 accuracy: 1.000000\n",
            "[62: 195/268] train loss: 0.000643 accuracy: 1.000000\n",
            "[62: 196/268] train loss: 0.001909 accuracy: 1.000000\n",
            "[62: 197/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[62: 198/268] train loss: 0.000085 accuracy: 1.000000\n",
            "[62: 199/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[62: 200/268] train loss: 0.000827 accuracy: 1.000000\n",
            "[62: 200/268] test loss: 0.000307 accuracy: 1.000000\n",
            "[62: 201/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[62: 202/268] train loss: 0.000243 accuracy: 1.000000\n",
            "[62: 203/268] train loss: 0.000141 accuracy: 1.000000\n",
            "[62: 204/268] train loss: 0.000303 accuracy: 1.000000\n",
            "[62: 205/268] train loss: 0.000214 accuracy: 1.000000\n",
            "[62: 206/268] train loss: 0.000107 accuracy: 1.000000\n",
            "[62: 207/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[62: 208/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[62: 209/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[62: 210/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[62: 210/268] test loss: 0.000196 accuracy: 1.000000\n",
            "[62: 211/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[62: 212/268] train loss: 0.000125 accuracy: 1.000000\n",
            "[62: 213/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[62: 214/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[62: 215/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[62: 216/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[62: 217/268] train loss: 0.000283 accuracy: 1.000000\n",
            "[62: 218/268] train loss: 0.000330 accuracy: 1.000000\n",
            "[62: 219/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[62: 220/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[62: 220/268] test loss: 0.001799 accuracy: 1.000000\n",
            "[62: 221/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[62: 222/268] train loss: 0.002415 accuracy: 1.000000\n",
            "[62: 223/268] train loss: 0.000289 accuracy: 1.000000\n",
            "[62: 224/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[62: 225/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[62: 226/268] train loss: 0.001021 accuracy: 1.000000\n",
            "[62: 227/268] train loss: 0.001586 accuracy: 1.000000\n",
            "[62: 228/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[62: 229/268] train loss: 0.006675 accuracy: 1.000000\n",
            "[62: 230/268] train loss: 0.000350 accuracy: 1.000000\n",
            "[62: 230/268] test loss: 0.001321 accuracy: 1.000000\n",
            "[62: 231/268] train loss: 0.000074 accuracy: 1.000000\n",
            "[62: 232/268] train loss: 0.000162 accuracy: 1.000000\n",
            "[62: 233/268] train loss: 0.000121 accuracy: 1.000000\n",
            "[62: 234/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[62: 235/268] train loss: 0.000401 accuracy: 1.000000\n",
            "[62: 236/268] train loss: 0.000645 accuracy: 1.000000\n",
            "[62: 237/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[62: 238/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[62: 239/268] train loss: 0.000106 accuracy: 1.000000\n",
            "[62: 240/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[62: 240/268] test loss: 0.000079 accuracy: 1.000000\n",
            "[62: 241/268] train loss: 0.024328 accuracy: 1.000000\n",
            "[62: 242/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[62: 243/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[62: 244/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[62: 245/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[62: 246/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[62: 247/268] train loss: 0.000243 accuracy: 1.000000\n",
            "[62: 248/268] train loss: 0.000496 accuracy: 1.000000\n",
            "[62: 249/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[62: 250/268] train loss: 0.000132 accuracy: 1.000000\n",
            "[62: 250/268] test loss: 0.000114 accuracy: 1.000000\n",
            "[62: 251/268] train loss: 0.000295 accuracy: 1.000000\n",
            "[62: 252/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[62: 253/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[62: 254/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[62: 255/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[62: 256/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[62: 257/268] train loss: 0.000104 accuracy: 1.000000\n",
            "[62: 258/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[62: 259/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[62: 260/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[62: 260/268] test loss: 0.002095 accuracy: 1.000000\n",
            "[62: 261/268] train loss: 0.001272 accuracy: 1.000000\n",
            "[62: 262/268] train loss: 0.000239 accuracy: 1.000000\n",
            "[62: 263/268] train loss: 0.000082 accuracy: 1.000000\n",
            "[62: 264/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[62: 265/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[62: 266/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[62: 267/268] train loss: 0.582935 accuracy: 0.700000\n",
            "[63: 0/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[63: 0/268] test loss: 0.000144 accuracy: 1.000000\n",
            "[63: 1/268] train loss: 0.000692 accuracy: 1.000000\n",
            "[63: 2/268] train loss: 0.000377 accuracy: 1.000000\n",
            "[63: 3/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[63: 4/268] train loss: 0.000198 accuracy: 1.000000\n",
            "[63: 5/268] train loss: 0.000120 accuracy: 1.000000\n",
            "[63: 6/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[63: 7/268] train loss: 0.000405 accuracy: 1.000000\n",
            "[63: 8/268] train loss: 0.000294 accuracy: 1.000000\n",
            "[63: 9/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[63: 10/268] train loss: 0.000108 accuracy: 1.000000\n",
            "[63: 10/268] test loss: 0.006682 accuracy: 1.000000\n",
            "[63: 11/268] train loss: 0.000979 accuracy: 1.000000\n",
            "[63: 12/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[63: 13/268] train loss: 0.001130 accuracy: 1.000000\n",
            "[63: 14/268] train loss: 0.000179 accuracy: 1.000000\n",
            "[63: 15/268] train loss: 0.001346 accuracy: 1.000000\n",
            "[63: 16/268] train loss: 0.005773 accuracy: 1.000000\n",
            "[63: 17/268] train loss: 0.000109 accuracy: 1.000000\n",
            "[63: 18/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[63: 19/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[63: 20/268] train loss: 0.000082 accuracy: 1.000000\n",
            "[63: 20/268] test loss: 0.001177 accuracy: 1.000000\n",
            "[63: 21/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[63: 22/268] train loss: 0.002506 accuracy: 1.000000\n",
            "[63: 23/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[63: 24/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[63: 25/268] train loss: 0.013197 accuracy: 1.000000\n",
            "[63: 26/268] train loss: 0.000115 accuracy: 1.000000\n",
            "[63: 27/268] train loss: 0.000182 accuracy: 1.000000\n",
            "[63: 28/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[63: 29/268] train loss: 0.001351 accuracy: 1.000000\n",
            "[63: 30/268] train loss: 0.000165 accuracy: 1.000000\n",
            "[63: 30/268] test loss: 0.000729 accuracy: 1.000000\n",
            "[63: 31/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[63: 32/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[63: 33/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[63: 34/268] train loss: 0.001247 accuracy: 1.000000\n",
            "[63: 35/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[63: 36/268] train loss: 0.000092 accuracy: 1.000000\n",
            "[63: 37/268] train loss: 0.000710 accuracy: 1.000000\n",
            "[63: 38/268] train loss: 0.000646 accuracy: 1.000000\n",
            "[63: 39/268] train loss: 0.000705 accuracy: 1.000000\n",
            "[63: 40/268] train loss: 0.000067 accuracy: 1.000000\n",
            "[63: 40/268] test loss: 0.011866 accuracy: 1.000000\n",
            "[63: 41/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[63: 42/268] train loss: 0.000227 accuracy: 1.000000\n",
            "[63: 43/268] train loss: 0.000128 accuracy: 1.000000\n",
            "[63: 44/268] train loss: 0.000247 accuracy: 1.000000\n",
            "[63: 45/268] train loss: 0.023446 accuracy: 1.000000\n",
            "[63: 46/268] train loss: 0.001701 accuracy: 1.000000\n",
            "[63: 47/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[63: 48/268] train loss: 0.002043 accuracy: 1.000000\n",
            "[63: 49/268] train loss: 0.000405 accuracy: 1.000000\n",
            "[63: 50/268] train loss: 0.000223 accuracy: 1.000000\n",
            "[63: 50/268] test loss: 0.001012 accuracy: 1.000000\n",
            "[63: 51/268] train loss: 0.000160 accuracy: 1.000000\n",
            "[63: 52/268] train loss: 0.000877 accuracy: 1.000000\n",
            "[63: 53/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[63: 54/268] train loss: 0.000271 accuracy: 1.000000\n",
            "[63: 55/268] train loss: 0.007046 accuracy: 1.000000\n",
            "[63: 56/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[63: 57/268] train loss: 0.000479 accuracy: 1.000000\n",
            "[63: 58/268] train loss: 0.000107 accuracy: 1.000000\n",
            "[63: 59/268] train loss: 0.000842 accuracy: 1.000000\n",
            "[63: 60/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[63: 60/268] test loss: 0.001370 accuracy: 1.000000\n",
            "[63: 61/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[63: 62/268] train loss: 0.000158 accuracy: 1.000000\n",
            "[63: 63/268] train loss: 0.000314 accuracy: 1.000000\n",
            "[63: 64/268] train loss: 0.000382 accuracy: 1.000000\n",
            "[63: 65/268] train loss: 0.001990 accuracy: 1.000000\n",
            "[63: 66/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[63: 67/268] train loss: 0.003484 accuracy: 1.000000\n",
            "[63: 68/268] train loss: 0.000243 accuracy: 1.000000\n",
            "[63: 69/268] train loss: 0.001371 accuracy: 1.000000\n",
            "[63: 70/268] train loss: 0.021886 accuracy: 1.000000\n",
            "[63: 70/268] test loss: 0.000035 accuracy: 1.000000\n",
            "[63: 71/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[63: 72/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[63: 73/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[63: 74/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[63: 75/268] train loss: 0.000081 accuracy: 1.000000\n",
            "[63: 76/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[63: 77/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[63: 78/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[63: 79/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[63: 80/268] train loss: 0.000137 accuracy: 1.000000\n",
            "[63: 80/268] test loss: 0.000118 accuracy: 1.000000\n",
            "[63: 81/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[63: 82/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[63: 83/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[63: 84/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[63: 85/268] train loss: 0.000079 accuracy: 1.000000\n",
            "[63: 86/268] train loss: 0.003767 accuracy: 1.000000\n",
            "[63: 87/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[63: 88/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[63: 89/268] train loss: 0.000553 accuracy: 1.000000\n",
            "[63: 90/268] train loss: 0.000264 accuracy: 1.000000\n",
            "[63: 90/268] test loss: 0.000129 accuracy: 1.000000\n",
            "[63: 91/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[63: 92/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[63: 93/268] train loss: 0.000328 accuracy: 1.000000\n",
            "[63: 94/268] train loss: 0.003254 accuracy: 1.000000\n",
            "[63: 95/268] train loss: 0.000536 accuracy: 1.000000\n",
            "[63: 96/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[63: 97/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[63: 98/268] train loss: 0.000301 accuracy: 1.000000\n",
            "[63: 99/268] train loss: 0.000382 accuracy: 1.000000\n",
            "[63: 100/268] train loss: 0.003021 accuracy: 1.000000\n",
            "[63: 100/268] test loss: 0.000454 accuracy: 1.000000\n",
            "[63: 101/268] train loss: 0.003129 accuracy: 1.000000\n",
            "[63: 102/268] train loss: 0.000127 accuracy: 1.000000\n",
            "[63: 103/268] train loss: 0.000055 accuracy: 1.000000\n",
            "[63: 104/268] train loss: 0.000052 accuracy: 1.000000\n",
            "[63: 105/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[63: 106/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[63: 107/268] train loss: 0.000902 accuracy: 1.000000\n",
            "[63: 108/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[63: 109/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[63: 110/268] train loss: 0.000197 accuracy: 1.000000\n",
            "[63: 110/268] test loss: 0.000303 accuracy: 1.000000\n",
            "[63: 111/268] train loss: 0.000354 accuracy: 1.000000\n",
            "[63: 112/268] train loss: 0.000321 accuracy: 1.000000\n",
            "[63: 113/268] train loss: 0.000822 accuracy: 1.000000\n",
            "[63: 114/268] train loss: 0.000316 accuracy: 1.000000\n",
            "[63: 115/268] train loss: 0.000397 accuracy: 1.000000\n",
            "[63: 116/268] train loss: 0.000187 accuracy: 1.000000\n",
            "[63: 117/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[63: 118/268] train loss: 0.001412 accuracy: 1.000000\n",
            "[63: 119/268] train loss: 0.022089 accuracy: 1.000000\n",
            "[63: 120/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[63: 120/268] test loss: 0.000037 accuracy: 1.000000\n",
            "[63: 121/268] train loss: 0.000275 accuracy: 1.000000\n",
            "[63: 122/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[63: 123/268] train loss: 0.000474 accuracy: 1.000000\n",
            "[63: 124/268] train loss: 0.002428 accuracy: 1.000000\n",
            "[63: 125/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[63: 126/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[63: 127/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[63: 128/268] train loss: 0.000627 accuracy: 1.000000\n",
            "[63: 129/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[63: 130/268] train loss: 0.000136 accuracy: 1.000000\n",
            "[63: 130/268] test loss: 0.000153 accuracy: 1.000000\n",
            "[63: 131/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[63: 132/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[63: 133/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[63: 134/268] train loss: 0.000271 accuracy: 1.000000\n",
            "[63: 135/268] train loss: 0.000192 accuracy: 1.000000\n",
            "[63: 136/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[63: 137/268] train loss: 0.004009 accuracy: 1.000000\n",
            "[63: 138/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[63: 139/268] train loss: 0.001733 accuracy: 1.000000\n",
            "[63: 140/268] train loss: 0.003217 accuracy: 1.000000\n",
            "[63: 140/268] test loss: 0.000489 accuracy: 1.000000\n",
            "[63: 141/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[63: 142/268] train loss: 0.000111 accuracy: 1.000000\n",
            "[63: 143/268] train loss: 0.002474 accuracy: 1.000000\n",
            "[63: 144/268] train loss: 0.000162 accuracy: 1.000000\n",
            "[63: 145/268] train loss: 0.022248 accuracy: 1.000000\n",
            "[63: 146/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[63: 147/268] train loss: 0.000111 accuracy: 1.000000\n",
            "[63: 148/268] train loss: 0.000085 accuracy: 1.000000\n",
            "[63: 149/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[63: 150/268] train loss: 0.000077 accuracy: 1.000000\n",
            "[63: 150/268] test loss: 0.001139 accuracy: 1.000000\n",
            "[63: 151/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[63: 152/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[63: 153/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[63: 154/268] train loss: 0.002790 accuracy: 1.000000\n",
            "[63: 155/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[63: 156/268] train loss: 0.002919 accuracy: 1.000000\n",
            "[63: 157/268] train loss: 0.000211 accuracy: 1.000000\n",
            "[63: 158/268] train loss: 0.064759 accuracy: 1.000000\n",
            "[63: 159/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[63: 160/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[63: 160/268] test loss: 0.000072 accuracy: 1.000000\n",
            "[63: 161/268] train loss: 0.000077 accuracy: 1.000000\n",
            "[63: 162/268] train loss: 0.000774 accuracy: 1.000000\n",
            "[63: 163/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[63: 164/268] train loss: 0.000762 accuracy: 1.000000\n",
            "[63: 165/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[63: 166/268] train loss: 0.001627 accuracy: 1.000000\n",
            "[63: 167/268] train loss: 0.000676 accuracy: 1.000000\n",
            "[63: 168/268] train loss: 0.000067 accuracy: 1.000000\n",
            "[63: 169/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[63: 170/268] train loss: 0.000162 accuracy: 1.000000\n",
            "[63: 170/268] test loss: 0.000286 accuracy: 1.000000\n",
            "[63: 171/268] train loss: 0.000227 accuracy: 1.000000\n",
            "[63: 172/268] train loss: 0.000374 accuracy: 1.000000\n",
            "[63: 173/268] train loss: 0.000500 accuracy: 1.000000\n",
            "[63: 174/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[63: 175/268] train loss: 0.000140 accuracy: 1.000000\n",
            "[63: 176/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[63: 177/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[63: 178/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[63: 179/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[63: 180/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[63: 180/268] test loss: 0.002012 accuracy: 1.000000\n",
            "[63: 181/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[63: 182/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[63: 183/268] train loss: 0.000127 accuracy: 1.000000\n",
            "[63: 184/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[63: 185/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[63: 186/268] train loss: 0.000778 accuracy: 1.000000\n",
            "[63: 187/268] train loss: 0.000133 accuracy: 1.000000\n",
            "[63: 188/268] train loss: 0.004714 accuracy: 1.000000\n",
            "[63: 189/268] train loss: 0.000879 accuracy: 1.000000\n",
            "[63: 190/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[63: 190/268] test loss: 0.000029 accuracy: 1.000000\n",
            "[63: 191/268] train loss: 0.000581 accuracy: 1.000000\n",
            "[63: 192/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[63: 193/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[63: 194/268] train loss: 0.001356 accuracy: 1.000000\n",
            "[63: 195/268] train loss: 0.006776 accuracy: 1.000000\n",
            "[63: 196/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[63: 197/268] train loss: 0.000217 accuracy: 1.000000\n",
            "[63: 198/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[63: 199/268] train loss: 0.000388 accuracy: 1.000000\n",
            "[63: 200/268] train loss: 0.000090 accuracy: 1.000000\n",
            "[63: 200/268] test loss: 0.000071 accuracy: 1.000000\n",
            "[63: 201/268] train loss: 0.000098 accuracy: 1.000000\n",
            "[63: 202/268] train loss: 0.005119 accuracy: 1.000000\n",
            "[63: 203/268] train loss: 0.000992 accuracy: 1.000000\n",
            "[63: 204/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[63: 205/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[63: 206/268] train loss: 0.000738 accuracy: 1.000000\n",
            "[63: 207/268] train loss: 0.000402 accuracy: 1.000000\n",
            "[63: 208/268] train loss: 0.000192 accuracy: 1.000000\n",
            "[63: 209/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[63: 210/268] train loss: 0.000144 accuracy: 1.000000\n",
            "[63: 210/268] test loss: 0.000460 accuracy: 1.000000\n",
            "[63: 211/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[63: 212/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[63: 213/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[63: 214/268] train loss: 0.087444 accuracy: 0.900000\n",
            "[63: 215/268] train loss: 0.002533 accuracy: 1.000000\n",
            "[63: 216/268] train loss: 0.036141 accuracy: 1.000000\n",
            "[63: 217/268] train loss: 0.000057 accuracy: 1.000000\n",
            "[63: 218/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[63: 219/268] train loss: 0.000103 accuracy: 1.000000\n",
            "[63: 220/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[63: 220/268] test loss: 0.000580 accuracy: 1.000000\n",
            "[63: 221/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[63: 222/268] train loss: 0.000103 accuracy: 1.000000\n",
            "[63: 223/268] train loss: 0.000322 accuracy: 1.000000\n",
            "[63: 224/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[63: 225/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[63: 226/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[63: 227/268] train loss: 0.000071 accuracy: 1.000000\n",
            "[63: 228/268] train loss: 0.000573 accuracy: 1.000000\n",
            "[63: 229/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[63: 230/268] train loss: 0.002396 accuracy: 1.000000\n",
            "[63: 230/268] test loss: 0.007620 accuracy: 1.000000\n",
            "[63: 231/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[63: 232/268] train loss: 0.000287 accuracy: 1.000000\n",
            "[63: 233/268] train loss: 0.000151 accuracy: 1.000000\n",
            "[63: 234/268] train loss: 0.000546 accuracy: 1.000000\n",
            "[63: 235/268] train loss: 0.000400 accuracy: 1.000000\n",
            "[63: 236/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[63: 237/268] train loss: 0.012606 accuracy: 1.000000\n",
            "[63: 238/268] train loss: 0.396567 accuracy: 0.900000\n",
            "[63: 239/268] train loss: 0.007817 accuracy: 1.000000\n",
            "[63: 240/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[63: 240/268] test loss: 0.000258 accuracy: 1.000000\n",
            "[63: 241/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[63: 242/268] train loss: 0.000430 accuracy: 1.000000\n",
            "[63: 243/268] train loss: 0.082350 accuracy: 1.000000\n",
            "[63: 244/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[63: 245/268] train loss: 0.000390 accuracy: 1.000000\n",
            "[63: 246/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[63: 247/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[63: 248/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[63: 249/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[63: 250/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[63: 250/268] test loss: 0.000608 accuracy: 1.000000\n",
            "[63: 251/268] train loss: 0.000052 accuracy: 1.000000\n",
            "[63: 252/268] train loss: 0.000088 accuracy: 1.000000\n",
            "[63: 253/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[63: 254/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[63: 255/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[63: 256/268] train loss: 0.007662 accuracy: 1.000000\n",
            "[63: 257/268] train loss: 0.000567 accuracy: 1.000000\n",
            "[63: 258/268] train loss: 0.000192 accuracy: 1.000000\n",
            "[63: 259/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[63: 260/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[63: 260/268] test loss: 0.000982 accuracy: 1.000000\n",
            "[63: 261/268] train loss: 0.048010 accuracy: 1.000000\n",
            "[63: 262/268] train loss: 0.005128 accuracy: 1.000000\n",
            "[63: 263/268] train loss: 0.000120 accuracy: 1.000000\n",
            "[63: 264/268] train loss: 0.000677 accuracy: 1.000000\n",
            "[63: 265/268] train loss: 0.000159 accuracy: 1.000000\n",
            "[63: 266/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[63: 267/268] train loss: 0.000049 accuracy: 0.800000\n",
            "[64: 0/268] train loss: 0.311978 accuracy: 0.900000\n",
            "[64: 0/268] test loss: 0.001433 accuracy: 1.000000\n",
            "[64: 1/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[64: 2/268] train loss: 0.001783 accuracy: 1.000000\n",
            "[64: 3/268] train loss: 0.007107 accuracy: 1.000000\n",
            "[64: 4/268] train loss: 0.000096 accuracy: 1.000000\n",
            "[64: 5/268] train loss: 0.000125 accuracy: 1.000000\n",
            "[64: 6/268] train loss: 0.000282 accuracy: 1.000000\n",
            "[64: 7/268] train loss: 0.000257 accuracy: 1.000000\n",
            "[64: 8/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[64: 9/268] train loss: 0.000144 accuracy: 1.000000\n",
            "[64: 10/268] train loss: 0.000356 accuracy: 1.000000\n",
            "[64: 10/268] test loss: 0.000521 accuracy: 1.000000\n",
            "[64: 11/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[64: 12/268] train loss: 0.001182 accuracy: 1.000000\n",
            "[64: 13/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[64: 14/268] train loss: 0.000369 accuracy: 1.000000\n",
            "[64: 15/268] train loss: 0.010990 accuracy: 1.000000\n",
            "[64: 16/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[64: 17/268] train loss: 0.001522 accuracy: 1.000000\n",
            "[64: 18/268] train loss: 0.000808 accuracy: 1.000000\n",
            "[64: 19/268] train loss: 0.040353 accuracy: 1.000000\n",
            "[64: 20/268] train loss: 0.000689 accuracy: 1.000000\n",
            "[64: 20/268] test loss: 0.000051 accuracy: 1.000000\n",
            "[64: 21/268] train loss: 0.000432 accuracy: 1.000000\n",
            "[64: 22/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[64: 23/268] train loss: 0.000224 accuracy: 1.000000\n",
            "[64: 24/268] train loss: 0.000136 accuracy: 1.000000\n",
            "[64: 25/268] train loss: 0.001574 accuracy: 1.000000\n",
            "[64: 26/268] train loss: 0.000492 accuracy: 1.000000\n",
            "[64: 27/268] train loss: 0.019501 accuracy: 1.000000\n",
            "[64: 28/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[64: 29/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[64: 30/268] train loss: 0.000522 accuracy: 1.000000\n",
            "[64: 30/268] test loss: 0.000510 accuracy: 1.000000\n",
            "[64: 31/268] train loss: 0.000493 accuracy: 1.000000\n",
            "[64: 32/268] train loss: 0.004146 accuracy: 1.000000\n",
            "[64: 33/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[64: 34/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[64: 35/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[64: 36/268] train loss: 0.003182 accuracy: 1.000000\n",
            "[64: 37/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[64: 38/268] train loss: 0.000123 accuracy: 1.000000\n",
            "[64: 39/268] train loss: 0.000157 accuracy: 1.000000\n",
            "[64: 40/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[64: 40/268] test loss: 0.000296 accuracy: 1.000000\n",
            "[64: 41/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[64: 42/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[64: 43/268] train loss: 0.000674 accuracy: 1.000000\n",
            "[64: 44/268] train loss: 0.000305 accuracy: 1.000000\n",
            "[64: 45/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[64: 46/268] train loss: 0.000340 accuracy: 1.000000\n",
            "[64: 47/268] train loss: 0.000095 accuracy: 1.000000\n",
            "[64: 48/268] train loss: 0.000147 accuracy: 1.000000\n",
            "[64: 49/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[64: 50/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[64: 50/268] test loss: 0.002899 accuracy: 1.000000\n",
            "[64: 51/268] train loss: 0.000102 accuracy: 1.000000\n",
            "[64: 52/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[64: 53/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[64: 54/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[64: 55/268] train loss: 0.000078 accuracy: 1.000000\n",
            "[64: 56/268] train loss: 0.000266 accuracy: 1.000000\n",
            "[64: 57/268] train loss: 0.000309 accuracy: 1.000000\n",
            "[64: 58/268] train loss: 0.000111 accuracy: 1.000000\n",
            "[64: 59/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[64: 60/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[64: 60/268] test loss: 0.001080 accuracy: 1.000000\n",
            "[64: 61/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[64: 62/268] train loss: 0.000489 accuracy: 1.000000\n",
            "[64: 63/268] train loss: 0.000486 accuracy: 1.000000\n",
            "[64: 64/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[64: 65/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[64: 66/268] train loss: 0.000107 accuracy: 1.000000\n",
            "[64: 67/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[64: 68/268] train loss: 0.000304 accuracy: 1.000000\n",
            "[64: 69/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[64: 70/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[64: 70/268] test loss: 0.001108 accuracy: 1.000000\n",
            "[64: 71/268] train loss: 0.000195 accuracy: 1.000000\n",
            "[64: 72/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[64: 73/268] train loss: 0.000077 accuracy: 1.000000\n",
            "[64: 74/268] train loss: 0.014858 accuracy: 1.000000\n",
            "[64: 75/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[64: 76/268] train loss: 0.000323 accuracy: 1.000000\n",
            "[64: 77/268] train loss: 0.000109 accuracy: 1.000000\n",
            "[64: 78/268] train loss: 0.000076 accuracy: 1.000000\n",
            "[64: 79/268] train loss: 0.019082 accuracy: 1.000000\n",
            "[64: 80/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[64: 80/268] test loss: 0.000160 accuracy: 1.000000\n",
            "[64: 81/268] train loss: 0.000802 accuracy: 1.000000\n",
            "[64: 82/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[64: 83/268] train loss: 0.000076 accuracy: 1.000000\n",
            "[64: 84/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[64: 85/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[64: 86/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[64: 87/268] train loss: 0.001143 accuracy: 1.000000\n",
            "[64: 88/268] train loss: 0.000368 accuracy: 1.000000\n",
            "[64: 89/268] train loss: 0.000356 accuracy: 1.000000\n",
            "[64: 90/268] train loss: 0.000202 accuracy: 1.000000\n",
            "[64: 90/268] test loss: 0.000325 accuracy: 1.000000\n",
            "[64: 91/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[64: 92/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[64: 93/268] train loss: 0.060087 accuracy: 1.000000\n",
            "[64: 94/268] train loss: 0.000149 accuracy: 1.000000\n",
            "[64: 95/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[64: 96/268] train loss: 0.001895 accuracy: 1.000000\n",
            "[64: 97/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[64: 98/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[64: 99/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[64: 100/268] train loss: 0.000103 accuracy: 1.000000\n",
            "[64: 100/268] test loss: 0.000738 accuracy: 1.000000\n",
            "[64: 101/268] train loss: 0.000245 accuracy: 1.000000\n",
            "[64: 102/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[64: 103/268] train loss: 0.000290 accuracy: 1.000000\n",
            "[64: 104/268] train loss: 0.000090 accuracy: 1.000000\n",
            "[64: 105/268] train loss: 0.001629 accuracy: 1.000000\n",
            "[64: 106/268] train loss: 0.001923 accuracy: 1.000000\n",
            "[64: 107/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[64: 108/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[64: 109/268] train loss: 0.000074 accuracy: 1.000000\n",
            "[64: 110/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[64: 110/268] test loss: 0.002292 accuracy: 1.000000\n",
            "[64: 111/268] train loss: 0.000153 accuracy: 1.000000\n",
            "[64: 112/268] train loss: 0.000095 accuracy: 1.000000\n",
            "[64: 113/268] train loss: 0.000141 accuracy: 1.000000\n",
            "[64: 114/268] train loss: 0.000137 accuracy: 1.000000\n",
            "[64: 115/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[64: 116/268] train loss: 0.000466 accuracy: 1.000000\n",
            "[64: 117/268] train loss: 0.000249 accuracy: 1.000000\n",
            "[64: 118/268] train loss: 0.000159 accuracy: 1.000000\n",
            "[64: 119/268] train loss: 0.002932 accuracy: 1.000000\n",
            "[64: 120/268] train loss: 0.014867 accuracy: 1.000000\n",
            "[64: 120/268] test loss: 0.000036 accuracy: 1.000000\n",
            "[64: 121/268] train loss: 0.000396 accuracy: 1.000000\n",
            "[64: 122/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[64: 123/268] train loss: 0.000340 accuracy: 1.000000\n",
            "[64: 124/268] train loss: 0.000881 accuracy: 1.000000\n",
            "[64: 125/268] train loss: 0.001318 accuracy: 1.000000\n",
            "[64: 126/268] train loss: 0.000910 accuracy: 1.000000\n",
            "[64: 127/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[64: 128/268] train loss: 0.000715 accuracy: 1.000000\n",
            "[64: 129/268] train loss: 0.006426 accuracy: 1.000000\n",
            "[64: 130/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[64: 130/268] test loss: 0.008189 accuracy: 1.000000\n",
            "[64: 131/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[64: 132/268] train loss: 0.000052 accuracy: 1.000000\n",
            "[64: 133/268] train loss: 0.000092 accuracy: 1.000000\n",
            "[64: 134/268] train loss: 0.000119 accuracy: 1.000000\n",
            "[64: 135/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[64: 136/268] train loss: 0.000218 accuracy: 1.000000\n",
            "[64: 137/268] train loss: 0.001353 accuracy: 1.000000\n",
            "[64: 138/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[64: 139/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[64: 140/268] train loss: 0.000348 accuracy: 1.000000\n",
            "[64: 140/268] test loss: 0.001882 accuracy: 1.000000\n",
            "[64: 141/268] train loss: 0.000626 accuracy: 1.000000\n",
            "[64: 142/268] train loss: 0.000859 accuracy: 1.000000\n",
            "[64: 143/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[64: 144/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[64: 145/268] train loss: 0.000161 accuracy: 1.000000\n",
            "[64: 146/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[64: 147/268] train loss: 0.000842 accuracy: 1.000000\n",
            "[64: 148/268] train loss: 0.000071 accuracy: 1.000000\n",
            "[64: 149/268] train loss: 0.000306 accuracy: 1.000000\n",
            "[64: 150/268] train loss: 0.003072 accuracy: 1.000000\n",
            "[64: 150/268] test loss: 0.003579 accuracy: 1.000000\n",
            "[64: 151/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[64: 152/268] train loss: 0.000650 accuracy: 1.000000\n",
            "[64: 153/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[64: 154/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[64: 155/268] train loss: 0.001987 accuracy: 1.000000\n",
            "[64: 156/268] train loss: 0.000077 accuracy: 1.000000\n",
            "[64: 157/268] train loss: 0.000980 accuracy: 1.000000\n",
            "[64: 158/268] train loss: 0.000364 accuracy: 1.000000\n",
            "[64: 159/268] train loss: 0.000267 accuracy: 1.000000\n",
            "[64: 160/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[64: 160/268] test loss: 0.000292 accuracy: 1.000000\n",
            "[64: 161/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[64: 162/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[64: 163/268] train loss: 0.000290 accuracy: 1.000000\n",
            "[64: 164/268] train loss: 0.000164 accuracy: 1.000000\n",
            "[64: 165/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[64: 166/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[64: 167/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[64: 168/268] train loss: 0.000605 accuracy: 1.000000\n",
            "[64: 169/268] train loss: 0.000370 accuracy: 1.000000\n",
            "[64: 170/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[64: 170/268] test loss: 0.000113 accuracy: 1.000000\n",
            "[64: 171/268] train loss: 0.000277 accuracy: 1.000000\n",
            "[64: 172/268] train loss: 0.000513 accuracy: 1.000000\n",
            "[64: 173/268] train loss: 0.000078 accuracy: 1.000000\n",
            "[64: 174/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[64: 175/268] train loss: 0.000962 accuracy: 1.000000\n",
            "[64: 176/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[64: 177/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[64: 178/268] train loss: 0.000346 accuracy: 1.000000\n",
            "[64: 179/268] train loss: 0.000074 accuracy: 1.000000\n",
            "[64: 180/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[64: 180/268] test loss: 0.000725 accuracy: 1.000000\n",
            "[64: 181/268] train loss: 0.001528 accuracy: 1.000000\n",
            "[64: 182/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[64: 183/268] train loss: 0.000403 accuracy: 1.000000\n",
            "[64: 184/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[64: 185/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[64: 186/268] train loss: 0.000296 accuracy: 1.000000\n",
            "[64: 187/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[64: 188/268] train loss: 0.000225 accuracy: 1.000000\n",
            "[64: 189/268] train loss: 0.000141 accuracy: 1.000000\n",
            "[64: 190/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[64: 190/268] test loss: 0.001774 accuracy: 1.000000\n",
            "[64: 191/268] train loss: 0.000664 accuracy: 1.000000\n",
            "[64: 192/268] train loss: 0.002395 accuracy: 1.000000\n",
            "[64: 193/268] train loss: 0.000738 accuracy: 1.000000\n",
            "[64: 194/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[64: 195/268] train loss: 0.000153 accuracy: 1.000000\n",
            "[64: 196/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[64: 197/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[64: 198/268] train loss: 0.000770 accuracy: 1.000000\n",
            "[64: 199/268] train loss: 0.000227 accuracy: 1.000000\n",
            "[64: 200/268] train loss: 0.001285 accuracy: 1.000000\n",
            "[64: 200/268] test loss: 0.000109 accuracy: 1.000000\n",
            "[64: 201/268] train loss: 0.003262 accuracy: 1.000000\n",
            "[64: 202/268] train loss: 0.000199 accuracy: 1.000000\n",
            "[64: 203/268] train loss: 0.001036 accuracy: 1.000000\n",
            "[64: 204/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[64: 205/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[64: 206/268] train loss: 0.000798 accuracy: 1.000000\n",
            "[64: 207/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[64: 208/268] train loss: 0.000346 accuracy: 1.000000\n",
            "[64: 209/268] train loss: 0.002543 accuracy: 1.000000\n",
            "[64: 210/268] train loss: 0.000430 accuracy: 1.000000\n",
            "[64: 210/268] test loss: 0.001323 accuracy: 1.000000\n",
            "[64: 211/268] train loss: 0.000064 accuracy: 1.000000\n",
            "[64: 212/268] train loss: 0.002827 accuracy: 1.000000\n",
            "[64: 213/268] train loss: 0.000179 accuracy: 1.000000\n",
            "[64: 214/268] train loss: 0.000207 accuracy: 1.000000\n",
            "[64: 215/268] train loss: 0.012211 accuracy: 1.000000\n",
            "[64: 216/268] train loss: 0.000076 accuracy: 1.000000\n",
            "[64: 217/268] train loss: 0.000324 accuracy: 1.000000\n",
            "[64: 218/268] train loss: 0.000796 accuracy: 1.000000\n",
            "[64: 219/268] train loss: 0.317354 accuracy: 0.900000\n",
            "[64: 220/268] train loss: 0.000104 accuracy: 1.000000\n",
            "[64: 220/268] test loss: 0.000046 accuracy: 1.000000\n",
            "[64: 221/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[64: 222/268] train loss: 0.000097 accuracy: 1.000000\n",
            "[64: 223/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[64: 224/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[64: 225/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[64: 226/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[64: 227/268] train loss: 0.000128 accuracy: 1.000000\n",
            "[64: 228/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[64: 229/268] train loss: 0.000325 accuracy: 1.000000\n",
            "[64: 230/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[64: 230/268] test loss: 0.000033 accuracy: 1.000000\n",
            "[64: 231/268] train loss: 0.009865 accuracy: 1.000000\n",
            "[64: 232/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[64: 233/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[64: 234/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[64: 235/268] train loss: 0.003180 accuracy: 1.000000\n",
            "[64: 236/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[64: 237/268] train loss: 0.000265 accuracy: 1.000000\n",
            "[64: 238/268] train loss: 0.000145 accuracy: 1.000000\n",
            "[64: 239/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[64: 240/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[64: 240/268] test loss: 0.000104 accuracy: 1.000000\n",
            "[64: 241/268] train loss: 0.000137 accuracy: 1.000000\n",
            "[64: 242/268] train loss: 0.000164 accuracy: 1.000000\n",
            "[64: 243/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[64: 244/268] train loss: 0.003682 accuracy: 1.000000\n",
            "[64: 245/268] train loss: 0.000367 accuracy: 1.000000\n",
            "[64: 246/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[64: 247/268] train loss: 0.000429 accuracy: 1.000000\n",
            "[64: 248/268] train loss: 0.000055 accuracy: 1.000000\n",
            "[64: 249/268] train loss: 0.000800 accuracy: 1.000000\n",
            "[64: 250/268] train loss: 0.000194 accuracy: 1.000000\n",
            "[64: 250/268] test loss: 0.000134 accuracy: 1.000000\n",
            "[64: 251/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[64: 252/268] train loss: 0.000361 accuracy: 1.000000\n",
            "[64: 253/268] train loss: 0.001718 accuracy: 1.000000\n",
            "[64: 254/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[64: 255/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[64: 256/268] train loss: 0.000376 accuracy: 1.000000\n",
            "[64: 257/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[64: 258/268] train loss: 0.000931 accuracy: 1.000000\n",
            "[64: 259/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[64: 260/268] train loss: 0.000074 accuracy: 1.000000\n",
            "[64: 260/268] test loss: 0.000309 accuracy: 1.000000\n",
            "[64: 261/268] train loss: 0.000296 accuracy: 1.000000\n",
            "[64: 262/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[64: 263/268] train loss: 0.295614 accuracy: 0.900000\n",
            "[64: 264/268] train loss: 0.000144 accuracy: 1.000000\n",
            "[64: 265/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[64: 266/268] train loss: 0.000140 accuracy: 1.000000\n",
            "[64: 267/268] train loss: 0.000200 accuracy: 0.800000\n",
            "[65: 0/268] train loss: 0.000101 accuracy: 1.000000\n",
            "[65: 0/268] test loss: 0.000707 accuracy: 1.000000\n",
            "[65: 1/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[65: 2/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[65: 3/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[65: 4/268] train loss: 0.003423 accuracy: 1.000000\n",
            "[65: 5/268] train loss: 0.000641 accuracy: 1.000000\n",
            "[65: 6/268] train loss: 0.000458 accuracy: 1.000000\n",
            "[65: 7/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[65: 8/268] train loss: 0.000848 accuracy: 1.000000\n",
            "[65: 9/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[65: 10/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[65: 10/268] test loss: 0.024179 accuracy: 1.000000\n",
            "[65: 11/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[65: 12/268] train loss: 0.000109 accuracy: 1.000000\n",
            "[65: 13/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[65: 14/268] train loss: 0.000064 accuracy: 1.000000\n",
            "[65: 15/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[65: 16/268] train loss: 0.003613 accuracy: 1.000000\n",
            "[65: 17/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[65: 18/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[65: 19/268] train loss: 0.116617 accuracy: 0.900000\n",
            "[65: 20/268] train loss: 0.000258 accuracy: 1.000000\n",
            "[65: 20/268] test loss: 0.014258 accuracy: 1.000000\n",
            "[65: 21/268] train loss: 0.000136 accuracy: 1.000000\n",
            "[65: 22/268] train loss: 0.000066 accuracy: 1.000000\n",
            "[65: 23/268] train loss: 0.000928 accuracy: 1.000000\n",
            "[65: 24/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[65: 25/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[65: 26/268] train loss: 0.000579 accuracy: 1.000000\n",
            "[65: 27/268] train loss: 0.004780 accuracy: 1.000000\n",
            "[65: 28/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[65: 29/268] train loss: 0.000107 accuracy: 1.000000\n",
            "[65: 30/268] train loss: 0.000036 accuracy: 1.000000\n",
            "[65: 30/268] test loss: 0.000550 accuracy: 1.000000\n",
            "[65: 31/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[65: 32/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[65: 33/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[65: 34/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[65: 35/268] train loss: 0.000097 accuracy: 1.000000\n",
            "[65: 36/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[65: 37/268] train loss: 0.005545 accuracy: 1.000000\n",
            "[65: 38/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[65: 39/268] train loss: 0.000125 accuracy: 1.000000\n",
            "[65: 40/268] train loss: 0.000518 accuracy: 1.000000\n",
            "[65: 40/268] test loss: 0.001863 accuracy: 1.000000\n",
            "[65: 41/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[65: 42/268] train loss: 0.000508 accuracy: 1.000000\n",
            "[65: 43/268] train loss: 0.023543 accuracy: 1.000000\n",
            "[65: 44/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[65: 45/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[65: 46/268] train loss: 0.000301 accuracy: 1.000000\n",
            "[65: 47/268] train loss: 0.000300 accuracy: 1.000000\n",
            "[65: 48/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[65: 49/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[65: 50/268] train loss: 0.000057 accuracy: 1.000000\n",
            "[65: 50/268] test loss: 0.000058 accuracy: 1.000000\n",
            "[65: 51/268] train loss: 0.000071 accuracy: 1.000000\n",
            "[65: 52/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[65: 53/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[65: 54/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[65: 55/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[65: 56/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[65: 57/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[65: 58/268] train loss: 0.027997 accuracy: 1.000000\n",
            "[65: 59/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[65: 60/268] train loss: 0.000248 accuracy: 1.000000\n",
            "[65: 60/268] test loss: 0.000149 accuracy: 1.000000\n",
            "[65: 61/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[65: 62/268] train loss: 0.000521 accuracy: 1.000000\n",
            "[65: 63/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[65: 64/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[65: 65/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[65: 66/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[65: 67/268] train loss: 0.000148 accuracy: 1.000000\n",
            "[65: 68/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[65: 69/268] train loss: 0.000093 accuracy: 1.000000\n",
            "[65: 70/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[65: 70/268] test loss: 0.001189 accuracy: 1.000000\n",
            "[65: 71/268] train loss: 0.001097 accuracy: 1.000000\n",
            "[65: 72/268] train loss: 0.000109 accuracy: 1.000000\n",
            "[65: 73/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[65: 74/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[65: 75/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[65: 76/268] train loss: 0.000438 accuracy: 1.000000\n",
            "[65: 77/268] train loss: 0.001656 accuracy: 1.000000\n",
            "[65: 78/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[65: 79/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[65: 80/268] train loss: 0.002115 accuracy: 1.000000\n",
            "[65: 80/268] test loss: 0.000305 accuracy: 1.000000\n",
            "[65: 81/268] train loss: 0.000157 accuracy: 1.000000\n",
            "[65: 82/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[65: 83/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[65: 84/268] train loss: 0.000755 accuracy: 1.000000\n",
            "[65: 85/268] train loss: 0.000080 accuracy: 1.000000\n",
            "[65: 86/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[65: 87/268] train loss: 0.000118 accuracy: 1.000000\n",
            "[65: 88/268] train loss: 0.000733 accuracy: 1.000000\n",
            "[65: 89/268] train loss: 0.000907 accuracy: 1.000000\n",
            "[65: 90/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[65: 90/268] test loss: 0.000220 accuracy: 1.000000\n",
            "[65: 91/268] train loss: 0.000104 accuracy: 1.000000\n",
            "[65: 92/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[65: 93/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[65: 94/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[65: 95/268] train loss: 0.000125 accuracy: 1.000000\n",
            "[65: 96/268] train loss: 0.000121 accuracy: 1.000000\n",
            "[65: 97/268] train loss: 0.000145 accuracy: 1.000000\n",
            "[65: 98/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[65: 99/268] train loss: 0.000177 accuracy: 1.000000\n",
            "[65: 100/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[65: 100/268] test loss: 0.000164 accuracy: 1.000000\n",
            "[65: 101/268] train loss: 0.000077 accuracy: 1.000000\n",
            "[65: 102/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[65: 103/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[65: 104/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[65: 105/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[65: 106/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[65: 107/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[65: 108/268] train loss: 0.000233 accuracy: 1.000000\n",
            "[65: 109/268] train loss: 0.001049 accuracy: 1.000000\n",
            "[65: 110/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[65: 110/268] test loss: 0.000381 accuracy: 1.000000\n",
            "[65: 111/268] train loss: 0.027305 accuracy: 1.000000\n",
            "[65: 112/268] train loss: 0.000823 accuracy: 1.000000\n",
            "[65: 113/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[65: 114/268] train loss: 0.000796 accuracy: 1.000000\n",
            "[65: 115/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[65: 116/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[65: 117/268] train loss: 0.000081 accuracy: 1.000000\n",
            "[65: 118/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[65: 119/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[65: 120/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[65: 120/268] test loss: 0.000644 accuracy: 1.000000\n",
            "[65: 121/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[65: 122/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[65: 123/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[65: 124/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[65: 125/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[65: 126/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[65: 127/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[65: 128/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[65: 129/268] train loss: 0.000105 accuracy: 1.000000\n",
            "[65: 130/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[65: 130/268] test loss: 0.000119 accuracy: 1.000000\n",
            "[65: 131/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[65: 132/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[65: 133/268] train loss: 0.000090 accuracy: 1.000000\n",
            "[65: 134/268] train loss: 0.000157 accuracy: 1.000000\n",
            "[65: 135/268] train loss: 0.000095 accuracy: 1.000000\n",
            "[65: 136/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[65: 137/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[65: 138/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[65: 139/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[65: 140/268] train loss: 0.003337 accuracy: 1.000000\n",
            "[65: 140/268] test loss: 0.000131 accuracy: 1.000000\n",
            "[65: 141/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[65: 142/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[65: 143/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[65: 144/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[65: 145/268] train loss: 0.078108 accuracy: 0.900000\n",
            "[65: 146/268] train loss: 0.000707 accuracy: 1.000000\n",
            "[65: 147/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[65: 148/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[65: 149/268] train loss: 0.000090 accuracy: 1.000000\n",
            "[65: 150/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[65: 150/268] test loss: 0.000030 accuracy: 1.000000\n",
            "[65: 151/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[65: 152/268] train loss: 0.007758 accuracy: 1.000000\n",
            "[65: 153/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[65: 154/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[65: 155/268] train loss: 0.000095 accuracy: 1.000000\n",
            "[65: 156/268] train loss: 0.000179 accuracy: 1.000000\n",
            "[65: 157/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[65: 158/268] train loss: 0.000518 accuracy: 1.000000\n",
            "[65: 159/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[65: 160/268] train loss: 0.000329 accuracy: 1.000000\n",
            "[65: 160/268] test loss: 0.000054 accuracy: 1.000000\n",
            "[65: 161/268] train loss: 0.000712 accuracy: 1.000000\n",
            "[65: 162/268] train loss: 0.000204 accuracy: 1.000000\n",
            "[65: 163/268] train loss: 0.000110 accuracy: 1.000000\n",
            "[65: 164/268] train loss: 0.000119 accuracy: 1.000000\n",
            "[65: 165/268] train loss: 0.000290 accuracy: 1.000000\n",
            "[65: 166/268] train loss: 0.000310 accuracy: 1.000000\n",
            "[65: 167/268] train loss: 0.000112 accuracy: 1.000000\n",
            "[65: 168/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[65: 169/268] train loss: 0.000082 accuracy: 1.000000\n",
            "[65: 170/268] train loss: 0.000787 accuracy: 1.000000\n",
            "[65: 170/268] test loss: 0.000047 accuracy: 1.000000\n",
            "[65: 171/268] train loss: 0.000075 accuracy: 1.000000\n",
            "[65: 172/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[65: 173/268] train loss: 0.003616 accuracy: 1.000000\n",
            "[65: 174/268] train loss: 0.000096 accuracy: 1.000000\n",
            "[65: 175/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[65: 176/268] train loss: 0.028320 accuracy: 1.000000\n",
            "[65: 177/268] train loss: 0.000087 accuracy: 1.000000\n",
            "[65: 178/268] train loss: 0.000071 accuracy: 1.000000\n",
            "[65: 179/268] train loss: 0.000184 accuracy: 1.000000\n",
            "[65: 180/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[65: 180/268] test loss: 0.000555 accuracy: 1.000000\n",
            "[65: 181/268] train loss: 0.000099 accuracy: 1.000000\n",
            "[65: 182/268] train loss: 0.000110 accuracy: 1.000000\n",
            "[65: 183/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[65: 184/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[65: 185/268] train loss: 0.000451 accuracy: 1.000000\n",
            "[65: 186/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[65: 187/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[65: 188/268] train loss: 0.000071 accuracy: 1.000000\n",
            "[65: 189/268] train loss: 0.000191 accuracy: 1.000000\n",
            "[65: 190/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[65: 190/268] test loss: 0.000023 accuracy: 1.000000\n",
            "[65: 191/268] train loss: 0.006029 accuracy: 1.000000\n",
            "[65: 192/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[65: 193/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[65: 194/268] train loss: 0.002324 accuracy: 1.000000\n",
            "[65: 195/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[65: 196/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[65: 197/268] train loss: 0.005243 accuracy: 1.000000\n",
            "[65: 198/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[65: 199/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[65: 200/268] train loss: 0.000292 accuracy: 1.000000\n",
            "[65: 200/268] test loss: 0.000380 accuracy: 1.000000\n",
            "[65: 201/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[65: 202/268] train loss: 0.000004 accuracy: 1.000000\n",
            "[65: 203/268] train loss: 0.000145 accuracy: 1.000000\n",
            "[65: 204/268] train loss: 0.000633 accuracy: 1.000000\n",
            "[65: 205/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[65: 206/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[65: 207/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[65: 208/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[65: 209/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[65: 210/268] train loss: 0.004091 accuracy: 1.000000\n",
            "[65: 210/268] test loss: 0.000259 accuracy: 1.000000\n",
            "[65: 211/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[65: 212/268] train loss: 0.000518 accuracy: 1.000000\n",
            "[65: 213/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[65: 214/268] train loss: 0.000241 accuracy: 1.000000\n",
            "[65: 215/268] train loss: 0.000273 accuracy: 1.000000\n",
            "[65: 216/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[65: 217/268] train loss: 0.000371 accuracy: 1.000000\n",
            "[65: 218/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[65: 219/268] train loss: 0.000049 accuracy: 1.000000\n",
            "[65: 220/268] train loss: 0.029855 accuracy: 1.000000\n",
            "[65: 220/268] test loss: 0.002570 accuracy: 1.000000\n",
            "[65: 221/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[65: 222/268] train loss: 0.000078 accuracy: 1.000000\n",
            "[65: 223/268] train loss: 0.000102 accuracy: 1.000000\n",
            "[65: 224/268] train loss: 0.000166 accuracy: 1.000000\n",
            "[65: 225/268] train loss: 0.001839 accuracy: 1.000000\n",
            "[65: 226/268] train loss: 0.000001 accuracy: 1.000000\n",
            "[65: 227/268] train loss: 0.006146 accuracy: 1.000000\n",
            "[65: 228/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[65: 229/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[65: 230/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[65: 230/268] test loss: 0.000049 accuracy: 1.000000\n",
            "[65: 231/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[65: 232/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[65: 233/268] train loss: 0.000762 accuracy: 1.000000\n",
            "[65: 234/268] train loss: 0.000081 accuracy: 1.000000\n",
            "[65: 235/268] train loss: 0.134231 accuracy: 0.900000\n",
            "[65: 236/268] train loss: 0.000167 accuracy: 1.000000\n",
            "[65: 237/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[65: 238/268] train loss: 0.000159 accuracy: 1.000000\n",
            "[65: 239/268] train loss: 0.000539 accuracy: 1.000000\n",
            "[65: 240/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[65: 240/268] test loss: 0.000160 accuracy: 1.000000\n",
            "[65: 241/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[65: 242/268] train loss: 0.000909 accuracy: 1.000000\n",
            "[65: 243/268] train loss: 0.000079 accuracy: 1.000000\n",
            "[65: 244/268] train loss: 0.000638 accuracy: 1.000000\n",
            "[65: 245/268] train loss: 0.004090 accuracy: 1.000000\n",
            "[65: 246/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[65: 247/268] train loss: 0.000050 accuracy: 1.000000\n",
            "[65: 248/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[65: 249/268] train loss: 0.011918 accuracy: 1.000000\n",
            "[65: 250/268] train loss: 0.001218 accuracy: 1.000000\n",
            "[65: 250/268] test loss: 0.000828 accuracy: 1.000000\n",
            "[65: 251/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[65: 252/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[65: 253/268] train loss: 0.007048 accuracy: 1.000000\n",
            "[65: 254/268] train loss: 0.000442 accuracy: 1.000000\n",
            "[65: 255/268] train loss: 0.000589 accuracy: 1.000000\n",
            "[65: 256/268] train loss: 0.000108 accuracy: 1.000000\n",
            "[65: 257/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[65: 258/268] train loss: 0.002045 accuracy: 1.000000\n",
            "[65: 259/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[65: 260/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[65: 260/268] test loss: 0.000145 accuracy: 1.000000\n",
            "[65: 261/268] train loss: 0.000126 accuracy: 1.000000\n",
            "[65: 262/268] train loss: 0.000407 accuracy: 1.000000\n",
            "[65: 263/268] train loss: 0.000157 accuracy: 1.000000\n",
            "[65: 264/268] train loss: 0.000092 accuracy: 1.000000\n",
            "[65: 265/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[65: 266/268] train loss: 0.005764 accuracy: 1.000000\n",
            "[65: 267/268] train loss: 0.000015 accuracy: 0.800000\n",
            "[66: 0/268] train loss: 0.000102 accuracy: 1.000000\n",
            "[66: 0/268] test loss: 0.000304 accuracy: 1.000000\n",
            "[66: 1/268] train loss: 0.001094 accuracy: 1.000000\n",
            "[66: 2/268] train loss: 0.000556 accuracy: 1.000000\n",
            "[66: 3/268] train loss: 0.000220 accuracy: 1.000000\n",
            "[66: 4/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[66: 5/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[66: 6/268] train loss: 0.000581 accuracy: 1.000000\n",
            "[66: 7/268] train loss: 0.000124 accuracy: 1.000000\n",
            "[66: 8/268] train loss: 0.021287 accuracy: 1.000000\n",
            "[66: 9/268] train loss: 0.000831 accuracy: 1.000000\n",
            "[66: 10/268] train loss: 0.000168 accuracy: 1.000000\n",
            "[66: 10/268] test loss: 0.000080 accuracy: 1.000000\n",
            "[66: 11/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[66: 12/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[66: 13/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[66: 14/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[66: 15/268] train loss: 0.000064 accuracy: 1.000000\n",
            "[66: 16/268] train loss: 0.000688 accuracy: 1.000000\n",
            "[66: 17/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[66: 18/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[66: 19/268] train loss: 0.000609 accuracy: 1.000000\n",
            "[66: 20/268] train loss: 0.000705 accuracy: 1.000000\n",
            "[66: 20/268] test loss: 0.001581 accuracy: 1.000000\n",
            "[66: 21/268] train loss: 0.000605 accuracy: 1.000000\n",
            "[66: 22/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[66: 23/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[66: 24/268] train loss: 0.000548 accuracy: 1.000000\n",
            "[66: 25/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[66: 26/268] train loss: 0.000219 accuracy: 1.000000\n",
            "[66: 27/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[66: 28/268] train loss: 0.000088 accuracy: 1.000000\n",
            "[66: 29/268] train loss: 0.000198 accuracy: 1.000000\n",
            "[66: 30/268] train loss: 0.000910 accuracy: 1.000000\n",
            "[66: 30/268] test loss: 0.000272 accuracy: 1.000000\n",
            "[66: 31/268] train loss: 0.002000 accuracy: 1.000000\n",
            "[66: 32/268] train loss: 0.000845 accuracy: 1.000000\n",
            "[66: 33/268] train loss: 0.000359 accuracy: 1.000000\n",
            "[66: 34/268] train loss: 0.000738 accuracy: 1.000000\n",
            "[66: 35/268] train loss: 0.000089 accuracy: 1.000000\n",
            "[66: 36/268] train loss: 0.009589 accuracy: 1.000000\n",
            "[66: 37/268] train loss: 0.000101 accuracy: 1.000000\n",
            "[66: 38/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[66: 39/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[66: 40/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[66: 40/268] test loss: 0.000562 accuracy: 1.000000\n",
            "[66: 41/268] train loss: 0.000001 accuracy: 1.000000\n",
            "[66: 42/268] train loss: 0.001191 accuracy: 1.000000\n",
            "[66: 43/268] train loss: 0.000161 accuracy: 1.000000\n",
            "[66: 44/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[66: 45/268] train loss: 0.000086 accuracy: 1.000000\n",
            "[66: 46/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[66: 47/268] train loss: 0.011903 accuracy: 1.000000\n",
            "[66: 48/268] train loss: 0.000067 accuracy: 1.000000\n",
            "[66: 49/268] train loss: 0.000183 accuracy: 1.000000\n",
            "[66: 50/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[66: 50/268] test loss: 0.000594 accuracy: 1.000000\n",
            "[66: 51/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[66: 52/268] train loss: 0.000975 accuracy: 1.000000\n",
            "[66: 53/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[66: 54/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[66: 55/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[66: 56/268] train loss: 0.000345 accuracy: 1.000000\n",
            "[66: 57/268] train loss: 0.000172 accuracy: 1.000000\n",
            "[66: 58/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[66: 59/268] train loss: 0.000613 accuracy: 1.000000\n",
            "[66: 60/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[66: 60/268] test loss: 0.000117 accuracy: 1.000000\n",
            "[66: 61/268] train loss: 0.000138 accuracy: 1.000000\n",
            "[66: 62/268] train loss: 0.000083 accuracy: 1.000000\n",
            "[66: 63/268] train loss: 0.000892 accuracy: 1.000000\n",
            "[66: 64/268] train loss: 4.745012 accuracy: 0.600000\n",
            "[66: 65/268] train loss: 0.001155 accuracy: 1.000000\n",
            "[66: 66/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[66: 67/268] train loss: 0.000076 accuracy: 1.000000\n",
            "[66: 68/268] train loss: 0.018569 accuracy: 1.000000\n",
            "[66: 69/268] train loss: 0.000098 accuracy: 1.000000\n",
            "[66: 70/268] train loss: 0.000282 accuracy: 1.000000\n",
            "[66: 70/268] test loss: 0.000138 accuracy: 1.000000\n",
            "[66: 71/268] train loss: 0.000063 accuracy: 1.000000\n",
            "[66: 72/268] train loss: 0.000169 accuracy: 1.000000\n",
            "[66: 73/268] train loss: 0.000118 accuracy: 1.000000\n",
            "[66: 74/268] train loss: 0.000440 accuracy: 1.000000\n",
            "[66: 75/268] train loss: 0.000274 accuracy: 1.000000\n",
            "[66: 76/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[66: 77/268] train loss: 0.000544 accuracy: 1.000000\n",
            "[66: 78/268] train loss: 0.000198 accuracy: 1.000000\n",
            "[66: 79/268] train loss: 0.000101 accuracy: 1.000000\n",
            "[66: 80/268] train loss: 0.000518 accuracy: 1.000000\n",
            "[66: 80/268] test loss: 0.004820 accuracy: 1.000000\n",
            "[66: 81/268] train loss: 0.000512 accuracy: 1.000000\n",
            "[66: 82/268] train loss: 0.000246 accuracy: 1.000000\n",
            "[66: 83/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[66: 84/268] train loss: 0.000118 accuracy: 1.000000\n",
            "[66: 85/268] train loss: 0.000191 accuracy: 1.000000\n",
            "[66: 86/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[66: 87/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[66: 88/268] train loss: 0.000356 accuracy: 1.000000\n",
            "[66: 89/268] train loss: 0.000111 accuracy: 1.000000\n",
            "[66: 90/268] train loss: 0.021186 accuracy: 1.000000\n",
            "[66: 90/268] test loss: 0.000069 accuracy: 1.000000\n",
            "[66: 91/268] train loss: 0.000124 accuracy: 1.000000\n",
            "[66: 92/268] train loss: 0.000124 accuracy: 1.000000\n",
            "[66: 93/268] train loss: 0.000420 accuracy: 1.000000\n",
            "[66: 94/268] train loss: 0.000827 accuracy: 1.000000\n",
            "[66: 95/268] train loss: 0.000609 accuracy: 1.000000\n",
            "[66: 96/268] train loss: 0.000045 accuracy: 1.000000\n",
            "[66: 97/268] train loss: 0.000118 accuracy: 1.000000\n",
            "[66: 98/268] train loss: 0.016688 accuracy: 1.000000\n",
            "[66: 99/268] train loss: 0.000082 accuracy: 1.000000\n",
            "[66: 100/268] train loss: 0.000055 accuracy: 1.000000\n",
            "[66: 100/268] test loss: 0.000680 accuracy: 1.000000\n",
            "[66: 101/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[66: 102/268] train loss: 0.000386 accuracy: 1.000000\n",
            "[66: 103/268] train loss: 0.000492 accuracy: 1.000000\n",
            "[66: 104/268] train loss: 0.000095 accuracy: 1.000000\n",
            "[66: 105/268] train loss: 0.004176 accuracy: 1.000000\n",
            "[66: 106/268] train loss: 0.000594 accuracy: 1.000000\n",
            "[66: 107/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[66: 108/268] train loss: 0.000154 accuracy: 1.000000\n",
            "[66: 109/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[66: 110/268] train loss: 0.000611 accuracy: 1.000000\n",
            "[66: 110/268] test loss: 0.000333 accuracy: 1.000000\n",
            "[66: 111/268] train loss: 0.000148 accuracy: 1.000000\n",
            "[66: 112/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[66: 113/268] train loss: 0.000108 accuracy: 1.000000\n",
            "[66: 114/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[66: 115/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[66: 116/268] train loss: 0.000416 accuracy: 1.000000\n",
            "[66: 117/268] train loss: 0.000649 accuracy: 1.000000\n",
            "[66: 118/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[66: 119/268] train loss: 0.002535 accuracy: 1.000000\n",
            "[66: 120/268] train loss: 0.000051 accuracy: 1.000000\n",
            "[66: 120/268] test loss: 0.004555 accuracy: 1.000000\n",
            "[66: 121/268] train loss: 0.000070 accuracy: 1.000000\n",
            "[66: 122/268] train loss: 0.000357 accuracy: 1.000000\n",
            "[66: 123/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[66: 124/268] train loss: 0.004648 accuracy: 1.000000\n",
            "[66: 125/268] train loss: 0.000669 accuracy: 1.000000\n",
            "[66: 126/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[66: 127/268] train loss: 0.000742 accuracy: 1.000000\n",
            "[66: 128/268] train loss: 0.000129 accuracy: 1.000000\n",
            "[66: 129/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[66: 130/268] train loss: 0.000416 accuracy: 1.000000\n",
            "[66: 130/268] test loss: 0.000155 accuracy: 1.000000\n",
            "[66: 131/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[66: 132/268] train loss: 0.000093 accuracy: 1.000000\n",
            "[66: 133/268] train loss: 0.000130 accuracy: 1.000000\n",
            "[66: 134/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[66: 135/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[66: 136/268] train loss: 0.004803 accuracy: 1.000000\n",
            "[66: 137/268] train loss: 0.000146 accuracy: 1.000000\n",
            "[66: 138/268] train loss: 0.000121 accuracy: 1.000000\n",
            "[66: 139/268] train loss: 0.002623 accuracy: 1.000000\n",
            "[66: 140/268] train loss: 0.000577 accuracy: 1.000000\n",
            "[66: 140/268] test loss: 0.000178 accuracy: 1.000000\n",
            "[66: 141/268] train loss: 0.003008 accuracy: 1.000000\n",
            "[66: 142/268] train loss: 0.000058 accuracy: 1.000000\n",
            "[66: 143/268] train loss: 0.000153 accuracy: 1.000000\n",
            "[66: 144/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[66: 145/268] train loss: 0.000160 accuracy: 1.000000\n",
            "[66: 146/268] train loss: 0.000374 accuracy: 1.000000\n",
            "[66: 147/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[66: 148/268] train loss: 0.000128 accuracy: 1.000000\n",
            "[66: 149/268] train loss: 0.000688 accuracy: 1.000000\n",
            "[66: 150/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[66: 150/268] test loss: 0.001809 accuracy: 1.000000\n",
            "[66: 151/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[66: 152/268] train loss: 0.000152 accuracy: 1.000000\n",
            "[66: 153/268] train loss: 0.000087 accuracy: 1.000000\n",
            "[66: 154/268] train loss: 0.000111 accuracy: 1.000000\n",
            "[66: 155/268] train loss: 0.000843 accuracy: 1.000000\n",
            "[66: 156/268] train loss: 0.000099 accuracy: 1.000000\n",
            "[66: 157/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[66: 158/268] train loss: 0.001102 accuracy: 1.000000\n",
            "[66: 159/268] train loss: 0.000668 accuracy: 1.000000\n",
            "[66: 160/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[66: 160/268] test loss: 0.003377 accuracy: 1.000000\n",
            "[66: 161/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[66: 162/268] train loss: 0.000755 accuracy: 1.000000\n",
            "[66: 163/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[66: 164/268] train loss: 0.000760 accuracy: 1.000000\n",
            "[66: 165/268] train loss: 0.000145 accuracy: 1.000000\n",
            "[66: 166/268] train loss: 0.005185 accuracy: 1.000000\n",
            "[66: 167/268] train loss: 0.006300 accuracy: 1.000000\n",
            "[66: 168/268] train loss: 0.002559 accuracy: 1.000000\n",
            "[66: 169/268] train loss: 0.000332 accuracy: 1.000000\n",
            "[66: 170/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[66: 170/268] test loss: 0.000908 accuracy: 1.000000\n",
            "[66: 171/268] train loss: 0.000140 accuracy: 1.000000\n",
            "[66: 172/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[66: 173/268] train loss: 0.000067 accuracy: 1.000000\n",
            "[66: 174/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[66: 175/268] train loss: 0.000251 accuracy: 1.000000\n",
            "[66: 176/268] train loss: 0.001416 accuracy: 1.000000\n",
            "[66: 177/268] train loss: 0.000567 accuracy: 1.000000\n",
            "[66: 178/268] train loss: 0.000108 accuracy: 1.000000\n",
            "[66: 179/268] train loss: 0.000316 accuracy: 1.000000\n",
            "[66: 180/268] train loss: 0.000056 accuracy: 1.000000\n",
            "[66: 180/268] test loss: 0.000967 accuracy: 1.000000\n",
            "[66: 181/268] train loss: 0.000468 accuracy: 1.000000\n",
            "[66: 182/268] train loss: 0.063739 accuracy: 1.000000\n",
            "[66: 183/268] train loss: 0.004112 accuracy: 1.000000\n",
            "[66: 184/268] train loss: 0.000236 accuracy: 1.000000\n",
            "[66: 185/268] train loss: 0.000720 accuracy: 1.000000\n",
            "[66: 186/268] train loss: 0.000149 accuracy: 1.000000\n",
            "[66: 187/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[66: 188/268] train loss: 0.000585 accuracy: 1.000000\n",
            "[66: 189/268] train loss: 0.000280 accuracy: 1.000000\n",
            "[66: 190/268] train loss: 0.000253 accuracy: 1.000000\n",
            "[66: 190/268] test loss: 0.002674 accuracy: 1.000000\n",
            "[66: 191/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[66: 192/268] train loss: 0.001863 accuracy: 1.000000\n",
            "[66: 193/268] train loss: 0.000069 accuracy: 1.000000\n",
            "[66: 194/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[66: 195/268] train loss: 0.000127 accuracy: 1.000000\n",
            "[66: 196/268] train loss: 0.000043 accuracy: 1.000000\n",
            "[66: 197/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[66: 198/268] train loss: 0.001256 accuracy: 1.000000\n",
            "[66: 199/268] train loss: 0.000479 accuracy: 1.000000\n",
            "[66: 200/268] train loss: 0.008160 accuracy: 1.000000\n",
            "[66: 200/268] test loss: 0.000195 accuracy: 1.000000\n",
            "[66: 201/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[66: 202/268] train loss: 0.054307 accuracy: 1.000000\n",
            "[66: 203/268] train loss: 0.072097 accuracy: 0.900000\n",
            "[66: 204/268] train loss: 0.000132 accuracy: 1.000000\n",
            "[66: 205/268] train loss: 0.006934 accuracy: 1.000000\n",
            "[66: 206/268] train loss: 0.001724 accuracy: 1.000000\n",
            "[66: 207/268] train loss: 0.553182 accuracy: 0.800000\n",
            "[66: 208/268] train loss: 0.000719 accuracy: 1.000000\n",
            "[66: 209/268] train loss: 0.000111 accuracy: 1.000000\n",
            "[66: 210/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[66: 210/268] test loss: 0.000737 accuracy: 1.000000\n",
            "[66: 211/268] train loss: 0.000808 accuracy: 1.000000\n",
            "[66: 212/268] train loss: 0.000546 accuracy: 1.000000\n",
            "[66: 213/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[66: 214/268] train loss: 0.008455 accuracy: 1.000000\n",
            "[66: 215/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[66: 216/268] train loss: 0.000591 accuracy: 1.000000\n",
            "[66: 217/268] train loss: 0.000252 accuracy: 1.000000\n",
            "[66: 218/268] train loss: 0.000661 accuracy: 1.000000\n",
            "[66: 219/268] train loss: 0.000048 accuracy: 1.000000\n",
            "[66: 220/268] train loss: 0.000933 accuracy: 1.000000\n",
            "[66: 220/268] test loss: 0.000975 accuracy: 1.000000\n",
            "[66: 221/268] train loss: 0.009263 accuracy: 1.000000\n",
            "[66: 222/268] train loss: 0.000196 accuracy: 1.000000\n",
            "[66: 223/268] train loss: 0.012575 accuracy: 1.000000\n",
            "[66: 224/268] train loss: 0.000054 accuracy: 1.000000\n",
            "[66: 225/268] train loss: 0.000096 accuracy: 1.000000\n",
            "[66: 226/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[66: 227/268] train loss: 0.002703 accuracy: 1.000000\n",
            "[66: 228/268] train loss: 0.000438 accuracy: 1.000000\n",
            "[66: 229/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[66: 230/268] train loss: 0.001458 accuracy: 1.000000\n",
            "[66: 230/268] test loss: 0.000574 accuracy: 1.000000\n",
            "[66: 231/268] train loss: 0.000923 accuracy: 1.000000\n",
            "[66: 232/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[66: 233/268] train loss: 0.000192 accuracy: 1.000000\n",
            "[66: 234/268] train loss: 0.000150 accuracy: 1.000000\n",
            "[66: 235/268] train loss: 0.002520 accuracy: 1.000000\n",
            "[66: 236/268] train loss: 0.000007 accuracy: 1.000000\n",
            "[66: 237/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[66: 238/268] train loss: 0.002425 accuracy: 1.000000\n",
            "[66: 239/268] train loss: 0.001573 accuracy: 1.000000\n",
            "[66: 240/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[66: 240/268] test loss: 0.000910 accuracy: 1.000000\n",
            "[66: 241/268] train loss: 0.000678 accuracy: 1.000000\n",
            "[66: 242/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[66: 243/268] train loss: 0.000110 accuracy: 1.000000\n",
            "[66: 244/268] train loss: 0.001233 accuracy: 1.000000\n",
            "[66: 245/268] train loss: 0.004810 accuracy: 1.000000\n",
            "[66: 246/268] train loss: 0.004305 accuracy: 1.000000\n",
            "[66: 247/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[66: 248/268] train loss: 0.000276 accuracy: 1.000000\n",
            "[66: 249/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[66: 250/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[66: 250/268] test loss: 0.000350 accuracy: 1.000000\n",
            "[66: 251/268] train loss: 0.000097 accuracy: 1.000000\n",
            "[66: 252/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[66: 253/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[66: 254/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[66: 255/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[66: 256/268] train loss: 0.004676 accuracy: 1.000000\n",
            "[66: 257/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[66: 258/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[66: 259/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[66: 260/268] train loss: 0.001528 accuracy: 1.000000\n",
            "[66: 260/268] test loss: 0.003720 accuracy: 1.000000\n",
            "[66: 261/268] train loss: 0.000576 accuracy: 1.000000\n",
            "[66: 262/268] train loss: 0.000270 accuracy: 1.000000\n",
            "[66: 263/268] train loss: 0.005373 accuracy: 1.000000\n",
            "[66: 264/268] train loss: 0.000322 accuracy: 1.000000\n",
            "[66: 265/268] train loss: 0.000580 accuracy: 1.000000\n",
            "[66: 266/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[66: 267/268] train loss: 0.000006 accuracy: 0.800000\n",
            "[67: 0/268] train loss: 0.000096 accuracy: 1.000000\n",
            "[67: 0/268] test loss: 0.000034 accuracy: 1.000000\n",
            "[67: 1/268] train loss: 0.000235 accuracy: 1.000000\n",
            "[67: 2/268] train loss: 0.000062 accuracy: 1.000000\n",
            "[67: 3/268] train loss: 0.000214 accuracy: 1.000000\n",
            "[67: 4/268] train loss: 0.000150 accuracy: 1.000000\n",
            "[67: 5/268] train loss: 0.000024 accuracy: 1.000000\n",
            "[67: 6/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[67: 7/268] train loss: 0.000142 accuracy: 1.000000\n",
            "[67: 8/268] train loss: 0.000170 accuracy: 1.000000\n",
            "[67: 9/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[67: 10/268] train loss: 0.000001 accuracy: 1.000000\n",
            "[67: 10/268] test loss: 0.000095 accuracy: 1.000000\n",
            "[67: 11/268] train loss: 0.000020 accuracy: 1.000000\n",
            "[67: 12/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[67: 13/268] train loss: 0.001825 accuracy: 1.000000\n",
            "[67: 14/268] train loss: 0.000567 accuracy: 1.000000\n",
            "[67: 15/268] train loss: 0.002465 accuracy: 1.000000\n",
            "[67: 16/268] train loss: 0.000082 accuracy: 1.000000\n",
            "[67: 17/268] train loss: 0.000107 accuracy: 1.000000\n",
            "[67: 18/268] train loss: 0.000469 accuracy: 1.000000\n",
            "[67: 19/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[67: 20/268] train loss: 0.001532 accuracy: 1.000000\n",
            "[67: 20/268] test loss: 0.000981 accuracy: 1.000000\n",
            "[67: 21/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[67: 22/268] train loss: 0.000134 accuracy: 1.000000\n",
            "[67: 23/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[67: 24/268] train loss: 0.000163 accuracy: 1.000000\n",
            "[67: 25/268] train loss: 0.005336 accuracy: 1.000000\n",
            "[67: 26/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[67: 27/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[67: 28/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[67: 29/268] train loss: 0.000103 accuracy: 1.000000\n",
            "[67: 30/268] train loss: 0.000800 accuracy: 1.000000\n",
            "[67: 30/268] test loss: 0.000296 accuracy: 1.000000\n",
            "[67: 31/268] train loss: 0.001950 accuracy: 1.000000\n",
            "[67: 32/268] train loss: 0.000233 accuracy: 1.000000\n",
            "[67: 33/268] train loss: 0.000046 accuracy: 1.000000\n",
            "[67: 34/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[67: 35/268] train loss: 0.000456 accuracy: 1.000000\n",
            "[67: 36/268] train loss: 0.000167 accuracy: 1.000000\n",
            "[67: 37/268] train loss: 0.000016 accuracy: 1.000000\n",
            "[67: 38/268] train loss: 0.000829 accuracy: 1.000000\n",
            "[67: 39/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[67: 40/268] train loss: 0.000569 accuracy: 1.000000\n",
            "[67: 40/268] test loss: 0.000709 accuracy: 1.000000\n",
            "[67: 41/268] train loss: 0.000147 accuracy: 1.000000\n",
            "[67: 42/268] train loss: 0.000143 accuracy: 1.000000\n",
            "[67: 43/268] train loss: 0.000008 accuracy: 1.000000\n",
            "[67: 44/268] train loss: 0.003590 accuracy: 1.000000\n",
            "[67: 45/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[67: 46/268] train loss: 0.000310 accuracy: 1.000000\n",
            "[67: 47/268] train loss: 0.000889 accuracy: 1.000000\n",
            "[67: 48/268] train loss: 0.000096 accuracy: 1.000000\n",
            "[67: 49/268] train loss: 0.000073 accuracy: 1.000000\n",
            "[67: 50/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[67: 50/268] test loss: 0.000776 accuracy: 1.000000\n",
            "[67: 51/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[67: 52/268] train loss: 0.000112 accuracy: 1.000000\n",
            "[67: 53/268] train loss: 0.000102 accuracy: 1.000000\n",
            "[67: 54/268] train loss: 0.001300 accuracy: 1.000000\n",
            "[67: 55/268] train loss: 0.000114 accuracy: 1.000000\n",
            "[67: 56/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[67: 57/268] train loss: 0.000028 accuracy: 1.000000\n",
            "[67: 58/268] train loss: 0.000099 accuracy: 1.000000\n",
            "[67: 59/268] train loss: 0.001968 accuracy: 1.000000\n",
            "[67: 60/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[67: 60/268] test loss: 0.000212 accuracy: 1.000000\n",
            "[67: 61/268] train loss: 0.000770 accuracy: 1.000000\n",
            "[67: 62/268] train loss: 0.000636 accuracy: 1.000000\n",
            "[67: 63/268] train loss: 0.000432 accuracy: 1.000000\n",
            "[67: 64/268] train loss: 0.000160 accuracy: 1.000000\n",
            "[67: 65/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[67: 66/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[67: 67/268] train loss: 0.000110 accuracy: 1.000000\n",
            "[67: 68/268] train loss: 0.000292 accuracy: 1.000000\n",
            "[67: 69/268] train loss: 0.000026 accuracy: 1.000000\n",
            "[67: 70/268] train loss: 0.000116 accuracy: 1.000000\n",
            "[67: 70/268] test loss: 0.000797 accuracy: 1.000000\n",
            "[67: 71/268] train loss: 0.000573 accuracy: 1.000000\n",
            "[67: 72/268] train loss: 0.000221 accuracy: 1.000000\n",
            "[67: 73/268] train loss: 0.002280 accuracy: 1.000000\n",
            "[67: 74/268] train loss: 0.000239 accuracy: 1.000000\n",
            "[67: 75/268] train loss: 0.000217 accuracy: 1.000000\n",
            "[67: 76/268] train loss: 0.000219 accuracy: 1.000000\n",
            "[67: 77/268] train loss: 0.000057 accuracy: 1.000000\n",
            "[67: 78/268] train loss: 0.000025 accuracy: 1.000000\n",
            "[67: 79/268] train loss: 0.000179 accuracy: 1.000000\n",
            "[67: 80/268] train loss: 0.000053 accuracy: 1.000000\n",
            "[67: 80/268] test loss: 0.000273 accuracy: 1.000000\n",
            "[67: 81/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[67: 82/268] train loss: 0.000283 accuracy: 1.000000\n",
            "[67: 83/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[67: 84/268] train loss: 0.000113 accuracy: 1.000000\n",
            "[67: 85/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[67: 86/268] train loss: 0.000118 accuracy: 1.000000\n",
            "[67: 87/268] train loss: 0.000242 accuracy: 1.000000\n",
            "[67: 88/268] train loss: 0.000776 accuracy: 1.000000\n",
            "[67: 89/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[67: 90/268] train loss: 0.000099 accuracy: 1.000000\n",
            "[67: 90/268] test loss: 0.000823 accuracy: 1.000000\n",
            "[67: 91/268] train loss: 0.000002 accuracy: 1.000000\n",
            "[67: 92/268] train loss: 0.000451 accuracy: 1.000000\n",
            "[67: 93/268] train loss: 0.000044 accuracy: 1.000000\n",
            "[67: 94/268] train loss: 0.001507 accuracy: 1.000000\n",
            "[67: 95/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[67: 96/268] train loss: 0.000957 accuracy: 1.000000\n",
            "[67: 97/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[67: 98/268] train loss: 0.000126 accuracy: 1.000000\n",
            "[67: 99/268] train loss: 0.000021 accuracy: 1.000000\n",
            "[67: 100/268] train loss: 0.000144 accuracy: 1.000000\n",
            "[67: 100/268] test loss: 0.000146 accuracy: 1.000000\n",
            "[67: 101/268] train loss: 0.000194 accuracy: 1.000000\n",
            "[67: 102/268] train loss: 0.001319 accuracy: 1.000000\n",
            "[67: 103/268] train loss: 0.000027 accuracy: 1.000000\n",
            "[67: 104/268] train loss: 0.000115 accuracy: 1.000000\n",
            "[67: 105/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[67: 106/268] train loss: 0.000114 accuracy: 1.000000\n",
            "[67: 107/268] train loss: 0.000013 accuracy: 1.000000\n",
            "[67: 108/268] train loss: 0.000266 accuracy: 1.000000\n",
            "[67: 109/268] train loss: 0.000071 accuracy: 1.000000\n",
            "[67: 110/268] train loss: 0.003653 accuracy: 1.000000\n",
            "[67: 110/268] test loss: 0.001188 accuracy: 1.000000\n",
            "[67: 111/268] train loss: 0.000804 accuracy: 1.000000\n",
            "[67: 112/268] train loss: 0.000099 accuracy: 1.000000\n",
            "[67: 113/268] train loss: 0.000030 accuracy: 1.000000\n",
            "[67: 114/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[67: 115/268] train loss: 0.000167 accuracy: 1.000000\n",
            "[67: 116/268] train loss: 0.000015 accuracy: 1.000000\n",
            "[67: 117/268] train loss: 0.000288 accuracy: 1.000000\n",
            "[67: 118/268] train loss: 0.000239 accuracy: 1.000000\n",
            "[67: 119/268] train loss: 0.000124 accuracy: 1.000000\n",
            "[67: 120/268] train loss: 0.000266 accuracy: 1.000000\n",
            "[67: 120/268] test loss: 0.000623 accuracy: 1.000000\n",
            "[67: 121/268] train loss: 0.000129 accuracy: 1.000000\n",
            "[67: 122/268] train loss: 0.000131 accuracy: 1.000000\n",
            "[67: 123/268] train loss: 0.000019 accuracy: 1.000000\n",
            "[67: 124/268] train loss: 0.000214 accuracy: 1.000000\n",
            "[67: 125/268] train loss: 0.000278 accuracy: 1.000000\n",
            "[67: 126/268] train loss: 0.000654 accuracy: 1.000000\n",
            "[67: 127/268] train loss: 0.000057 accuracy: 1.000000\n",
            "[67: 128/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[67: 129/268] train loss: 0.000115 accuracy: 1.000000\n",
            "[67: 130/268] train loss: 0.000168 accuracy: 1.000000\n",
            "[67: 130/268] test loss: 0.000382 accuracy: 1.000000\n",
            "[67: 131/268] train loss: 0.000041 accuracy: 1.000000\n",
            "[67: 132/268] train loss: 0.000064 accuracy: 1.000000\n",
            "[67: 133/268] train loss: 0.000060 accuracy: 1.000000\n",
            "[67: 134/268] train loss: 0.000277 accuracy: 1.000000\n",
            "[67: 135/268] train loss: 0.000437 accuracy: 1.000000\n",
            "[67: 136/268] train loss: 0.000072 accuracy: 1.000000\n",
            "[67: 137/268] train loss: 0.001227 accuracy: 1.000000\n",
            "[67: 138/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[67: 139/268] train loss: 0.000037 accuracy: 1.000000\n",
            "[67: 140/268] train loss: 0.070737 accuracy: 1.000000\n",
            "[67: 140/268] test loss: 0.000083 accuracy: 1.000000\n",
            "[67: 141/268] train loss: 0.000061 accuracy: 1.000000\n",
            "[67: 142/268] train loss: 0.005149 accuracy: 1.000000\n",
            "[67: 143/268] train loss: 0.000040 accuracy: 1.000000\n",
            "[67: 144/268] train loss: 0.000308 accuracy: 1.000000\n",
            "[67: 145/268] train loss: 0.000009 accuracy: 1.000000\n",
            "[67: 146/268] train loss: 0.001928 accuracy: 1.000000\n",
            "[67: 147/268] train loss: 0.000400 accuracy: 1.000000\n",
            "[67: 148/268] train loss: 0.000115 accuracy: 1.000000\n",
            "[67: 149/268] train loss: 0.000042 accuracy: 1.000000\n",
            "[67: 150/268] train loss: 0.000181 accuracy: 1.000000\n",
            "[67: 150/268] test loss: 0.000071 accuracy: 1.000000\n",
            "[67: 151/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[67: 152/268] train loss: 0.000909 accuracy: 1.000000\n",
            "[67: 153/268] train loss: 0.000079 accuracy: 1.000000\n",
            "[67: 154/268] train loss: 0.000038 accuracy: 1.000000\n",
            "[67: 155/268] train loss: 0.000215 accuracy: 1.000000\n",
            "[67: 156/268] train loss: 0.000198 accuracy: 1.000000\n",
            "[67: 157/268] train loss: 0.001002 accuracy: 1.000000\n",
            "[67: 158/268] train loss: 0.000213 accuracy: 1.000000\n",
            "[67: 159/268] train loss: 0.000018 accuracy: 1.000000\n",
            "[67: 160/268] train loss: 0.000031 accuracy: 1.000000\n",
            "[67: 160/268] test loss: 0.000147 accuracy: 1.000000\n",
            "[67: 161/268] train loss: 0.000022 accuracy: 1.000000\n",
            "[67: 162/268] train loss: 0.000088 accuracy: 1.000000\n",
            "[67: 163/268] train loss: 0.000256 accuracy: 1.000000\n",
            "[67: 164/268] train loss: 0.000010 accuracy: 1.000000\n",
            "[67: 165/268] train loss: 0.000267 accuracy: 1.000000\n",
            "[67: 166/268] train loss: 0.000238 accuracy: 1.000000\n",
            "[67: 167/268] train loss: 0.000435 accuracy: 1.000000\n",
            "[67: 168/268] train loss: 0.000059 accuracy: 1.000000\n",
            "[67: 169/268] train loss: 0.000169 accuracy: 1.000000\n",
            "[67: 170/268] train loss: 0.034641 accuracy: 1.000000\n",
            "[67: 170/268] test loss: 0.002262 accuracy: 1.000000\n",
            "[67: 171/268] train loss: 0.000091 accuracy: 1.000000\n",
            "[67: 172/268] train loss: 0.000405 accuracy: 1.000000\n",
            "[67: 173/268] train loss: 0.000065 accuracy: 1.000000\n",
            "[67: 174/268] train loss: 0.000047 accuracy: 1.000000\n",
            "[67: 175/268] train loss: 0.000023 accuracy: 1.000000\n",
            "[67: 176/268] train loss: 0.000700 accuracy: 1.000000\n",
            "[67: 177/268] train loss: 0.000341 accuracy: 1.000000\n",
            "[67: 178/268] train loss: 0.000605 accuracy: 1.000000\n",
            "[67: 179/268] train loss: 0.002301 accuracy: 1.000000\n",
            "[67: 180/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[67: 180/268] test loss: 0.000781 accuracy: 1.000000\n",
            "[67: 181/268] train loss: 0.000039 accuracy: 1.000000\n",
            "[67: 182/268] train loss: 0.000110 accuracy: 1.000000\n",
            "[67: 183/268] train loss: 0.000779 accuracy: 1.000000\n",
            "[67: 184/268] train loss: 0.000159 accuracy: 1.000000\n",
            "[67: 185/268] train loss: 0.000032 accuracy: 1.000000\n",
            "[67: 186/268] train loss: 0.001811 accuracy: 1.000000\n",
            "[67: 187/268] train loss: 0.000715 accuracy: 1.000000\n",
            "[67: 188/268] train loss: 0.032645 accuracy: 1.000000\n",
            "[67: 189/268] train loss: 0.000014 accuracy: 1.000000\n",
            "[67: 190/268] train loss: 0.000029 accuracy: 1.000000\n",
            "[67: 190/268] test loss: 0.000232 accuracy: 1.000000\n",
            "[67: 191/268] train loss: 0.000511 accuracy: 1.000000\n",
            "[67: 192/268] train loss: 0.000034 accuracy: 1.000000\n",
            "[67: 193/268] train loss: 0.000306 accuracy: 1.000000\n",
            "[67: 194/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[67: 195/268] train loss: 0.000341 accuracy: 1.000000\n",
            "[67: 196/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[67: 197/268] train loss: 0.000356 accuracy: 1.000000\n",
            "[67: 198/268] train loss: 0.000068 accuracy: 1.000000\n",
            "[67: 199/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[67: 200/268] train loss: 0.000100 accuracy: 1.000000\n",
            "[67: 200/268] test loss: 0.000437 accuracy: 1.000000\n",
            "[67: 201/268] train loss: 0.000329 accuracy: 1.000000\n",
            "[67: 202/268] train loss: 0.000084 accuracy: 1.000000\n",
            "[67: 203/268] train loss: 0.000179 accuracy: 1.000000\n",
            "[67: 204/268] train loss: 0.000331 accuracy: 1.000000\n",
            "[67: 205/268] train loss: 0.000286 accuracy: 1.000000\n",
            "[67: 206/268] train loss: 0.000003 accuracy: 1.000000\n",
            "[67: 207/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[67: 208/268] train loss: 0.000005 accuracy: 1.000000\n",
            "[67: 209/268] train loss: 0.000006 accuracy: 1.000000\n",
            "[67: 210/268] train loss: 0.000720 accuracy: 1.000000\n",
            "[67: 210/268] test loss: 0.000100 accuracy: 1.000000\n",
            "[67: 211/268] train loss: 0.000110 accuracy: 1.000000\n",
            "[67: 212/268] train loss: 0.000205 accuracy: 1.000000\n",
            "[67: 213/268] train loss: 0.005147 accuracy: 1.000000\n",
            "[67: 214/268] train loss: 0.000129 accuracy: 1.000000\n",
            "[67: 215/268] train loss: 0.000011 accuracy: 1.000000\n",
            "[67: 216/268] train loss: 0.000988 accuracy: 1.000000\n",
            "[67: 217/268] train loss: 0.000033 accuracy: 1.000000\n",
            "[67: 218/268] train loss: 0.000017 accuracy: 1.000000\n",
            "[67: 219/268] train loss: 0.000753 accuracy: 1.000000\n",
            "[67: 220/268] train loss: 0.000106 accuracy: 1.000000\n",
            "[67: 220/268] test loss: 0.000236 accuracy: 1.000000\n",
            "[67: 221/268] train loss: 0.000035 accuracy: 1.000000\n",
            "[67: 222/268] train loss: 0.000553 accuracy: 1.000000\n",
            "[67: 223/268] train loss: 0.000012 accuracy: 1.000000\n",
            "[67: 224/268] train loss: 0.000094 accuracy: 1.000000\n",
            "[67: 225/268] train loss: 0.000509 accuracy: 1.000000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-92a05c555ae0>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mpred_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_choice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[%d: %d/%d] train loss: %f accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(classifier.state_dict(), str(\"model\" + str(epoch)))\n",
        "torch.load(\"model67\")\n"
      ],
      "metadata": {
        "id": "J9-UAL7mJNPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_classifier = torch.load(\"model67\")\n",
        "model = classifier"
      ],
      "metadata": {
        "id": "k2xYTycEJfjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "model.eval()\n",
        "device = torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "def run_data_through_model_and_save_data(dataloader, model, filename, save_dir):\n",
        "    print(\"Running training data through the OpenShape model\")\n",
        "    feats = []\n",
        "    for i, (data, label) in tqdm(enumerate(dataloader)):\n",
        "\n",
        "        data = data.to(device)\n",
        "        data = data.squeeze(0)\n",
        "        data = data[:, :3, :]\n",
        "\n",
        "        output_feat = model.get_feature_space_of_last_layer(data)\n",
        "\n",
        "        feats.append(output_feat.detach().numpy())\n",
        "\n",
        "    outfile = save_dir + \"/\" + filename + \".npy\"\n",
        "    np.save(outfile, np.array(feats))\n",
        "    print(\"Saved data to \" + outfile)\n",
        "    return feats\n",
        "\n",
        "%cd /content\n",
        "save_dir = \"Extension_outputs_with_different_distortions\"\n",
        "if not os.path.exists(save_dir):\n",
        "    print(f\"Creating directory: {save_dir}\")\n",
        "    os.makedirs(save_dir)"
      ],
      "metadata": {
        "id": "ni0PibGBJk79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993813d7-46d5-4dfc-a2f1-6fc5980c8ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load chair\n",
        "points = np.load(\"/content/output_point_cloud.npy\")\n",
        "points = torch.from_numpy(points)\n",
        "points = points.reshape(-1, 3)\n",
        "\n",
        "pcd = o3d.geometry.PointCloud()\n",
        "pcd.points = o3d.utility.Vector3dVector(points)\n",
        "\n",
        "# Create a defect point cloud\n",
        "number_of_samples_per_defect = 300\n",
        "type_of_defects = [\"distort\", \"remove parts\", \"scale\", \"add noise\", \"asymmetrical\"]\n",
        "defect_chairs_distort = generate_defect_chairs(pcd, [\"distort\"], number_of_noisy_chairs=number_of_samples_per_defect)\n",
        "defect_chairs_remove_parts = generate_defect_chairs(pcd, [\"remove parts\"], number_of_noisy_chairs=number_of_samples_per_defect)\n",
        "defect_chairs_scale = generate_defect_chairs(pcd, [\"scale\"], number_of_noisy_chairs=number_of_samples_per_defect)\n",
        "defect_chairs_add_noise = generate_defect_chairs(pcd, [\"add noise\"], number_of_noisy_chairs=number_of_samples_per_defect)\n",
        "defect_chairs_asymmetrical = generate_defect_chairs(pcd, [\"asymmetrical\"], number_of_noisy_chairs=number_of_samples_per_defect)\n",
        "\n",
        "# Create labels\n",
        "defect_chairs_distort_labels = [\"distort\"] * number_of_samples_per_defect\n",
        "defect_chairs_remove_parts_labels = [\"remove parts\"] * number_of_samples_per_defect\n",
        "defect_chairs_scale_labels = [\"scale\"] * number_of_samples_per_defect\n",
        "defect_chairs_add_noise_labels = [\"add noise\"] * number_of_samples_per_defect\n",
        "defect_chairs_asymmetrical_labels = [\"asymmetrical\"] * number_of_samples_per_defect\n",
        "\n",
        "# Filter for OpenShape\n",
        "for i in range(0, number_of_samples_per_defect):\n",
        "    _, defect_chairs_distort[i] = filter_for_OpenShape(defect_chairs_distort[i], num_points=10000)\n",
        "    _, defect_chairs_remove_parts[i] = filter_for_OpenShape(defect_chairs_remove_parts[i], num_points=10000)\n",
        "    _, defect_chairs_scale[i] = filter_for_OpenShape(defect_chairs_scale[i], num_points=10000)\n",
        "    _, defect_chairs_add_noise[i] = filter_for_OpenShape(defect_chairs_add_noise[i], num_points=10000)\n",
        "    _, defect_chairs_asymmetrical[i] = filter_for_OpenShape(defect_chairs_asymmetrical[i], num_points=10000)\n",
        "\n",
        "# Create datasets\n",
        "defect_chairs_distort_dataset = PointCloudDatasetWithLabels(defect_chairs_distort, defect_chairs_distort_labels)\n",
        "defect_chairs_remove_parts_dataset = PointCloudDatasetWithLabels(defect_chairs_remove_parts, defect_chairs_remove_parts_labels)\n",
        "defect_chairs_scale_dataset = PointCloudDatasetWithLabels(defect_chairs_scale, defect_chairs_scale_labels)\n",
        "defect_chairs_add_noise_dataset = PointCloudDatasetWithLabels(defect_chairs_add_noise, defect_chairs_add_noise_labels)\n",
        "defect_chairs_asymmetrical_dataset = PointCloudDatasetWithLabels(defect_chairs_asymmetrical, defect_chairs_asymmetrical_labels)\n",
        "\n",
        "# Create dataloaders\n",
        "defect_chairs_distort_loader = DataLoader(defect_chairs_distort_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "defect_chairs_remove_parts_loader = DataLoader(defect_chairs_remove_parts_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "defect_chairs_scale_loader = DataLoader(defect_chairs_scale_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "defect_chairs_add_noise_loader = DataLoader(defect_chairs_add_noise_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "defect_chairs_asymmetrical_loader = DataLoader(defect_chairs_asymmetrical_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "# Run data through PointNet\n",
        "\n",
        "defect_chairs_distort_feats = run_data_through_model_and_save_data(defect_chairs_distort_loader, model, \"defect_chairs_distort_feats\", save_dir)\n",
        "defect_chairs_remove_parts_feats = run_data_through_model_and_save_data(defect_chairs_remove_parts_loader, model, \"defect_chairs_remove_parts_feats\", save_dir)\n",
        "defect_chairs_scale_feats = run_data_through_model_and_save_data(defect_chairs_scale_loader, model, \"defect_chairs_scale_feats\", save_dir)\n",
        "defect_chairs_add_noise_feats = run_data_through_model_and_save_data(defect_chairs_add_noise_loader, model, \"defect_chairs_add_noise_feats\", save_dir)\n",
        "defect_chairs_asymmetrical_feats = run_data_through_model_and_save_data(defect_chairs_asymmetrical_loader, model, \"defect_chairs_asymmetrical_feats\", save_dir)\n",
        "\n",
        "train_feats = run_data_through_model_and_save_data(train_loader, \"train_feats\", save_dir)\n",
        "src_feats = run_data_through_model_and_save_data(id_loader, \"src_feats\", save_dir)\n",
        "\n",
        "train_feats = np.load(\"Extension_outputs_with_different_distortions/train_feats.npy\")\n",
        "src_feats = np.load(\"Extension_outputs_with_different_distortions/src_feats.npy\")\n",
        "defect_chairs_distort_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_distort_feats.npy\")\n",
        "defect_chairs_remove_parts_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_remove_parts_feats.npy\")\n",
        "defect_chairs_scale_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_scale_feats.npy\")\n",
        "defect_chairs_add_noise_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_add_noise_feats.npy\")\n",
        "defect_chairs_asymmetrical_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_asymmetrical_feats.npy\")\n",
        "\n",
        "train_feats = torch.from_numpy(train_feats)\n",
        "src_feats = torch.from_numpy(src_feats)\n",
        "defect_chairs_distort_feats = torch.from_numpy(defect_chairs_distort_feats)\n",
        "defect_chairs_remove_parts_feats = torch.from_numpy(defect_chairs_remove_parts_feats)\n",
        "defect_chairs_scale_feats = torch.from_numpy(defect_chairs_scale_feats)\n",
        "defect_chairs_add_noise_feats = torch.from_numpy(defect_chairs_add_noise_feats)\n",
        "defect_chairs_asymmetrical_feats = torch.from_numpy(defect_chairs_asymmetrical_feats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "sGIDOUhpM2--",
        "outputId": "a1c40500-cef9-4672-a030-99947d0ae94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running training data through the OpenShape model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "300it [00:58,  5.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved data to Extension_outputs_with_different_distortions/defect_chairs_distort_feats.npy\n",
            "Running training data through the OpenShape model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "300it [01:00,  4.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved data to Extension_outputs_with_different_distortions/defect_chairs_remove_parts_feats.npy\n",
            "Running training data through the OpenShape model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "300it [00:56,  5.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved data to Extension_outputs_with_different_distortions/defect_chairs_scale_feats.npy\n",
            "Running training data through the OpenShape model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "300it [00:58,  5.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved data to Extension_outputs_with_different_distortions/defect_chairs_add_noise_feats.npy\n",
            "Running training data through the OpenShape model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "300it [01:01,  4.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved data to Extension_outputs_with_different_distortions/defect_chairs_asymmetrical_feats.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "run_data_through_model_and_save_data() missing 1 required positional argument: 'save_dir'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-98ba4aec922d>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mdefect_chairs_asymmetrical_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_data_through_model_and_save_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefect_chairs_asymmetrical_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"defect_chairs_asymmetrical_feats\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mtrain_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_data_through_model_and_save_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_feats\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0msrc_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_data_through_model_and_save_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"src_feats\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: run_data_through_model_and_save_data() missing 1 required positional argument: 'save_dir'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_feats = run_data_through_model_and_save_data(train_loader, model, \"train_feats\", save_dir)\n",
        "src_feats = run_data_through_model_and_save_data(id_loader, model, \"src_feats\", save_dir)\n",
        "\n",
        "train_feats = np.load(\"Extension_outputs_with_different_distortions/train_feats.npy\")\n",
        "src_feats = np.load(\"Extension_outputs_with_different_distortions/src_feats.npy\")\n",
        "defect_chairs_distort_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_distort_feats.npy\")\n",
        "defect_chairs_remove_parts_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_remove_parts_feats.npy\")\n",
        "defect_chairs_scale_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_scale_feats.npy\")\n",
        "defect_chairs_add_noise_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_add_noise_feats.npy\")\n",
        "defect_chairs_asymmetrical_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_asymmetrical_feats.npy\")\n",
        "\n",
        "train_feats = torch.from_numpy(train_feats)\n",
        "src_feats = torch.from_numpy(src_feats)\n",
        "defect_chairs_distort_feats = torch.from_numpy(defect_chairs_distort_feats)\n",
        "defect_chairs_remove_parts_feats = torch.from_numpy(defect_chairs_remove_parts_feats)\n",
        "defect_chairs_scale_feats = torch.from_numpy(defect_chairs_scale_feats)\n",
        "defect_chairs_add_noise_feats = torch.from_numpy(defect_chairs_add_noise_feats)\n",
        "defect_chairs_asymmetrical_feats = torch.from_numpy(defect_chairs_asymmetrical_feats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYELpQ9zQTdf",
        "outputId": "b94b326d-a13f-47ae-be2a-c7f4d151cbb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running training data through the OpenShape model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "300it [01:03,  4.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved data to Extension_outputs_with_different_distortions/train_feats.npy\n",
            "Running training data through the OpenShape model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "300it [01:03,  4.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved data to Extension_outputs_with_different_distortions/src_feats.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_feats = np.load(\"Extension_outputs_with_different_distortions/train_feats.npy\")\n",
        "src_feats = np.load(\"Extension_outputs_with_different_distortions/src_feats.npy\")\n",
        "defect_chairs_distort_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_distort_feats.npy\")\n",
        "defect_chairs_remove_parts_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_remove_parts_feats.npy\")\n",
        "defect_chairs_scale_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_scale_feats.npy\")\n",
        "defect_chairs_add_noise_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_add_noise_feats.npy\")\n",
        "defect_chairs_asymmetrical_feats = np.load(\"/content/Extension_outputs_with_different_distortions/defect_chairs_asymmetrical_feats.npy\")\n",
        "\n",
        "train_feats = torch.from_numpy(train_feats)\n",
        "src_feats = torch.from_numpy(src_feats)\n",
        "defect_chairs_distort_feats = torch.from_numpy(defect_chairs_distort_feats)\n",
        "defect_chairs_remove_parts_feats = torch.from_numpy(defect_chairs_remove_parts_feats)\n",
        "defect_chairs_scale_feats = torch.from_numpy(defect_chairs_scale_feats)\n",
        "defect_chairs_add_noise_feats = torch.from_numpy(defect_chairs_add_noise_feats)\n",
        "defect_chairs_asymmetrical_feats = torch.from_numpy(defect_chairs_asymmetrical_feats)\n",
        "\n",
        "# Evaluate model\n",
        "from knn_cuda import KNN\n",
        "%cd /content/3D_OS\n",
        "from utils.ood_utils import eval_ood_sncore\n",
        "%cd /content\n",
        "\n",
        "knn = KNN(k=1, transpose_mode=True)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "train_feats = train_feats.to(device)\n",
        "src_feats = src_feats.to(device)\n",
        "defect_chairs_distort_feats = defect_chairs_distort_feats.to(device)\n",
        "defect_chairs_remove_parts_feats = defect_chairs_remove_parts_feats.to(device)\n",
        "defect_chairs_scale_feats = defect_chairs_scale_feats.to(device)\n",
        "defect_chairs_add_noise_feats = defect_chairs_add_noise_feats.to(device)\n",
        "defect_chairs_asymmetrical_feats = defect_chairs_asymmetrical_feats.to(device)\n",
        "\n",
        "\n",
        "def print_CeL2_scores(train_feats, src_feats, ood_feats):\n",
        "    print(\"Euclidean distances in a non-normalized space:\")\n",
        "    # eucl distance in a non-normalized space\n",
        "    src_dist, src_ids = knn(train_feats.unsqueeze(1), src_feats.unsqueeze(1))\n",
        "    src_dist = src_dist.squeeze().cpu()\n",
        "    src_scores = 1 / src_dist\n",
        "\n",
        "    ood_dist, _ = knn(train_feats.unsqueeze(0), ood_feats.unsqueeze(0))\n",
        "    ood_dist = ood_dist.squeeze().cpu()\n",
        "    ood_scores = 1 / ood_dist\n",
        "\n",
        "    # Scores for distort\n",
        "    eval_ood_sncore(\n",
        "        scores_list=[src_scores, ood_scores, ood_scores],\n",
        "        preds_list=[None, None, None],  # [src_pred, None, None],\n",
        "        labels_list=[None, None, None],  # [src_labels, None, None],\n",
        "        src_label=1  # confidence should be higher for ID samples\n",
        "    )\n",
        "# Print scores\n",
        "print(\"defect_chairs_distort_feats\")\n",
        "print_CeL2_scores(train_feats, src_feats, defect_chairs_distort_feats)\n",
        "print(\"defect_chairs_remove_parts_feats\")\n",
        "print_CeL2_scores(train_feats, src_feats, defect_chairs_remove_parts_feats)\n",
        "print(\"defect_chairs_scale_feats\")\n",
        "print_CeL2_scores(train_feats, src_feats, defect_chairs_scale_feats)\n",
        "print(\"defect_chairs_add_noise_feats\")\n",
        "print_CeL2_scores(train_feats, src_feats, defect_chairs_add_noise_feats)\n",
        "print(\"defect_chairs_asymmetrical_feats\")\n",
        "print_CeL2_scores(train_feats, src_feats, defect_chairs_asymmetrical_feats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEBB8VKSNLFM",
        "outputId": "21f1cd4e-beb3-4889-ef95-1e664e774ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/3D_OS\n",
            "/content\n",
            "defect_chairs_distort_feats\n",
            "Euclidean distances in a non-normalized space:\n",
            "AUROC - Src label: 1, Tar label: 0\n",
            "SRC->TAR1:      AUROC: 0.9883, FPR95: 0.0200, AUPR_IN: 0.9783, AUPR_OUT: 0.9930\n",
            "SRC->TAR2:      AUROC: 0.9883, FPR95: 0.0200, AUPR_IN: 0.9783, AUPR_OUT: 0.9930\n",
            "SRC->TAR1+TAR2: AUROC: 0.9883, FPR95: 0.0200, AUPR_IN: 0.9576, AUPR_OUT: 0.9956\n",
            "to spreadsheet: 0.9883000000000001,0.02,0.9782759380244943,0.9930391921999192,0.9883000000000001,0.02,0.9782759380244943,0.9930391921999192,0.9883000000000001,0.02,0.9576018812897065,0.9956366433258719\n",
            "defect_chairs_remove_parts_feats\n",
            "Euclidean distances in a non-normalized space:\n",
            "AUROC - Src label: 1, Tar label: 0\n",
            "SRC->TAR1:      AUROC: 1.0000, FPR95: 0.0000, AUPR_IN: 1.0000, AUPR_OUT: 1.0000\n",
            "SRC->TAR2:      AUROC: 1.0000, FPR95: 0.0000, AUPR_IN: 1.0000, AUPR_OUT: 1.0000\n",
            "SRC->TAR1+TAR2: AUROC: 1.0000, FPR95: 0.0000, AUPR_IN: 1.0000, AUPR_OUT: 1.0000\n",
            "to spreadsheet: 1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0\n",
            "defect_chairs_scale_feats\n",
            "Euclidean distances in a non-normalized space:\n",
            "AUROC - Src label: 1, Tar label: 0\n",
            "SRC->TAR1:      AUROC: 1.0000, FPR95: 0.0000, AUPR_IN: 1.0000, AUPR_OUT: 1.0000\n",
            "SRC->TAR2:      AUROC: 1.0000, FPR95: 0.0000, AUPR_IN: 1.0000, AUPR_OUT: 1.0000\n",
            "SRC->TAR1+TAR2: AUROC: 1.0000, FPR95: 0.0000, AUPR_IN: 1.0000, AUPR_OUT: 1.0000\n",
            "to spreadsheet: 1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0\n",
            "defect_chairs_add_noise_feats\n",
            "Euclidean distances in a non-normalized space:\n",
            "AUROC - Src label: 1, Tar label: 0\n",
            "SRC->TAR1:      AUROC: 1.0000, FPR95: 0.0000, AUPR_IN: 1.0000, AUPR_OUT: 1.0000\n",
            "SRC->TAR2:      AUROC: 1.0000, FPR95: 0.0000, AUPR_IN: 1.0000, AUPR_OUT: 1.0000\n",
            "SRC->TAR1+TAR2: AUROC: 1.0000, FPR95: 0.0000, AUPR_IN: 1.0000, AUPR_OUT: 1.0000\n",
            "to spreadsheet: 1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0\n",
            "defect_chairs_asymmetrical_feats\n",
            "Euclidean distances in a non-normalized space:\n",
            "AUROC - Src label: 1, Tar label: 0\n",
            "SRC->TAR1:      AUROC: 1.0000, FPR95: 0.0000, AUPR_IN: 1.0000, AUPR_OUT: 1.0000\n",
            "SRC->TAR2:      AUROC: 1.0000, FPR95: 0.0000, AUPR_IN: 1.0000, AUPR_OUT: 1.0000\n",
            "SRC->TAR1+TAR2: AUROC: 1.0000, FPR95: 0.0000, AUPR_IN: 1.0000, AUPR_OUT: 1.0000\n",
            "to spreadsheet: 1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(defect_chairs_distort_feats)\n",
        "print(defect_chairs_remove_parts_feats)\n",
        "print(defect_chairs_scale_feats)\n",
        "print(defect_chairs_add_noise_feats)\n",
        "print(defect_chairs_asymmetrical_feats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lit4f8DgRhab",
        "outputId": "0c23e6db-40b5-4527-a642-25b40b441c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ -3.6706,  -7.9788,  -6.7237,  ...,  -6.7765, -12.1274,  13.5159]],\n",
            "\n",
            "        [[ -3.6679,  -7.9653,  -6.6952,  ...,  -6.7639, -12.1006,  13.4855]],\n",
            "\n",
            "        [[ -3.6453,  -7.9775,  -6.7163,  ...,  -6.7448, -12.0890,  13.4815]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ -4.0671,  -6.2134,  -6.7875,  ...,  -6.0805, -11.1878,  12.4834]],\n",
            "\n",
            "        [[ -4.0651,  -6.2147,  -6.7866,  ...,  -6.0691, -11.1833,  12.4827]],\n",
            "\n",
            "        [[ -4.0427,  -6.2368,  -6.7901,  ...,  -6.0542, -11.1746,  12.4764]]],\n",
            "       device='cuda:0')\n",
            "tensor([[[ -2.8410, -12.7275,  -5.5621,  ...,  -8.0009, -15.0294,  14.5772]],\n",
            "\n",
            "        [[ -5.9113,  -7.9825,  -4.8631,  ...,  -2.3361, -11.7184,   5.8913]],\n",
            "\n",
            "        [[ -3.2606,  -8.1111,  -7.7994,  ...,  -4.3259, -11.4267,  12.6995]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ -1.7480,  -6.0828,  -6.9717,  ...,  -7.3377, -10.9451,  11.2264]],\n",
            "\n",
            "        [[ -4.6273,  -8.9385,  -2.4137,  ...,  -5.5968, -11.8166,  10.9567]],\n",
            "\n",
            "        [[ -1.9186,  -5.9729,  -6.8812,  ...,  -7.5194, -11.0737,  11.3135]]],\n",
            "       device='cuda:0')\n",
            "tensor([[[ -4.0428,  -6.2369,  -6.7894,  ...,  -6.0542, -11.1743,  12.4763]],\n",
            "\n",
            "        [[ -4.0421,  -6.2377,  -6.7907,  ...,  -6.0548, -11.1751,  12.4771]],\n",
            "\n",
            "        [[ -4.0426,  -6.2373,  -6.7902,  ...,  -6.0546, -11.1749,  12.4769]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ -4.0426,  -6.2376,  -6.7903,  ...,  -6.0545, -11.1750,  12.4771]],\n",
            "\n",
            "        [[ -4.0426,  -6.2376,  -6.7903,  ...,  -6.0545, -11.1750,  12.4771]],\n",
            "\n",
            "        [[ -4.0426,  -6.2376,  -6.7903,  ...,  -6.0545, -11.1750,  12.4771]]],\n",
            "       device='cuda:0')\n",
            "tensor([[[ -4.0426,  -6.2376,  -6.7903,  ...,  -6.0545, -11.1750,  12.4771]],\n",
            "\n",
            "        [[ -4.0426,  -6.2376,  -6.7903,  ...,  -6.0545, -11.1750,  12.4771]],\n",
            "\n",
            "        [[ -4.0426,  -6.2376,  -6.7903,  ...,  -6.0545, -11.1750,  12.4771]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ -4.0426,  -6.2376,  -6.7903,  ...,  -6.0545, -11.1750,  12.4771]],\n",
            "\n",
            "        [[ -4.0426,  -6.2376,  -6.7903,  ...,  -6.0545, -11.1750,  12.4771]],\n",
            "\n",
            "        [[ -4.0426,  -6.2376,  -6.7903,  ...,  -6.0545, -11.1750,  12.4771]]],\n",
            "       device='cuda:0')\n",
            "tensor([[[ -5.4428,  -3.6909,  -4.6349,  ...,  -6.0100, -10.0239,   8.8753]],\n",
            "\n",
            "        [[ -4.4007,  -7.9739,  -7.5285,  ...,  -4.3121, -11.8018,  13.3370]],\n",
            "\n",
            "        [[ -3.2135,  -8.2003,  -7.5500,  ...,  -6.6882, -12.2030,  13.2277]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ -4.0843,  -7.6152,  -8.6780,  ...,  -5.5947, -12.2761,  13.3930]],\n",
            "\n",
            "        [[ -4.4727,  -7.9187,  -6.9969,  ...,  -5.6598, -12.0288,  13.4797]],\n",
            "\n",
            "        [[ -2.4099,  -6.4353,  -5.0007,  ...,  -7.4354, -10.5529,  10.8963]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(src_feats.shape)\n",
        "print(defect_chairs_remove_parts_feats.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-FiKv48ScOE",
        "outputId": "4bc7fdf2-a97a-46fb-a35b-59231ab66e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([300, 1, 7])\n",
            "torch.Size([300, 1, 7])\n"
          ]
        }
      ]
    }
  ]
}